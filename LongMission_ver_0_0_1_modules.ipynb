{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amodernmarketer/flask-boilerplate/blob/master/LongMission_ver_0_0_1_modules.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "create a project\n",
        "write a save_project function that will auto-save, or be triggered manually\n",
        "save project\n",
        "load project\n",
        "Create outcome\n",
        "add missions\n",
        "add steps\n",
        "add substeps\n",
        "import promptcabin api to use prompts\n",
        "'''"
      ],
      "metadata": {
        "id": "69QFV4UkUyab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run(**kwargs):\n",
        "  project = Project('.')\n",
        "  if default:\n",
        "    project\n",
        "\n",
        "if __name__ == \"main\""
      ],
      "metadata": {
        "id": "4fFyHcWwV3LD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yj77lA507Vp3"
      },
      "outputs": [],
      "source": [
        "# install dependencies\n",
        "!apt install chromium-browser xvfb\n",
        "!pip install -U selenium_profiles pyChatGPT\n",
        "\n",
        "# install chromedriver\n",
        "from selenium_profiles.utils.installer import install_chromedriver\n",
        "install_chromedriver()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dO4J4K7qDigu"
      },
      "outputs": [],
      "source": [
        "# start your script as normal\n",
        "!python3 -m pyChatGPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Hk9bO-cNvJa"
      },
      "outputs": [],
      "source": [
        "import threading\n",
        "import time\n",
        "\n",
        "def run():\n",
        "        print('output from background {}'.format('hello'))\n",
        "\n",
        "t = threading.Thread(target=run)\n",
        "t.start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UfG1hEPuVDw0"
      },
      "outputs": [],
      "source": [
        "import threading\n",
        "from IPython.display import display, HTML\n",
        "import ipywidgets as widgets\n",
        "import time\n",
        "\n",
        "def thread_func(something, out):\n",
        "    for i in range(1, 5):\n",
        "        time.sleep(0.3)\n",
        "        out.append_stdout('{} {} {}\\n'.format(i, '**'*i, something))\n",
        "    out.append_display_data(HTML(\"<em>All done!</em>\"))\n",
        "\n",
        "display('Display in main thread')\n",
        "out = widgets.Output()\n",
        "# Now the key: the container is displayed (while empty) in the main thread\n",
        "display(out)\n",
        "\n",
        "thread = threading.Thread(\n",
        "    target=thread_func,\n",
        "    args=(\"some text\", out))\n",
        "thread.start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0WgcmrnCaP0"
      },
      "source": [
        "#LongMission API\n",
        "\n",
        "PromptCabin's LongMission API serves as a sophisticated and reliable workflow assistant that empowers users to streamline their task completion process. With its comprehensive set of features, LongMission API provides valuable guidance, insightful tips, and actionable steps to support users in various endeavors, ranging from brainstorming ideas for plugin design to orchestrating a successful product launch.\n",
        "##Design\n",
        "Designed to be a powerful tool in enhancing productivity and maintaining focus, the LongMission API offers a host of functionalities that enable users to effectively manage their projects and achieve their goals. By leveraging the API's capabilities, users can access a wealth of resources and leverage a structured approach to navigate complex tasks and assignments.\n",
        "## Core Functionality\n",
        "The LongMission API's core functionality revolves around the concept of missions, which represent overarching objectives or goals. Users can create, update, and track missions, ensuring a clear understanding of project milestones and progress. Through an intuitive interface, users can specify mission details such as title, description, and associated outcomes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndIE3anvriP5"
      },
      "source": [
        "##Output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebY6apJasGhU"
      },
      "source": [
        "I want to be able to\n",
        "\n",
        "how\n",
        "\n",
        "\"I am working on a {outcome}, how can I use the {pattern} here?\n",
        "\n",
        "howall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "hOLAYivhYK6m"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "class Project:\n",
        "    def __init__(self, path):\n",
        "        self.path = path\n",
        "        self.data = None\n",
        "\n",
        "    def load_project(self):\n",
        "        try:\n",
        "            with open(self.path, 'r') as file:\n",
        "                self.data = json.load(file)\n",
        "                print(\"Project loaded successfully.\")\n",
        "        except FileNotFoundError:\n",
        "            print(\"Project file not found. Creating a new project.\")\n",
        "            self.create_project()\n",
        "\n",
        "    def save_project(self):\n",
        "        with open(self.path, 'w') as file:\n",
        "            json.dump(self.data, file, indent=4)\n",
        "        print(\"Project saved successfully.\")\n",
        "\n",
        "    def create_project(self):\n",
        "        self.data = {\n",
        "            'outcomes': []\n",
        "        }\n",
        "        self.save_project()\n",
        "\n",
        "    def add_outcome(self, outcome):\n",
        "        if not self.data:\n",
        "            print(\"Project not loaded. Load or create a project first.\")\n",
        "            return\n",
        "        self.data['outcomes'].append(outcome)\n",
        "        self.save_project()\n",
        "        print(\"Outcome added to the project.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUbPYYSWEu8O"
      },
      "source": [
        "# Getting Started\n",
        "\n",
        "Welcome to the QuickGuide for using the LongMission API in your project. This guide will walk you through the process of creating and managing your projects, outcomes, missions, goals, steps, and substeps. Let's get started!\n",
        "\n",
        "## Creating a Project\n",
        "\n",
        "To begin, you'll need to create a project to organize your tasks and objectives. Follow these steps to create a project:\n",
        "\n",
        "1. Instantiate the `LongMission` API client in your code.\n",
        "2. Use the `create_project()` method to create a new project.\n",
        "3. Provide the necessary details for your project, such as the project name, description, and any additional metadata.\n",
        "\n",
        "Once your project is created, you can proceed with defining the outcomes, missions, goals, steps, and substeps that will drive your project's success.\n",
        "\n",
        "### Create an Outcome\n",
        "\n",
        "Outcomes represent the overarching objectives or goals that you want to achieve in your project. To create an outcome, follow these steps:\n",
        "\n",
        "1. Use the `create_outcome()` method of the `LongMission` API client.\n",
        "2. Provide the required information for the outcome, such as the outcome title, description, and any relevant metadata.\n",
        "3. Associate the outcome with your project using the project ID.\n",
        "\n",
        "Creating clear and well-defined outcomes will help you stay focused on your project's ultimate goals and measure your progress effectively.\n",
        "\n",
        "### Create a Mission\n",
        "\n",
        "Missions represent key milestones or phases within your project. To create a mission, follow these steps:\n",
        "\n",
        "1. Use the `create_mission()` method of the `LongMission` API client.\n",
        "2. Specify the mission details, including the mission title, description, associated outcome, and any additional metadata.\n",
        "3. Link the mission to your project using the project and outcome IDs.\n",
        "\n",
        "By organizing your project into missions, you can break down complex tasks into manageable chunks and ensure a structured approach to project management.\n",
        "\n",
        "### Create a Goal\n",
        "\n",
        "Goals represent specific objectives that contribute to the achievement of a mission. To create a goal, follow these steps:\n",
        "\n",
        "1. Utilize the `create_goal()` method of the `LongMission` API client.\n",
        "2. Provide the necessary information for the goal, such as the goal title, description, associated mission, and any relevant metadata.\n",
        "3. Associate the goal with your project, outcome, and mission using the respective IDs.\n",
        "\n",
        "Setting clear and actionable goals will help you track progress and maintain focus on the mission at hand.\n",
        "\n",
        "### Create a Step\n",
        "\n",
        "Steps represent individual tasks or actions that need to be completed to achieve a goal. To create a step, follow these steps:\n",
        "\n",
        "1. Use the `create_step()` method of the `LongMission` API client.\n",
        "2. Specify the step details, including the step title, description, associated goal, and any additional metadata.\n",
        "3. Link the step to your project, outcome, mission, and goal using the respective IDs.\n",
        "\n",
        "Creating well-defined steps will provide a roadmap for task execution and ensure a structured workflow within your project.\n",
        "\n",
        "### Create a Substep\n",
        "\n",
        "Substeps allow you to further break down complex steps into smaller, actionable tasks. To create a substep, follow these steps:\n",
        "\n",
        "1. Utilize the `create_substep()` method of the `LongMission` API client.\n",
        "2. Provide the required information for the substep, including the substep title, description, associated step, and any relevant metadata.\n",
        "3. Associate the substep with your project, outcome, mission, goal, and step using the respective IDs.\n",
        "\n",
        "By creating substeps, you can manage intricate tasks more effectively and maintain a clear hierarchy within your project structure.\n",
        "\n",
        "Congratulations! You have now learned how to create and manage projects, outcomes, missions, goals, steps, and substeps using the LongMission API. Start organizing your tasks and achieving your project objectives with confidence.\n",
        "\n",
        "# LongMission API\n",
        "\n",
        "The LongMission API provides functionality for managing long missions, outcomes, missions, goals, steps, substeps, prompts, prompt patterns, and prompt pattern metadata.\n",
        "\n",
        "## API Endpoints\n",
        "\n",
        "The following are the available API endpoints:\n",
        "\n",
        "### Outcomes\n",
        "\n",
        "- `GET /outcomes`: Retrieve a list of all outcomes.\n",
        "- `GET /outcomes/{outcome_id}`: Retrieve a specific outcome by ID.\n",
        "- `POST /outcomes`: Create a new outcome.\n",
        "- `PUT /outcomes/{outcome_id}`: Update an existing outcome by ID.\n",
        "- `DELETE /outcomes/{outcome_id}`: Delete an outcome by ID.\n",
        "\n",
        "### Missions\n",
        "\n",
        "- `GET /missions`: Retrieve a list of all missions.\n",
        "- `GET /missions/{mission_id}`: Retrieve a specific mission by ID.\n",
        "- `POST /missions`: Create a new mission.\n",
        "- `PUT /missions/{mission_id}`: Update an existing mission by ID.\n",
        "- `DELETE /missions/{mission_id}`: Delete a mission by ID.\n",
        "\n",
        "### Goals\n",
        "\n",
        "- `GET /goals`: Retrieve a list of all goals.\n",
        "- `GET /goals/{goal_id}`: Retrieve a specific goal by ID.\n",
        "- `POST /goals`: Create a new goal.\n",
        "- `PUT /goals/{goal_id}`: Update an existing goal by ID.\n",
        "- `DELETE /goals/{goal_id}`: Delete a goal by ID.\n",
        "\n",
        "### Steps\n",
        "\n",
        "- `GET /steps`: Retrieve a list of all steps.\n",
        "- `GET /steps/{step_id}`: Retrieve a specific step by ID.\n",
        "- `POST /steps`: Create a new step.\n",
        "- `PUT /steps/{step_id}`: Update an existing step by ID.\n",
        "- `DELETE /steps/{step_id}`: Delete a step by ID.\n",
        "\n",
        "### Substeps\n",
        "\n",
        "- `GET /substeps`: Retrieve a list of all substeps.\n",
        "- `GET /substeps/{substep_id}`: Retrieve a specific substep by ID.\n",
        "- `POST /substeps`: Create a new substep.\n",
        "- `PUT /substeps/{substep_id}`: Update an existing substep by ID.\n",
        "- `DELETE /substeps/{substep_id}`: Delete a substep by ID.\n",
        "\n",
        "### Prompts\n",
        "\n",
        "- `GET /prompts`: Retrieve a list of all prompts.\n",
        "- `GET /prompts/{prompt_id}`: Retrieve a specific prompt by ID.\n",
        "- `POST /prompts`: Create a new prompt.\n",
        "- `PUT /prompts/{prompt_id}`: Update an existing prompt by ID.\n",
        "- `DELETE /prompts/{prompt_id}`: Delete a prompt by ID.\n",
        "\n",
        "### Prompt Patterns\n",
        "\n",
        "- `GET /prompt-patterns`: Retrieve a list of all prompt patterns.\n",
        "- `GET /prompt-patterns/{pattern_id}`: Retrieve a specific prompt pattern by ID.\n",
        "- `POST /prompt-patterns`: Create a new prompt pattern.\n",
        "- `PUT /prompt-patterns/{pattern_id}`: Update an existing prompt pattern by ID.\n",
        "- `DELETE /prompt-patterns/{pattern_id}`: Delete a prompt pattern by ID.\n",
        "\n",
        "### Prompt Pattern Metadata\n",
        "\n",
        "- `GET /prompt-pattern-metadata`: Retrieve a list of all prompt pattern metadata.\n",
        "- `GET /prompt-pattern-metadata/{meta_id}`: Retrieve a specific prompt pattern metadata by ID.\n",
        "- `POST /prompt-pattern-metadata`: Create a new prompt pattern metadata.\n",
        "- `PUT /prompt-pattern-metadata/{meta_id}`: Update an existing prompt pattern metadata by ID.\n",
        "- `DELETE /prompt-pattern-metadata/{meta_id}`: Delete a prompt pattern metadata by ID.\n",
        "\n",
        "## Usage\n",
        "\n",
        "To interact with the LongMission API, you can send HTTP requests to the appropriate endpoints using your preferred programming language or API client.\n",
        "\n",
        "<code><pre>\n",
        "Example Usage in Python\n",
        "\n",
        "import requests\n",
        "\n",
        "# Retrieve all outcomes\n",
        "response = requests.get('https://api.example.com/outcomes')\n",
        "outcomes = response.json()\n",
        "print(outcomes)\n",
        "\n",
        "# Create a new mission\n",
        "data = {\n",
        "    'title': 'New Mission',\n",
        "    'description': 'This is a new mission',\n",
        "    'outcome_id': '123456'\n",
        "}\n",
        "response = requests.post('https://api.example.com/missions', json=data)\n",
        "new_mission = response.json()\n",
        "print(new_mission)\n",
        "</pre></code>\n",
        "\n",
        "Make sure to replace <code>https://api.example.com</code> with the actual URL of the LongMission API.\n",
        "\n",
        "For more details on the request payloads and responses, please refer to the API documentation or consult the LongMission API reference.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add Data"
      ],
      "metadata": {
        "id": "qvxky8RF8fVx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "6020fb1c507f41579a4785a6746acfee",
            "e1ab55771f6e40f48b8e4bdcc696163b",
            "20d9be9c1ef2492ab16c5d336c2e7a11"
          ]
        },
        "id": "8kQz3urP7U52",
        "outputId": "9e931205-9772-41d2-d738-68d309e85a77"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "FileUpload(value={}, description='Upload JSON File')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6020fb1c507f41579a4785a6746acfee"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "import json\n",
        "\n",
        "# Upload Widget\n",
        "upload_widget = widgets.FileUpload(description='Upload JSON File')\n",
        "\n",
        "# Output Widget\n",
        "output_widget = widgets.Output()\n",
        "\n",
        "# Upload Handler\n",
        "def upload_handler(change):\n",
        "    with output_widget:\n",
        "        if change['type'] == 'change' and change['name'] == 'value':\n",
        "            content = change['new']\n",
        "            if content:\n",
        "                try:\n",
        "                    json_data = content[list(content.keys())[0]]['content'].decode()\n",
        "                    data = json.loads(json_data)\n",
        "\n",
        "                    # Process JSON data and create objects as needed\n",
        "                    # ...\n",
        "\n",
        "                    print('JSON file uploaded and processed successfully!')\n",
        "                except Exception as e:\n",
        "                    print('Error occurred while processing JSON file:', str(e))\n",
        "\n",
        "# Event Handlers\n",
        "upload_widget.observe(upload_handler, names='value')\n",
        "\n",
        "# Display Widgets\n",
        "display(upload_widget)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "class DataLoader:\n",
        "    def __init__(self):\n",
        "        self.templates = []\n",
        "        self.lm = []\n",
        "\n",
        "    def load_template(self, file_path):\n",
        "        try:\n",
        "            with open(file_path, 'r') as file:\n",
        "                template_data = json.load(file)\n",
        "                template = PromptTemplateBook()\n",
        "                if isinstance(template, PromptTemplateBook):\n",
        "                    self.templates.append(template)\n",
        "                    print(f\"Template loaded successfully: {file_path}\")\n",
        "                else:\n",
        "                    print(f\"Failed to load template from {file_path}. The file does not contain a valid PromptTemplate.\")\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Failed to load template. File not found: {file_path}\")\n",
        "\n",
        "    def save_template(self, template, file_path):\n",
        "        if isinstance(template, PromptTemplateBook):\n",
        "            try:\n",
        "                with open(file_path, 'w') as file:\n",
        "                    template_data = template.to_json()\n",
        "                    json.dump(template_data, file, indent=4)\n",
        "                    print(f\"Template saved successfully: {file_path}\")\n",
        "            except:\n",
        "                print(f\"Failed to save template to {file_path}.\")\n",
        "        else:\n",
        "            print(f\"Invalid template. The template should be an instance of PromptTemplate.\")\n",
        "\n",
        "# Example usage\n",
        "data_loader = DataLoader()\n",
        "template = PromptTemplateBook()\n",
        "data_loader.save_template(template, \"template.json\")\n",
        "data_loader.load_template(\"template.json\")\n"
      ],
      "metadata": {
        "id": "ED8xaI-5miZT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "d06915db-d55e-4499-f83a-99d153c6dd01"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to save template to template.json.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "JSONDecodeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-b5c584237da1>\u001b[0m in \u001b[0;36m<cell line: 37>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mtemplate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPromptTemplateBook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_template\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemplate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"template.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mdata_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_template\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"template.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-34-b5c584237da1>\u001b[0m in \u001b[0;36mload_template\u001b[0;34m(self, file_path)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                 \u001b[0mtemplate_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m                 \u001b[0mtemplate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPromptTemplateBook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemplate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPromptTemplateBook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0mkwarg\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0motherwise\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mJSONDecoder\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mused\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m     \"\"\"\n\u001b[0;32m--> 293\u001b[0;31m     return loads(fp.read(),\n\u001b[0m\u001b[1;32m    294\u001b[0m         \u001b[0mcls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject_hook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject_hook\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0mparse_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_float\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_int\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_int\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "4JMIh92Ty3u-"
      },
      "outputs": [],
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "class ProjectWidget:\n",
        "    def __init__(self):\n",
        "        self.project = None\n",
        "\n",
        "    def create_project(self, path):\n",
        "        self.project = Project(path)\n",
        "        self.project.create_project()\n",
        "        print(\"New project created.\")\n",
        "\n",
        "    def load_project(self, path):\n",
        "        self.project = Project(path)\n",
        "        self.project.load_project()\n",
        "        print(\"Project loaded.\")\n",
        "\n",
        "    def save_project(self):\n",
        "        if self.project:\n",
        "            self.project.save_project()\n",
        "            print(\"Project saved.\")\n",
        "        else:\n",
        "            print(\"No project loaded.\")\n",
        "\n",
        "    def display_widget(self):\n",
        "        load_button = widgets.Button(description=\"Load Project\")\n",
        "        create_button = widgets.Button(description=\"Create Project\")\n",
        "        save_button = widgets.Button(description=\"Save Project\")\n",
        "\n",
        "        load_text = widgets.Text(description=\"Project Path:\")\n",
        "        create_text = widgets.Text(description=\"Project Path:\")\n",
        "\n",
        "        load_button.on_click(lambda _: self.load_project(load_text.value))\n",
        "        create_button.on_click(lambda _: self.create_project(create_text.value))\n",
        "        save_button.on_click(lambda _: self.save_project())\n",
        "\n",
        "        load_box = widgets.HBox([load_text, load_button])\n",
        "        create_box = widgets.HBox([create_text, create_button])\n",
        "        save_box = widgets.HBox([save_button])\n",
        "\n",
        "        widget_box = widgets.VBox([load_box, create_box, save_box])\n",
        "        widget_title = widgets.HTML(\"<h2>Project</h2>\")\n",
        "\n",
        "        display(widget_title, widget_box)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "2kkq5b3fUaJt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e726bad1-e617-4664-b941-f35edc56df71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Project saved successfully.\n",
            "Project saved successfully.\n",
            "Outcome added to the project.\n",
            "Project loaded successfully.\n",
            "Project saved successfully.\n",
            "Outcome added to the project.\n"
          ]
        }
      ],
      "source": [
        "# Create a new project and add an outcome\n",
        "project = Project('project.json')\n",
        "project.create_project()\n",
        "\n",
        "outcome1 = {\n",
        "    'title': 'Outcome 1',\n",
        "    'description': 'This is the first outcome'\n",
        "}\n",
        "project.add_outcome(outcome1)\n",
        "\n",
        "# Load an existing project and add another outcome\n",
        "project.load_project()\n",
        "\n",
        "outcome2 = {\n",
        "    'title': 'Outcome 2',\n",
        "    'description': 'This is the second outcome'\n",
        "}\n",
        "project.add_outcome(outcome2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "911ipVJRpbAw"
      },
      "outputs": [],
      "source": [
        "p = ProjectWidget()\n",
        "p.display_widget()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDnt--605deT"
      },
      "source": [
        "#Prompts"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data"
      ],
      "metadata": {
        "id": "oUiVtyJoW8NQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gD5AdVDbXULw"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_prompt_book():\n",
        "    # Create a PromptBook instance\n",
        "    prompt_book1 = PromptBook()\n",
        "\n",
        "    # Add prompts to the PromptBook\n",
        "    prompt_id1 = prompt_book1.add_prompt(\n",
        "        name=\"Prompt 1\",\n",
        "        description=\"Description for Prompt 1\",\n",
        "        prompt_template=\"I am working on a {wanted_outcome}, I want {details}, how can I use the {pattern} here? {pattern_def}\",\n",
        "        meta_info=\"Meta Info for Prompt 1\"\n",
        "    )\n",
        "\n",
        "    prompt_id2 = prompt_book1.add_prompt(\n",
        "        name=\"Prompt 2\",\n",
        "        description=\"Description for Prompt 2\",\n",
        "        prompt_template=\"Can you explain how {details} work in the context of {pattern}?\",\n",
        "        meta_info=\"Meta Info for Prompt 2\"\n",
        "    )\n",
        "\n",
        "    # Print all prompts in the PromptBook\n",
        "    prompt_book1.print_prompts()\n",
        "\n",
        "    # Update a prompt\n",
        "    prompt_book1.update_prompt(prompt_id1, name=\"Updated Prompt 1\", description=\"Updated Description for Prompt 1\")\n",
        "\n",
        "    # Print all prompts after update\n",
        "    prompt_book1.print_prompts()\n",
        "\n",
        "    # Serialize and save the PromptBook to a file\n",
        "    file_path = \"prompt_book_data.json\"\n",
        "    prompt_book1.save_to_file(file_path)\n",
        "\n",
        "    # Load the PromptBook from the file\n",
        "    prompt_book2 = PromptBook.load_from_file(file_path)\n",
        "\n",
        "    # Print all prompts in the loaded PromptBook\n",
        "    prompt_book2.print_prompts()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test_prompt_book()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGUOt-E9bV6T",
        "outputId": "698634bc-c702-4376-b3b5-1ecaa4676498"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt 1: Prompt 1 - Description for Prompt 1\n",
            "Prompt 2: Prompt 2 - Description for Prompt 2\n",
            "Prompt 1: Updated Prompt 1 - Updated Description for Prompt 1\n",
            "Prompt 2: Prompt 2 - Description for Prompt 2\n",
            "Prompt 1: Updated Prompt 1 - Updated Description for Prompt 1\n",
            "Prompt 2: Prompt 2 - Description for Prompt 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import uuid\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "class PromptTemplate:\n",
        "    def __init__(self, template, variables=None):\n",
        "        self.template = template\n",
        "        self.variables = variables or {}\n",
        "        self.templates = {}\n",
        "\n",
        "    def set_variable(self, name, value):\n",
        "        self.variables[name] = value\n",
        "\n",
        "    def get_variable(self, name):\n",
        "        return self.variables.get(name)\n",
        "\n",
        "    def set_template(self, template):\n",
        "        self.template = template\n",
        "\n",
        "    def get_template(self):\n",
        "        return self.template\n",
        "\n",
        "    def generate_prompt(self):\n",
        "        prompt = self.template\n",
        "        for name, value in self.variables.items():\n",
        "            prompt = prompt.replace(f\"{{{name}}}\", str(value))\n",
        "        return prompt\n",
        "\n",
        "    def add_template(self, key, template):\n",
        "        self.templates[key] = template\n",
        "\n",
        "    def printMeta(self):\n",
        "        print(f\"Number of saved PromptTemplates: {len(self.templates)}\")\n",
        "\n",
        "class PromptTemplateBook:\n",
        "    def __init__(self):\n",
        "        self.templates = {}\n",
        "\n",
        "    def add_template(self, key, template):\n",
        "        self.templates[key] = template\n",
        "\n",
        "    def printMeta(self):\n",
        "        print(f\"Number of saved PromptTemplates: {len(self.templates)}\")\n",
        "\n",
        "    def __str__(self):\n",
        "        output = \"PromptTemplateBook Contents:\\n\"\n",
        "        for key, value in self.templates.items():\n",
        "            output += f\"Template Key: {key}\\n\"\n",
        "            output += f\"Template Value: {value.template}\\n\"\n",
        "            output += f\"Template Variables: {value.variables}\\n\\n\"\n",
        "        return output\n",
        "\n",
        "\n",
        "class PCPrompt:\n",
        "    def __init__(self, template='', variable_names=None):\n",
        "        self.id = str(uuid.uuid4())\n",
        "        self.variables = {name: None for name in variable_names or []}\n",
        "        self.template = template\n",
        "\n",
        "    def set_variable(self, name, value):\n",
        "        if name in self.variables:\n",
        "            self.variables[name] = value\n",
        "        else:\n",
        "            raise ValueError(f\"Variable '{name}' is not defined in the prompt\")\n",
        "\n",
        "    def generate_prompt(self):\n",
        "        prompt = self.template\n",
        "        for name, value in self.variables.items():\n",
        "            prompt = prompt.replace(f\"{{{name}}}\", str(value))\n",
        "        return prompt\n",
        "\n",
        "    def format_prompt(self, **variable_values):\n",
        "        prompt = self.template\n",
        "        for name, value in variable_values.items():\n",
        "            if name in self.variables:\n",
        "                prompt = prompt.replace(f\"{{{name}}}\", str(value))\n",
        "            else:\n",
        "                raise ValueError(f\"Variable '{name}' is not defined in the prompt\")\n",
        "        return prompt\n",
        "\n",
        "import json\n",
        "import uuid\n",
        "\n",
        "class PromptBook:\n",
        "    def __init__(self):\n",
        "        self.id = str(uuid.uuid4())\n",
        "        self.saved_prompts = {}\n",
        "\n",
        "    def print_prompts(self):\n",
        "        for prompt_id, prompt_data in self.saved_prompts.items():\n",
        "            print(f\"Prompt {prompt_id}: {prompt_data['name']} - {prompt_data['description']}\")\n",
        "\n",
        "    def add_prompt(self, name, description, prompt_template, meta_info,**kwargs):\n",
        "        prompt_id = len(self.saved_prompts) + 1\n",
        "        self.saved_prompts[prompt_id] = {'name': name, 'description': description, 'prompt_template': prompt_template, 'meta_info': meta_info,**kwargs}\n",
        "        return prompt_id\n",
        "\n",
        "    def remove_prompt(self, prompt_id):\n",
        "        if prompt_id in self.saved_prompts:\n",
        "            del self.saved_prompts[prompt_id]\n",
        "        else:\n",
        "            print(f\"Prompt with ID {prompt_id} not found.\")\n",
        "\n",
        "    def update_prompt(self, prompt_id, name=None, description=None, prompt_template=None, meta_info=None):\n",
        "        if prompt_id in self.saved_prompts:\n",
        "            prompt_data = self.saved_prompts[prompt_id]\n",
        "            if name is not None:\n",
        "                prompt_data['name'] = name\n",
        "            if description is not None:\n",
        "                prompt_data['description'] = description\n",
        "            if prompt_template is not None:\n",
        "                prompt_data['prompt_template'] = prompt_template\n",
        "            if meta_info is not None:\n",
        "                prompt_data['meta_info'] = meta_info\n",
        "        else:\n",
        "            print(f\"Prompt with ID {prompt_id} not found.\")\n",
        "\n",
        "\n",
        "    @classmethod\n",
        "    def format(cls, prompt_template, **kwargs):\n",
        "        formatted_prompt = prompt_template.format(**kwargs)\n",
        "        return formatted_prompt\n",
        "\n",
        "\n",
        "\n",
        "        return prompt\n",
        "\n",
        "    def serialize(self):\n",
        "        return json.dumps({'id': self.id, 'saved_prompts': self.saved_prompts})\n",
        "\n",
        "    @classmethod\n",
        "    def deserialize(cls, serialized_data):\n",
        "        data = json.loads(serialized_data)\n",
        "        prompt_book = cls()\n",
        "        prompt_book.id = data['id']\n",
        "        prompt_book.saved_prompts = data['saved_prompts']\n",
        "        return prompt_book\n",
        "\n",
        "    def save_to_file(self, file_path):\n",
        "        with open(file_path, 'w') as file:\n",
        "            serialized_data = self.serialize()\n",
        "            file.write(serialized_data)\n",
        "\n",
        "    @classmethod\n",
        "    def load_from_file(cls, file_path):\n",
        "        with open(file_path, 'r') as file:\n",
        "            serialized_data = file.read()\n",
        "            return cls.deserialize(serialized_data)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "031F1iYvXvL-"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template_book = PromptTemplateBook()\n",
        "prompt_book = PromptBook()"
      ],
      "metadata": {
        "id": "9DgsJ0dbW72I"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import Prompt\n",
        "\n",
        "pattern_ideas_tmp = 'I am working on a {wanted_outcome}, I want {details}, how can I use the {pattern} here? {pattern_def}'\n",
        "\n",
        "prompt = Prompt.from_template(pattern_ideas_tmp)\n",
        "context = {\n",
        "    'wanted_outcome': 'chatbot project',\n",
        "    'details': 'to integrate with a messaging app',\n",
        "    'pattern': 'Flipped Interaction pattern',\n",
        "    'pattern_def': 'This pattern involves flipping the conversation flow...'\n",
        "}\n",
        "\n",
        "generated_prompt = prompt.format(wanted_outcome='chatbot project',\n",
        "                                        details ='to integrate with a messaging app',\n",
        "                                        pattern= 'Flipped Interaction pattern',\n",
        "                                        pattern_def = 'This pattern involves flipping the conversation flow...'\n",
        "                                        )\n",
        "print(generated_prompt)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "id": "Imv7Y3JQLOWB",
        "outputId": "89d38577-7082-4820-90af-19c9d8334e0c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-f4b9f3c34556>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPrompt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpattern_ideas_tmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'I am working on a {wanted_outcome}, I want {details}, how can I use the {pattern} here? {pattern_def}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPrompt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_template\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern_ideas_tmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import Prompt,PromptTemplate\n",
        "\n",
        "pattern_ideas_tmp = 'I am working on a {wanted_outcome}, I want {details}, how can I use the {pattern} here? {pattern_def}'\n",
        "\n",
        "prompt = PromptTemplate(template=pattern_ideas_tmp,input_variables=['wanted_outcome','details','pattern','pattern_def'])\n",
        "\n",
        "generated_prompt = prompt.format(wanted_outcome='chatbot project',\n",
        "                                        details ='to integrate with a messaging app',\n",
        "                                        pattern= 'Flipped Interaction pattern',\n",
        "                                        pattern_def = 'This pattern involves flipping the conversation flow...'\n",
        "                                        )\n",
        "print(generated_prompt)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eg371LCFM4vT",
        "outputId": "c75f97fc-41b5-4324-985f-c443c431ec18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am working on a chatbot project, I want to integrate with a messaging app, how can I use the Flipped Interaction pattern here? This pattern involves flipping the conversation flow...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage of PromptTemplateBook\n",
        "template_book = PromptTemplateBook()\n",
        "\n",
        "# Add templates to the book using add_template method\n",
        "template_book.add_template(\"1\", PromptTemplate(\"Hello, {name}!\", variables={\"name\": \"John\"}))\n",
        "template_book.add_template(\"2\", PromptTemplate(\"Welcome to {location}.\", variables={\"location\": \"New York\"}))\n",
        "\n",
        "# Print the contents of PromptTemplateBook\n",
        "print(template_book)"
      ],
      "metadata": {
        "id": "9FE50GiYY9Rv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "outputId": "b431c079-d731-4132-dc4a-c3dbf361742a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-4096481791ad>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Example usage of PromptTemplateBook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtemplate_book\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPromptTemplateBook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Add templates to the book using add_template method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtemplate_book\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_template\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPromptTemplate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Hello, {name}!\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"name\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"John\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'PromptTemplateBook' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the template and variable names as keyword arguments\n",
        "pattern_ideas_tmp = 'I am working on a {wanted_outcome}, I want {details}, how can I use the {pattern} here? {pattern_def}'\n",
        "variable_names = ['wanted_outcome', 'details', 'pattern', 'pattern_def']\n",
        "\n",
        "# Create a Prompt object with the template and variable names\n",
        "prompt = PCPrompt(template=pattern_ideas_tmp, variable_names=variable_names)\n",
        "\n",
        "# Format the prompt with the variable values using the 'format_prompt' method\n",
        "formatted_prompt = prompt.format_prompt(wanted_outcome='chatbot project', details='to integrate with a messaging app', pattern='Flipped Interaction pattern', pattern_def='This pattern involves flipping the conversation flow...')\n",
        "\n",
        "# Print the formatted prompt\n",
        "print(formatted_prompt)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYM_Do_8k_rg",
        "outputId": "fb9ae29e-061f-4d34-df51-0db085b4e50b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am working on a chatbot project, I want to integrate with a messaging app, how can I use the Flipped Interaction pattern here? This pattern involves flipping the conversation flow...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import uuid\n",
        "import ipywidgets as widgets\n",
        "\n",
        "class PromptWidget:\n",
        "    def __init__(self):\n",
        "        self.template_input = widgets.Textarea(description='Template:')\n",
        "        self.variable_names_input = widgets.Text(description='Variable Names (comma-separated):')\n",
        "        self.create_button = widgets.Button(description='Create')\n",
        "        self.output_area = widgets.Output()\n",
        "\n",
        "        self.create_button.on_click(self.create_prompt)\n",
        "\n",
        "        self.widget = widgets.VBox([\n",
        "            self.template_input,\n",
        "            self.variable_names_input,\n",
        "            self.create_button,\n",
        "            self.output_area\n",
        "        ])\n",
        "\n",
        "    def create_prompt(self, _):\n",
        "        template = self.template_input.value\n",
        "        variable_names = [name.strip() for name in self.variable_names_input.value.split(',')]\n",
        "        prompt = Prompt(template=template, variable_names=variable_names)\n",
        "\n",
        "        formatted_prompt = prompt.generate_prompt()\n",
        "\n",
        "        with self.output_area:\n",
        "            print(formatted_prompt)\n"
      ],
      "metadata": {
        "id": "tdhifOybnNKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##nav"
      ],
      "metadata": {
        "id": "HTDl6t1G9ym0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we should show the user the number of variables we parse from the template and also give them a button called variable_names that will be an input text box with the value of the parsed variable. when the user presses the create button, initialize a prompt with that template and the variable names passed in."
      ],
      "metadata": {
        "id": "A1jfsH55rXd6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the functionality we want here to couple th template and prompt classes a bit more tighty. pattern will be pasting in the template and from there we pass in variable names, or read them from the template. finally we generate the prompt with the variable values passed in.\n",
        "\n",
        "\n",
        "SHould we keep the template and prompt separate?\n",
        "\n",
        "a prompt has the values resolved, whiel a tempalte has the placeholders. so well use mutiple classes, but one wiget"
      ],
      "metadata": {
        "id": "fTzoGcgdpv7X"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZCZY1neW1Bk"
      },
      "source": [
        "## Template Builder\n",
        "\n",
        "showing the template like this for readability"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "templates = PromptTemplateBook()"
      ],
      "metadata": {
        "id": "xCxroGwuI7h_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pattern_ideas_tmp = 'I am working on a {wanted_outcome}, I want {details}, how can I use the {pattern} here? {pattern_def}'\n"
      ],
      "metadata": {
        "id": "4FGMC2u5JUdQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the user input textarea\n",
        "user_input = widgets.Textarea(\n",
        "    value=\"\",\n",
        "    placeholder='Type here...',\n",
        "    description='',\n",
        "    layout=widgets.Layout(width='95%', height='auto', background='white'),\n",
        ")\n",
        "\n",
        "# Create the output text widget\n",
        "output_text = widgets.Textarea(\n",
        "    value=\"\",\n",
        "    description='',\n",
        "    layout=widgets.Layout(width='95%', height='auto', justify_items=\"center\"),\n",
        ")\n",
        "\n",
        "# Create the \"Fill\" button\n",
        "fill_button = widgets.Button(\n",
        "    description='Fill',\n",
        "    button_style='success',\n",
        "    layout=widgets.Layout(width='auto', justify_items=\"center\")\n",
        ")\n",
        "\n",
        "# Create the \"Process\" button\n",
        "process_button = widgets.Button(\n",
        "    description='Process',\n",
        "    button_style='danger',\n",
        "    layout=widgets.Layout(width='auto', justify_items=\"center\")\n",
        ")\n",
        "\n",
        "# Create the \"Create\" button\n",
        "create_button = widgets.Button(\n",
        "    description='Create',\n",
        "    button_style='info',\n",
        "    layout=widgets.Layout(width='auto', justify_items=\"center\")\n",
        ")\n",
        "\n",
        "# Create the \"Clear\" button\n",
        "clear_button = widgets.Button(\n",
        "    description='Clear',\n",
        "    button_style='warning',\n",
        "    layout=widgets.Layout(width='auto', justify_items=\"center\")\n",
        ")\n",
        "\n",
        "# Create the \"Show Stats\" button\n",
        "show_stats_button = widgets.Button(\n",
        "    description='Show Stats',\n",
        "    button_style='info',\n",
        "    layout=widgets.Layout(width='auto', justify_items=\"center\")\n",
        ")\n",
        "\n",
        "def clear_input(button):\n",
        "    user_input.value = \"\"  # Clear the input textbox\n",
        "    output_text.value = \"\"  # Clear the output textbox\n",
        "    for input_box in input_boxes:\n",
        "        input_box.value = \"\"  # Clear all input boxes in the grid\n",
        "\n",
        "\n",
        "def update_input_height(change):\n",
        "    # Function to update the height of the input box\n",
        "    lines = change['new'].count('\\n') + 1\n",
        "    lineHeight = 16  # Adjust this value to control the line height\n",
        "    newRows = max(lines, 2)  # Ensure a minimum of 2 rows\n",
        "    maxHeight = 400\n",
        "    newHeight = newRows * lineHeight\n",
        "    if newHeight > maxHeight:\n",
        "        user_input.layout.overflow_y = \"scroll\"\n",
        "        user_input.layout.height = f\"{maxHeight}px\"\n",
        "    else:\n",
        "        user_input.layout.overflow_y = \"hidden\"\n",
        "        user_input.layout.height = f\"{newHeight}px\"\n",
        "\n",
        "\n",
        "def fill_template(button):\n",
        "    template = user_input.value\n",
        "    placeholders = re.findall(r'{(.*?)}', template)\n",
        "    for i, placeholder in enumerate(placeholders):\n",
        "        variable = input_boxes[i].value\n",
        "        template = template.replace(f'{{{placeholder}}}', variable)\n",
        "    output_text.value = template\n",
        "\n",
        "def generate_input_boxes(button):\n",
        "    template = user_input.value\n",
        "    placeholders = re.findall(r'{(.*?)}', template)\n",
        "    input_grid.children = []  # Clear the previous input text boxes\n",
        "    input_boxes.clear()  # Clear the array of input text boxes\n",
        "    for placeholder in placeholders:\n",
        "        input_box = widgets.Text(\n",
        "            placeholder=f'Enter value for {placeholder}',\n",
        "            layout=widgets.Layout(width='100%')\n",
        "        )\n",
        "        input_grid.children += (input_box,)\n",
        "        input_boxes.append(input_box)  # Add the input box to the array\n",
        "\n",
        "mytemplates = {}\n",
        "\n",
        "def create_template(button):\n",
        "    content = user_input.value\n",
        "    if content:\n",
        "        template_id = str(uuid.uuid4())\n",
        "        template = PromptTemplateBook(content)\n",
        "        template.add_template(template_id, template)\n",
        "        mytemplates[template_id] = template\n",
        "        print(f\"PromptTemplate with ID '{template_id}' has been saved.\")\n",
        "    else:\n",
        "        print(\"No content in the input textbox. Cannot create a PromptTemplate.\")\n",
        "\n",
        "def show_stats(button):\n",
        "   print(mytemplates)\n",
        "\n",
        "# Observe changes in the user input textarea\n",
        "user_input.observe(update_input_height, names='value')\n",
        "\n",
        "# Register event handlers for the buttons\n",
        "fill_button.on_click(fill_template)\n",
        "process_button.on_click(generate_input_boxes)\n",
        "clear_button.on_click(clear_input)\n",
        "create_button.on_click(create_template)\n",
        "show_stats_button.on_click(show_stats)\n",
        "\n",
        "# Create a list to store generated input text boxes\n",
        "input_boxes = []\n",
        "\n",
        "# Create a grid container for input text boxes\n",
        "input_grid = widgets.GridBox(\n",
        "    layout=widgets.Layout(grid_template_columns='repeat(4, 1fr)', grid_gap='10px')\n",
        ")\n",
        "\n",
        "# Display the widgets with the \"Clear\" button and \"Show Stats\" button\n",
        "widgets.VBox([widgets.HBox([fill_button, process_button, clear_button, create_button, show_stats_button]), user_input, input_grid, output_text])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "376cd3a515fb4ac2b517dddd2b450c93",
            "26757bc60dd843ad93a4fd7c81958633",
            "ae9bcee047d6405d8e735cf7d4dba280",
            "db692baf6346460988771983b5867c42",
            "86552ce9592f4cd69e6b18711f827023",
            "d48371e748ac42dc8a919e39c480464c",
            "71f84eab2ef9470b91c108e49476a34e",
            "8bc3517c511e474dab7aafca97c0f995",
            "acf2024b389843869ccc45d68dc01e36",
            "9320a0a5b69b46ed8939e3d616ca7682",
            "14f856785e2f4bdd9d2aa4eca6835fd3",
            "8c74589a7bc64ebb8ab48b1c893f8748",
            "d84295db45004e9f9fa15b496eeffadd",
            "01170ada0ca74fd486d1d86540ecbee8",
            "eac6d32c0c544f0d913b8b0ee7620f23",
            "b402f639135849aa87ac731b0822ea1f",
            "7e27ea8df3144f53ad2f17fcb204fb0f",
            "782aab3437e14d62ad52f964d2ab14f9",
            "0af73aaddf004a20a0c8e1d6da2d79f6",
            "17d3636cfc074f29beae8c396a9d7a3a",
            "0c44f02a823e4e15973fa6e3424ba033",
            "b15fb842c8a44eabb2914a880146a5d4",
            "41c3b6495cc14505ad93e5b6947794a4",
            "9ebd6caf14724ef096e9e8010441fc25",
            "bc0abc8c1f864e0e8ec5031c46f864d4",
            "810994e4c6ec4f3781e1523b5bc83a8b",
            "500591327a114028b52fd796068e5cbd"
          ]
        },
        "id": "-Icmb8DuCcC1",
        "outputId": "e974cf6e-4f15-4cf0-b7aa-382bb9af62a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HBox(children=(Button(button_style='success', description='Fill', layout=Layout(justify_items='…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "376cd3a515fb4ac2b517dddd2b450c93"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(mytemplates['e69ff097-ed97-4aea-a770-87e4b5858e0f'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5-7P46-NQA5",
        "outputId": "db60df45-2eea-4051-8345-aeb9f37113eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PromptTemplateBook Contents:\n",
            "Template Key: e69ff097-ed97-4aea-a770-87e4b5858e0f\n",
            "Template Value: Dvdvdvsv\n",
            "Template Variables: {}\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53,
          "referenced_widgets": [
            "8c4726d5cf084b7aaeed3664c0462e9d",
            "4efc96ddbf21446bb43f769812d64a37"
          ]
        },
        "id": "wVOdnl6g7Aa9",
        "outputId": "534b1a11-b894-40b4-dd28-db61ffb16103"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8c4726d5cf084b7aaeed3664c0462e9d"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "class MetaStats:\n",
        "    def __init__(self):\n",
        "        self.output_widget = widgets.Output()\n",
        "\n",
        "    def get_stats(self, stats_dict):\n",
        "        with self.output_widget:\n",
        "            clear_output()\n",
        "            line = []\n",
        "            for key, value in stats_dict.items():\n",
        "                line.append(f\"{key}: {value} | \")\n",
        "            print(\"\".join(line))\n",
        "\n",
        "    def add_stats(self, stats_dict):\n",
        "        with self.output_widget:\n",
        "            line = []\n",
        "            for key, value in stats_dict.items():\n",
        "                line.append(f\"{key}: {value} | \")\n",
        "            print(\"\".join(line))\n",
        "\n",
        "# Example usage\n",
        "meta_stats = MetaStats()\n",
        "stats1 = {\"Name\": \"John Doe\", \"Age\": 30, \"Occupation\": \"Engineer\"}\n",
        "stats2 = {\"Location\": \"New York\", \"Hobbies\": [\"Reading\", \"Coding\", \"Traveling\"]}\n",
        "\n",
        "# Call get_stats method to display the initial stats\n",
        "meta_stats.get_stats(stats1)\n",
        "\n",
        "# Call add_stats method to display additional stats\n",
        "meta_stats.add_stats(stats2)\n",
        "\n",
        "# Display the output widget\n",
        "display(meta_stats.output_widget)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsFYIVJhKbFi"
      },
      "source": [
        "### Improve Prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KDIM81WZWa_u"
      },
      "outputs": [],
      "source": [
        "\"we'll eventually add a scorer to analyze the prompt and how effective it is\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKjygO_yz_rD"
      },
      "source": [
        "## Saved Prompts\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jylbt2c2K6nN"
      },
      "source": [
        "goal now is to write functions to save and load prompts after we create the saved ones\n",
        "\n",
        "\n",
        "we'll also need to edit the code for the different patterns so that it is printing out properly\n",
        "\n",
        "for the first test prompt that asks why, we can encapsulate that in a fuction that is\n",
        "attached to each metapattern. we'll add this prompt to the savedprompts dict,\n",
        "\n",
        "pattern.how"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vck8ZFFaJxuM"
      },
      "outputs": [],
      "source": [
        "import ipywidgets as widgets\n",
        "\n",
        "class PromptBookWidget:\n",
        "    def __init__(self):\n",
        "        self.prompt_book = PromptBook()\n",
        "\n",
        "        self.output = widgets.Output()\n",
        "\n",
        "        self.name_input = widgets.Text(description=\"Name:\")\n",
        "        self.description_input = widgets.Text(description=\"Description:\")\n",
        "        self.template_input = widgets.Textarea(description=\"Prompt Template:\")\n",
        "        self.meta_info_input = widgets.Textarea(description=\"Meta Info:\")\n",
        "        self.add_prompt_button = widgets.Button(description=\"Add Prompt\")\n",
        "        self.prompt_id_input = widgets.IntText(description=\"Prompt ID:\", value=1)\n",
        "        self.remove_prompt_button = widgets.Button(description=\"Remove Prompt\")\n",
        "        self.update_prompt_button = widgets.Button(description=\"Update Prompt\")\n",
        "\n",
        "        self.add_prompt_button.on_click(self.add_prompt)\n",
        "        self.remove_prompt_button.on_click(self.remove_prompt)\n",
        "        self.update_prompt_button.on_click(self.update_prompt)\n",
        "\n",
        "        self.display_prompts()\n",
        "\n",
        "        self.widgets = widgets.VBox([\n",
        "            self.name_input,\n",
        "            self.description_input,\n",
        "            self.template_input,\n",
        "            self.meta_info_input,\n",
        "            self.add_prompt_button,\n",
        "            self.prompt_id_input,\n",
        "            self.remove_prompt_button,\n",
        "            self.update_prompt_button,\n",
        "            self.output\n",
        "        ])\n",
        "\n",
        "    def add_prompt(self, _):\n",
        "        name = self.name_input.value\n",
        "        description = self.description_input.value\n",
        "        template = self.template_input.value\n",
        "        meta_info = self.meta_info_input.value\n",
        "        prompt_id = self.prompt_book.add_prompt(name, description, template, meta_info)\n",
        "        self.display_prompts()\n",
        "        with self.output:\n",
        "            print(f\"Added prompt with ID {prompt_id}\")\n",
        "\n",
        "    def remove_prompt(self, _):\n",
        "        prompt_id = self.prompt_id_input.value\n",
        "        self.prompt_book.remove_prompt(prompt_id)\n",
        "        self.display_prompts()\n",
        "        with self.output:\n",
        "            print(f\"Removed prompt with ID {prompt_id}\")\n",
        "\n",
        "    def update_prompt(self, _):\n",
        "        prompt_id = self.prompt_id_input.value\n",
        "        name = self.name_input.value\n",
        "        description = self.description_input.value\n",
        "        template = self.template_input.value\n",
        "        meta_info = self.meta_info_input.value\n",
        "        self.prompt_book.update_prompt(prompt_id, name, description, template, meta_info)\n",
        "        self.display_prompts()\n",
        "        with self.output:\n",
        "            print(f\"Updated prompt with ID {prompt_id}\")\n",
        "\n",
        "    def display_prompts(self):\n",
        "        with self.output:\n",
        "            self.output.clear_output()\n",
        "            self.prompt_book.print_prompts()\n",
        "\n",
        "    def get_widget(self):\n",
        "        return self.widgets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309,
          "referenced_widgets": [
            "658bf7f790904c4d809ffc8a814d4a63",
            "5dc1b8bddd1b42bfaa93827bd3e913ef",
            "470c493e65734754a2266c330aa4297c",
            "3b729c6404a84529b2adf0eccb5964e2",
            "e3a7703861d04dd891966d067070f801",
            "b2e3799cc01f4be4adcd2d2b3a121f47",
            "6aa8f903752e4d78b3b641a7b044dadd",
            "496b0391f70c44338f232e3e702c8d8d",
            "37966874b3134c6783d957224c64b8ef",
            "993630ff140043e6b0eb160a1b39d34e",
            "2fb00fd3db554d9d9d08558752f19435",
            "9602682f7562498c8c1f49e976f07e89",
            "235f0f040f584ace8c99f73c4266c601",
            "89ed21b5a3134b1a82112163cb1e368e",
            "55b7e7c3b1cc41cab4a9ea26f2600e9a",
            "31b703840cc249bebb1e8ccd211e2a1c",
            "47bd2726bb4d4b82b94ceab41fcf3b05",
            "1b83f17d455d4a8da3896979f22ddb17",
            "932ee962841c493292c27dccddbdd2b6",
            "0c48e0317cd04640945e2b42ebfc409d",
            "4a6e25e8b15a4a0f870211394f01ff9c",
            "863a5feaf6804207b32e9a343eeb2450",
            "8858dd5401f04b53831fbe1d5bf4c344",
            "f80ac2da957e485cb678a0aec58e7534",
            "00ed0fb9262e42aaabf0afd2ff329435",
            "eed6139aeea64d4bba23a8097c377ebd",
            "7ba824377a464d87a2859c6806f05b5c",
            "2e18554797954a5f83a4a286aabfedee"
          ]
        },
        "id": "Fo5uWjrlKob4",
        "outputId": "852fd974-fedf-49df-ee15-73660d1dba92"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "658bf7f790904c4d809ffc8a814d4a63",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Text(value='', description='Name:'), Text(value='', description='Description:'), Textarea(value…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    prompt_widget = PromptBookWidget()\n",
        "    display(prompt_widget.get_widget())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eK1_NP3_sRCE"
      },
      "outputs": [],
      "source": [
        "prompt_widget.prompt_book.print_prompts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGJS2gpZJuZV"
      },
      "source": [
        "## Prompt Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KbuKQsLNuEuZ"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import AIMessagePromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate, ChatMessagePromptTemplate, MessagesPlaceholder, ChatPromptTemplate\n",
        "\n",
        "class LC:\n",
        "    def __init__(self,prompt_book=None):\n",
        "        self.pb = prompt_book  # Create an instance of SavedPrompts\n",
        "\n",
        "#  def create_ai_prompt_template(self, template, input_variables, output_parser=None, partial_variables=None,\n",
        "#                                   template_format=\"f-string\", validate_template=True):\n",
        "\n",
        "\n",
        "    # Method to create AI Prompt Template\n",
        "    def create_ai_prompt_template(self, template, input_variables, output_parser=None, partial_variables=None,\n",
        "                                  template_format=\"f-string\", validate_template=True):\n",
        "\n",
        "        return AIMessagePromptTemplate.from_template(template,template_format=template_format)\n",
        "\n",
        "    # Method to format AI Prompt Template\n",
        "    def format_ai_template(self, ai_template, **kwargs):\n",
        "        return ai_template.format(**kwargs)\n",
        "\n",
        "    # Method to create System Prompt Template\n",
        "    def create_system_prompt_template(self, template, input_variables, output_parser=None, partial_variables=None,\n",
        "                                       template_format=\"f-string\", validate_template=True):\n",
        "        return SystemMessagePromptTemplate.from_template(template,template_format=template_format)\n",
        "\n",
        "    # Method to format System Prompt Template\n",
        "    def format_system_template(self, system_template, **kwargs):\n",
        "        return system_template.format(**kwargs)\n",
        "\n",
        "    # Method to create Human Prompt Template\n",
        "    def create_human_prompt_template(self, template, input_variables, output_parser=None, partial_variables=None,\n",
        "                                       template_format=\"f-string\", validate_template=True):\n",
        "        return HumanMessagePromptTemplate.from_template(template,template_format=template_format)\n",
        "\n",
        "    # Method to format Human Prompt Template\n",
        "    def format_human_template(self, human_template, **kwargs):\n",
        "        return human_template.format(**kwargs)\n",
        "\n",
        "    # Method to create Chat Message Prompt Template\n",
        "    def create_chat_message_prompt_template(self, template, role, template_format=\"f-string\", validate_template=True):\n",
        "        return ChatMessagePromptTemplate.from_template(template=template, role=role)\n",
        "\n",
        "    # Method to format Chat Message Prompt Template\n",
        "    def format_chat_message_template(self, chat_message_template,role, **kwargs):\n",
        "        return chat_message_template.format(role=role,**kwargs)\n",
        "\n",
        "    # Method to create Chat Prompt Template\n",
        "    def create_chat_prompt_template(self, messages):\n",
        "        return ChatPromptTemplate(messages=messages)\n",
        "\n",
        "    # Method to format Chat Prompt Template\n",
        "    def format_chat_template(self, chat_template, **kwargs):\n",
        "        return chat_template.format_prompt(**kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import langchain"
      ],
      "metadata": {
        "id": "rEP7bpdFfFZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4xcZOtsuqKD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3503edd-67a2-4be8-9ad6-f869bb5a5b61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='This is an AI prompt for image captioning.' additional_kwargs={} example=False\n",
            "content='System Message: Hello, world!.' additional_kwargs={}\n",
            "content='Human Message: How can I help you?.' additional_kwargs={} example=False\n",
            "content='Chat Message from AI: Welcome to the chat!.' additional_kwargs={} role='AI'\n",
            "messages=[HumanMessage(content='What is your name?', additional_kwargs={}, example=False), AIMessage(content='My name is ChatGPT.', additional_kwargs={}, example=False)]\n"
          ]
        }
      ],
      "source": [
        "lc = LC()\n",
        "\n",
        "# Test creating and formatting an AI Prompt Template\n",
        "ai_template = lc.create_ai_prompt_template(\"This is an AI prompt for {task}.\", input_variables=[\"task\"])\n",
        "ai_formatted = lc.format_ai_template(ai_template, task=\"image captioning\")\n",
        "print(ai_formatted)\n",
        "\n",
        "# Test creating and formatting a System Prompt Template\n",
        "system_template = lc.create_system_prompt_template(\"System Message: {message}.\",input_variables=['message'])\n",
        "system_formatted = lc.format_system_template(system_template, message=\"Hello, world!\")\n",
        "print(system_formatted)\n",
        "\n",
        "# Test creating and formatting a Human Prompt Template\n",
        "human_template = lc.create_human_prompt_template(\"Human Message: {message}.\",input_variables=['message'])\n",
        "human_formatted = lc.format_human_template(human_template, message=\"How can I help you?\")\n",
        "print(human_formatted)\n",
        "\n",
        "# Test creating and formatting a Chat Message Prompt Template\n",
        "chat_message_template = lc.create_chat_message_prompt_template(\"Chat Message from {role}: {message}.\", role=\"AI\")\n",
        "chat_message_formatted = lc.format_chat_message_template(chat_message_template,message=\"Welcome to the chat!\",role=\"AI\")\n",
        "print(chat_message_formatted)\n",
        "\n",
        "# Test creating and formatting a Chat Prompt Template\n",
        "human_template = lc.create_human_prompt_template(\"What is your name?\",input_variables=[\"name\"])\n",
        "ai_template = lc.create_ai_prompt_template(\"My name is {name}.\", input_variables=[\"name\"])\n",
        "chat_messages = [\n",
        "    lc.format_human_template(human_template),\n",
        "    lc.format_ai_template(ai_template, name=\"ChatGPT\"),\n",
        "    # lc.format_chat_message_template(chat_message_template,message='vdv',role='tester')\n",
        "]\n",
        "chat_prompt = lc.create_chat_prompt_template(messages=chat_messages)\n",
        "chat_formatted = lc.format_chat_template(chat_prompt)\n",
        "print(chat_formatted)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5S0P9bcwm8w"
      },
      "source": [
        "now that we are able to work with the different prompt types. We'll move on to the other stages. once place we can take this is using the metaptterns to improve the prompts and then use langchain to create different prompt types from that. Let's start off with a few propmts to see how we would turn them into messages and eventually serialzie them"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BXgd0z38v_n"
      },
      "source": [
        "now we have the hang of it down well have a look at pipelines propmts and from there we'll move on to creating the propmts from the metapattersn.\n",
        "\n",
        "\n",
        "\n",
        "I am working on a {wanted_outcome}, I want {details}, how can I use the {pattern} here?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kjL1-n3k-q4B"
      },
      "outputs": [],
      "source": [
        "m2 = MetaLanguageCreator()\n",
        "all = m2.print_meta()\n",
        "pb = m2.pb._how\n",
        "temp = 'I am working on a {wanted_outcome}, I want {details}, how can I use the {pattern} here? {pattern_def}'\n",
        "p2 = PromptTemplate(template=temp,input_variables=['wanted_outcome', 'details','pattern','pattern_def'])\n",
        "p2.format(wanted_outcome='wantedoutcomee',details=\"detailssss\",pattern=m2.name,pattern_def=all)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6FWM2T_uQUOn"
      },
      "outputs": [],
      "source": [
        "import langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3las0jAzTK6"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import (\n",
        "    ChatPromptTemplate,\n",
        "    PromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    AIMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "from langchain.schema import (\n",
        "    AIMessage,\n",
        "    HumanMessage,\n",
        "    SystemMessage\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYN41hP4_lQD"
      },
      "outputs": [],
      "source": [
        "from abc import ABC, abstractmethod\n",
        "\n",
        "# Rest of the code remains the same\n",
        "\n",
        "\n",
        "from typing import Any, Dict, List\n",
        "\n",
        "# Define the BaseMessage and BaseMessagePromptTemplate classes\n",
        "class BaseMessage:\n",
        "    def __init__(self, content: str, role: str):\n",
        "        self.content = content\n",
        "        self.role = role\n",
        "\n",
        "class BaseMessagePromptTemplate:\n",
        "    def __init__(self, prompt: str):\n",
        "        self.prompt = prompt\n",
        "        self.messages = []\n",
        "\n",
        "# Define the ChatPromptTemplate class\n",
        "class ChatPromptTemplate(BaseMessagePromptTemplate):\n",
        "    def __init__(self, prompt: str):\n",
        "        super().__init__(prompt)\n",
        "\n",
        "    def from_messages(\n",
        "        cls, messages: List[Dict[str, Any]], **kwargs: Any\n",
        "    ) -> ChatPromptTemplate:\n",
        "        # Instantiate the ChatPromptTemplate class\n",
        "        chat_prompt_template = cls(**kwargs)\n",
        "\n",
        "        # Process the chat messages and add them to the template\n",
        "        for message in messages:\n",
        "            role = message.get(\"role\")\n",
        "            content = message.get(\"content\")\n",
        "            if role and content:\n",
        "                chat_message = BaseMessage(content, role)\n",
        "                chat_prompt_template.messages.append(chat_message)\n",
        "\n",
        "        return chat_prompt_template\n",
        "\n",
        "    def format_messages(self):\n",
        "        # Implement message formatting logic here\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n3p4-EYr0aYO"
      },
      "outputs": [],
      "source": [
        "class BaseMessagePromptTemplate(ABC):\n",
        "    @property\n",
        "    @abstractmethod\n",
        "    def input_variables(self):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def format_messages(self, **kwargs):\n",
        "        pass\n",
        "\n",
        "\n",
        "class BaseMessage(ABC):\n",
        "    @property\n",
        "    @abstractmethod\n",
        "    def type(self):\n",
        "        pass\n",
        "\n",
        "\n",
        "class AIMessage(BaseMessage):\n",
        "    def __init__(self, content):\n",
        "        self.content = content\n",
        "\n",
        "    @property\n",
        "    def type(self):\n",
        "        return \"ai\"\n",
        "\n",
        "    def format(self):\n",
        "        return {\"role\": \"ai\", \"content\": self.content}\n",
        "\n",
        "\n",
        "class HumanMessage(BaseMessage):\n",
        "    def __init__(self, content):\n",
        "        self.content = content\n",
        "\n",
        "    @property\n",
        "    def type(self):\n",
        "        return \"human\"\n",
        "\n",
        "    def format(self):\n",
        "        return {\"role\": \"human\", \"content\": self.content}\n",
        "\n",
        "\n",
        "class SystemMessage(BaseMessage):\n",
        "    def __init__(self, content):\n",
        "        self.content = content\n",
        "\n",
        "    @property\n",
        "    def type(self):\n",
        "        return \"system\"\n",
        "\n",
        "    def format(self):\n",
        "        return {\"role\": \"system\", \"content\": self.content}\n",
        "\n",
        "\n",
        "class ChatPromptTemplate(BaseMessagePromptTemplate):\n",
        "    def __init__(self, messages):\n",
        "        self.messages = messages\n",
        "\n",
        "    @property\n",
        "    def input_variables(self):\n",
        "        input_vars = set()\n",
        "        for msg in self.messages:\n",
        "            input_vars.update(msg.get(\"prompt_template\").input_variables)\n",
        "        return list(input_vars)\n",
        "\n",
        "    def format_messages(self, **kwargs):\n",
        "        formatted_messages = []\n",
        "        for msg in self.messages:\n",
        "            role = msg.get(\"role\")\n",
        "            content = msg.get(\"prompt_template\").format(**kwargs)\n",
        "            if role == \"ai\":\n",
        "                formatted_messages.append(AIMessage(content))\n",
        "            elif role == \"human\":\n",
        "                formatted_messages.append(HumanMessage(content))\n",
        "            elif role == \"system\":\n",
        "                formatted_messages.append(SystemMessage(content))\n",
        "        return formatted_messages\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tR0I74JGtzDl"
      },
      "source": [
        "## Creating custom chat templates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twlZ219o-uYe",
        "outputId": "f162d767-560c-456c-87d9-46de5b2fcf9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SystemMessage(content='You are a helpful assistant that translates English to French.', additional_kwargs={}), HumanMessage(content='I love programming.', additional_kwargs={}, example=False)]\n"
          ]
        }
      ],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "# Define chat messages with required keys (role and content)\n",
        "system_message = {\"role\": \"system\", \"content\": \"You are a helpful assistant that translates {input_language} to {output_language}.\"}\n",
        "human_message = {\"role\": \"human\", \"content\": \"I love programming.\"}\n",
        "template=\"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
        "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
        "human_template=\"{text}\"\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
        "# Build the ChatPromptTemplate from the chat messages\n",
        "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
        "\n",
        "# Get a chat completion from the formatted messages\n",
        "formatted_messages = chat_prompt.format_prompt(input_language=\"English\", output_language=\"French\", text=\"I love programming.\").to_messages()\n",
        "print(formatted_messages)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "od1SBsn7d_TH"
      },
      "source": [
        "## Goals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yTTBDQzFgxq9"
      },
      "outputs": [],
      "source": [
        "class Goal:\n",
        "    def __init__(self, description, status=False):\n",
        "        self.description = description\n",
        "        self.status = status\n",
        "        self.steps = []\n",
        "\n",
        "    def add_step(self, description):\n",
        "        step = Step(description)\n",
        "        self.steps.append(step)\n",
        "\n",
        "    def mark_completed(self):\n",
        "        self.status = True\n",
        "        for step in self.steps:\n",
        "            step.mark_completed()\n",
        "\n",
        "    def __str__(self):\n",
        "        step_str = \"\\n\".join(str(step) for step in self.steps)\n",
        "        return f\"Goal: {self.description} {'(Completed)' if self.status else '(Incomplete)'}\\n{step_str}\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "\n",
        "\n",
        "class GoalWidget:\n",
        "    def __init__(self):\n",
        "        self.description_widget = widgets.Text(\n",
        "            placeholder=\"Enter goal description...\",\n",
        "            description=\"Goal:\",\n",
        "            layout=widgets.Layout(width=\"auto\")\n",
        "        )\n",
        "        self.add_goal_button = widgets.Button(description=\"Add Goal\", button_style=\"success\")\n",
        "        self.goal_output = widgets.Output(layout={'border': '1px solid black'})\n",
        "\n",
        "        # Store created goals\n",
        "        self.goals = []\n",
        "\n",
        "        # Register button click event\n",
        "        self.add_goal_button.on_click(self.add_goal)\n",
        "\n",
        "    def add_goal(self, button):\n",
        "        description = self.description_widget.value.strip()\n",
        "        if description:\n",
        "            goal = Goal(description)\n",
        "            self.goals.append(goal)\n",
        "            self.description_widget.value = \"\"\n",
        "\n",
        "            with self.goal_output:\n",
        "                self.clear_output()\n",
        "                print(\"Goal added successfully.\")\n",
        "                print(str(goal))\n",
        "\n",
        "    def clear_output(self):\n",
        "        self.goal_output.clear_output()\n",
        "\n",
        "    def display(self):\n",
        "        display(self.description_widget)\n",
        "        display(self.add_goal_button)\n",
        "        display(self.goal_output)\n",
        "\n",
        "\n",
        "# Usage Example:\n",
        "\n",
        "goal_widget = GoalWidget()\n",
        "goal_widget.display()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "d833ade0951d453cbaab2280a74267b2",
            "3c1731cdd429457e886d94edfe37e7de",
            "44eb447fdecf4f5c8b3e9ef015e8aa72",
            "ed71b562fa0e4a2a8d97d0e932b5826c",
            "9560a9f8c84242d5b3dd24844bbb091f",
            "3b11bce9b9f345d097b0ec410ca7b2af",
            "96cbe7bfc6b640f59907541330588ad8",
            "a671f2a5d6b442baa24016d64e4b1a93"
          ]
        },
        "id": "ziYhxRWneD9d",
        "outputId": "5085ed07-08da-43ee-9073-93c7ab1a4265"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Text(value='', description='Goal:', layout=Layout(width='auto'), placeholder='Enter goal description...')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d833ade0951d453cbaab2280a74267b2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Button(button_style='success', description='Add Goal', style=ButtonStyle())"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ed71b562fa0e4a2a8d97d0e932b5826c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output(layout=Layout(border='1px solid black'))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "96cbe7bfc6b640f59907541330588ad8"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3_K7y1IeD2L"
      },
      "source": [
        "## Missions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qt7b4XzriFST"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Mission:\n",
        "    def __init__(self, description, status=False):\n",
        "        self.description = description\n",
        "        self.status = status\n",
        "        self.goals = []\n",
        "\n",
        "    def add_goal(self, description):\n",
        "        goal = Goal(description)\n",
        "        self.goals.append(goal)\n",
        "\n",
        "    def mark_completed(self):\n",
        "        self.status = True\n",
        "        for goal in self.goals:\n",
        "            goal.mark_completed()\n",
        "\n",
        "    def __str__(self):\n",
        "        goal_str = \"\\n\\n\".join(str(goal) for goal in self.goals)\n",
        "        return f\"Mission: {self.description} {'(Completed)' if self.status else '(Incomplete)'}\\n{goal_str}\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MissionWidget:\n",
        "    def __init__(self):\n",
        "        self.goals = []\n",
        "        self.description_input = widgets.Text(placeholder='Enter goal description...')\n",
        "        self.add_goal_button = widgets.Button(description='Add Goal')\n",
        "        self.create_mission_button = widgets.Button(description='Create Mission')\n",
        "        self.output = widgets.Output()\n",
        "\n",
        "        self.add_goal_button.on_click(self.add_goal)\n",
        "        self.create_mission_button.on_click(self.create_mission)\n",
        "\n",
        "    def add_goal(self, _):\n",
        "        description = self.description_input.value\n",
        "        goal = Goal(description)\n",
        "        self.goals.append(goal)\n",
        "        self.description_input.value = ''\n",
        "        with self.output:\n",
        "            print(f\"Goal '{description}' added to the mission.\")\n",
        "\n",
        "    def create_mission(self, _):\n",
        "        with self.output:\n",
        "            print(\"Mission created:\")\n",
        "            for goal in self.goals:\n",
        "                print(goal)\n",
        "\n",
        "    def display(self):\n",
        "        display(widgets.VBox([self.description_input, self.add_goal_button, self.create_mission_button, self.output]))\n",
        "\n",
        "# Create and display the mission widget\n",
        "mission_widget = MissionWidget()\n",
        "mission_widget.display()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "5bb114cd007f4af2a8d5dca174c305e6",
            "827ff42383534ca08bbeceb46d4b8fba",
            "80a24bf4d96744b68d165ab651024f22",
            "cc9d16e341364245a12234500bdcc614",
            "6affeccc412045268edce302b6fcc3c8",
            "60922646cebf4a71b3b8a1ea1572a635",
            "8e8124ab954c46b481ddf2edebdfa07a",
            "5e8cb0ba712843f99b4473963be18ffc",
            "7021bd6f8c58472cacc12453385bb4f4",
            "9a0679b37d3644edbb353d80d33a9c22",
            "c7df5631ac634de79b4ee656a9e9807a",
            "776d3b68ae28440aacee2402c94a9905",
            "9fd2ca1617c0404298739a303698c8a2"
          ]
        },
        "id": "07EMwUdEdSNs",
        "outputId": "24e06012-5078-47a4-c938-6af4b250366f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Text(value='', placeholder='Enter goal description...'), Button(description='Add Goal', style=B…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5bb114cd007f4af2a8d5dca174c305e6"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkEgSh3qeGai"
      },
      "source": [
        "## Step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xqttpwAGifXH"
      },
      "outputs": [],
      "source": [
        "class Step:\n",
        "    def __init__(self, description, status=False):\n",
        "        self.description = description\n",
        "        self.status = status\n",
        "        self.substeps = []\n",
        "\n",
        "    def add_substep(self, description):\n",
        "        substep = Substep(description)\n",
        "        self.substeps.append(substep)\n",
        "\n",
        "    def mark_completed(self):\n",
        "        self.status = True\n",
        "        for substep in self.substeps:\n",
        "            substep.mark_completed()\n",
        "\n",
        "    def __str__(self):\n",
        "        substep_str = \"\\n\\t\".join(str(substep) for substep in self.substeps)\n",
        "        return f\"Step: {self.description} {'(Completed)' if self.status else '(Incomplete)'}\\n\\t{substep_str}\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "class StepWidget:\n",
        "    def __init__(self):\n",
        "        self.description_widget = widgets.Text(\n",
        "            placeholder=\"Enter step description...\",\n",
        "            description=\"Step:\",\n",
        "            layout=widgets.Layout(width=\"auto\")\n",
        "        )\n",
        "        self.add_step_button = widgets.Button(description=\"Add Step\", button_style=\"success\")\n",
        "        self.step_output = widgets.Output(layout={'border': '1px solid black'})\n",
        "\n",
        "        # Store created steps\n",
        "        self.steps = []\n",
        "\n",
        "        # Register button click event\n",
        "        self.add_step_button.on_click(self.add_step)\n",
        "\n",
        "    def add_step(self, button):\n",
        "        description = self.description_widget.value.strip()\n",
        "        if description:\n",
        "            step = Step(description)\n",
        "            self.steps.append(step)\n",
        "            self.description_widget.value = \"\"\n",
        "\n",
        "            with self.step_output:\n",
        "                self.clear_output()\n",
        "                print(\"Step added successfully.\")\n",
        "                print(str(step))\n",
        "\n",
        "    def clear_output(self):\n",
        "        self.step_output.clear_output()\n",
        "\n",
        "    def display(self):\n",
        "        display(self.description_widget)\n",
        "        display(self.add_step_button)\n",
        "        display(self.step_output)\n",
        "\n",
        "\n",
        "# Usage Example:\n",
        "\n",
        "step_widget = StepWidget()\n",
        "step_widget.display()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "a6e7dd9db4c94c6388da45f0f3d06504",
            "0a78453503384b86ae1f0be9886fbf5b",
            "ece38d13f56049d9a3988343d3e760db",
            "20a48509d221494cbda6ddfda4694bc0",
            "8319942214f24ad793eba5da04e40a13",
            "6169dda181cc4c6b990594678dfeb55e",
            "c30ddee3731e4cb486cb7073960735a8",
            "b653d5915d9b46ff836fa7047fde4d28"
          ]
        },
        "id": "904uUHU6c0OH",
        "outputId": "37be95d8-fe9d-4f02-c677-29a1cc9aaf04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Text(value='', description='Step:', layout=Layout(width='auto'), placeholder='Enter step description...')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a6e7dd9db4c94c6388da45f0f3d06504"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Button(button_style='success', description='Add Step', style=ButtonStyle())"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "20a48509d221494cbda6ddfda4694bc0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output(layout=Layout(border='1px solid black'))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c30ddee3731e4cb486cb7073960735a8"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyUIQAcQeJRN"
      },
      "source": [
        "## Substep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DOyzf0_cj_wr"
      },
      "outputs": [],
      "source": [
        "class Substep:\n",
        "    def __init__(self, description, status=False):\n",
        "        self.description = description\n",
        "        self.status = status\n",
        "\n",
        "    def mark_completed(self):\n",
        "        self.status = True\n",
        "\n",
        "    def __str__(self):\n",
        "        return f\"Substep: {self.description} {'(Completed)' if self.status else '(Incomplete)'}\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "class SubstepWidget:\n",
        "    def __init__(self):\n",
        "        self.description_widget = widgets.Text(\n",
        "            placeholder=\"Enter substep description...\",\n",
        "            description=\"Substep:\",\n",
        "            layout=widgets.Layout(width=\"auto\")\n",
        "        )\n",
        "        self.add_substep_button = widgets.Button(description=\"Add Substep\", button_style=\"success\")\n",
        "        self.substep_output = widgets.Output(layout={'border': '1px solid black'})\n",
        "\n",
        "        # Store created substeps\n",
        "        self.substeps = []\n",
        "\n",
        "        # Register button click event\n",
        "        self.add_substep_button.on_click(self.add_substep)\n",
        "\n",
        "    def add_substep(self, button):\n",
        "        description = self.description_widget.value.strip()\n",
        "        if description:\n",
        "            substep = Substep(description)\n",
        "            self.substeps.append(substep)\n",
        "            self.description_widget.value = \"\"\n",
        "\n",
        "            with self.substep_output:\n",
        "                self.clear_output()\n",
        "                print(\"Substep added successfully.\")\n",
        "                print(str(substep))\n",
        "\n",
        "    def clear_output(self):\n",
        "        self.substep_output.clear_output()\n",
        "\n",
        "    def display(self):\n",
        "        display(self.description_widget)\n",
        "        display(self.add_substep_button)\n",
        "        display(self.substep_output)\n",
        "\n",
        "\n",
        "# Usage Example:\n",
        "\n",
        "substep_widget = SubstepWidget()\n",
        "substep_widget.display()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "0be2706901a042c1aa3fc03da8af68cd",
            "dcd5026469d74559847ec60161042915",
            "8ad6166d17744fd3ad436fc99a8371b7",
            "579de07ee1b74643b4c0fbb85609fd0b",
            "d90fea3ccea04b759ed7989253e2df10",
            "f17f260da2aa40eca0206ca26b84e6b7",
            "b698cb74434d4ee381e0bc4a6dbaf9f3",
            "e7807fad83634fffba46770bb96563e8"
          ]
        },
        "id": "IedG8WPTfg24",
        "outputId": "716db1b6-99ef-49c5-9523-cbccdce6440e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Text(value='', description='Substep:', layout=Layout(width='auto'), placeholder='Enter substep description...'…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0be2706901a042c1aa3fc03da8af68cd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Button(button_style='success', description='Add Substep', style=ButtonStyle())"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "579de07ee1b74643b4c0fbb85609fd0b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output(layout=Layout(border='1px solid black'))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b698cb74434d4ee381e0bc4a6dbaf9f3"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dependencies"
      ],
      "metadata": {
        "id": "WYq538-wZV-R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JACJe7jRzZBx"
      },
      "outputs": [],
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "import sys\n",
        "\n",
        "def check_langchain_installed():\n",
        "    try:\n",
        "        !pip show langchain\n",
        "        return True\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "def install_langchain(_):\n",
        "    with output:\n",
        "        clear_output()\n",
        "        !pip install langchain\n",
        "        if check_langchain_installed():\n",
        "            status.value = \"Status: langchain is now installed.\"\n",
        "        else:\n",
        "            status.value = \"Status: Installation failed. Please try again.\"\n",
        "\n",
        "status = widgets.Label(value=\"Status: langchain is installed.\" if check_langchain_installed() else \"Status: langchain is not installed.\")\n",
        "install_button = widgets.Button(description=\"Install langchain\")\n",
        "install_button.on_click(install_langchain)\n",
        "output = widgets.Output()\n",
        "\n",
        "widget = widgets.VBox([status, install_button, output])\n",
        "display(widget)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mi-UCl0w5Hm-"
      },
      "outputs": [],
      "source": [
        "class BasePattern:\n",
        "    def __init__(self):\n",
        "        self.name = None\n",
        "        self.intent = None\n",
        "        self.motivation = None\n",
        "        self.example_implementation = None\n",
        "        self.consequences = None\n",
        "\n",
        "    def print_name(self):\n",
        "        print(\"Pattern:\", self.name)\n",
        "\n",
        "    def print_intent(self):\n",
        "        print(\"Intent and Context:\", self.intent)\n",
        "\n",
        "    def print_motivation(self):\n",
        "        print(\"Motivation:\", self.motivation)\n",
        "\n",
        "    def print_example_implementation(self):\n",
        "        print(\"Example Implementation:\", self.example_implementation)\n",
        "\n",
        "    def print_consequences(self):\n",
        "        print(\"Consequences:\", self.consequences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y56tpzdKGQsu"
      },
      "source": [
        "# Meta Language Creator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ds6gBmUEzb-7"
      },
      "outputs": [],
      "source": [
        "{\n",
        "  \"Pattern\": \"Meta Language Creator\",\n",
        "  \"Intent and Context\": \"During a conversation with an LLM, the user would like to create the prompt via an alternate language, such as a textual short-hand notation for graphs, a description of states and state transitions for a state machine, a set of commands for prompt automation, etc. The intent of this pattern is to explain the semantics of this alternative language to the LLM so the user can write future prompts using this new language and its semantics.\",\n",
        "  \"Motivation\": \"Many problems, structures, or other ideas communicated in a prompt may be more concisely, unambiguously, or clearly expressed in a language other than English (or whatever conventional human language is used to interact with an LLM). To produce output based on an alternative language, however, an LLM needs to understand the language’s semantics.\",\n",
        "  \"Example Implementation\": \"The key to successfully using the Meta Language Creation pattern is developing an unambiguous notation or shorthand, such as the following:\\n\\n\\\"From now on, whenever I type two identifiers separated by a '→', I am describing a graph. For example, 'a → b' is describing a graph with nodes 'a' and 'b' and an edge between them. If I separate identifiers by '-[w:2, z:3]→', I am adding properties of the edge, such as a weight or label.\\\"\\n\\nThis example of the Meta Language Creation pattern establishes a standardized notation for describing graphs by defining a convention for representing nodes and edges. Whenever the author types two identifiers separated by a '→' symbol, it is an indication that a graph is being described. For example, if the author types 'a → b', this indicates that a graph is being defined with nodes 'a' and 'b', and that there is an edge between them. This convention provides a clear and concise way to communicate the structure of a graph in written form.\\n\\nMoreover, the prompt goes on to specify that additional information about the edges, such as a weight or label, can be provided using the syntax '-[w:2, z:3]→'.\\n\\nThis notation allows for the specification of additional properties beyond the basic structure of the graph. The specified properties are associated with the edge between the two nodes and can provide important context for the interpretation of the graph. This standardized notation for describing graphs can make it easier to communicate graph structures and properties, which may not be easy or are very verbose to describe as a series of sentences.\",\n",
        "  \"Consequences\": \"Although this pattern provides a powerful means to customize a user’s interaction with an LLM, it may create the potential for confusion within the LLM. As important as it is to clearly define the semantics of the language, it is also essential to ensure the language itself introduces no ambiguities that degrade the LLM’s performance or accuracy. For example, the prompt 'whenever I separate two things by commas, it means that the first thing precedes the second thing' will likely create significant potential for ambiguity and create the potential for unexpected semantics if punctuation involving commas is used in the prompt.\\n\\nTo showcase the potential for confusion, ChatGPT will warn the user and potentially be unable to perform a specific mapping of a symbol or term to a new meaning. For example, if the following prompt is given to ChatGPT: 'Whenever I say 'a', I am referring to Marie Antoinette.', it will respond that this prompt creates too much confusion as 'a' is an indefinite article and too commonly used.\\n\\nIn general, an LLM will perform better on data it was trained on. Therefore, when an existing notation is available, but may not be the dominant meaning, the Meta Language Creation pattern can provide context to scope the meaning of the symbol to improve the accuracy and utility of the output. Although Meta Language Creation is a powerful pattern, it must be used carefully, particularly when describing concepts that may otherwise be hard to precisely or concisely describe. These types of prompts are thus best used in completely new conversation sessions. Using a single meta-language-per-conversation session may also be a best practice since it avoids the potential for conflicting or unexpected semantics being applied to the conversation over time.\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oeFV--yDwIl2"
      },
      "outputs": [],
      "source": [
        "class MetaLanguageCreator(BasePattern):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.name = \"Meta Language Creator\"\n",
        "        self.intent = \"During a conversation with an LLM, the user would like to create the prompt via an alternate language, such as a textual short-hand notation for graphs, a description of states and state transitions for a state machine, a set of commands for prompt automation, etc. The intent of this pattern is to explain the semantics of this alternative language to the LLM so the user can write future prompts using this new language and its semantics.\"\n",
        "        self.motivation = \"Many problems, structures, or other ideas communicated in a prompt may be more concisely, unambiguously, or clearly expressed in a language other than English (or whatever conventional human language is used to interact with an LLM). To produce output based on an alternative language, however, an LLM needs to understand the language’s semantics.\"\n",
        "        self.example_implementation = \"The key to successfully using the Meta Language Creation pattern is developing an unambiguous notation or shorthand, such as the following:\\n\\n\\\"From now on, whenever I type two identifiers separated by a '→', I am describing a graph. For example, 'a → b' is describing a graph with nodes 'a' and 'b' and an edge between them. If I separate identifiers by '-[w:2, z:3]→', I am adding properties of the edge, such as a weight or label.\\\"\\n\\nThis example of the Meta Language Creation pattern establishes a standardized notation for describing graphs by defining a convention for representing nodes and edges. Whenever the author types two identifiers separated by a '→' symbol, it is an indication that a graph is being described. For example, if the author types 'a → b', this indicates that a graph is being defined with nodes 'a' and 'b', and that there is an edge between them. This convention provides a clear and concise way to communicate the structure of a graph in written form.\\n\\nMoreover, the prompt goes on to specify that additional information about the edges, such as a weight or label, can be provided using the syntax '-[w:2, z:3]→'.\\n\\nThis notation allows for the specification of additional properties beyond the basic structure of the graph. The specified properties are associated with the edge between the two nodes and can provide important context for the interpretation of the graph. This standardized notation for describing graphs can make it easier to communicate graph structures and properties, which may not be easy or are very verbose to describe as a series of sentences.\"\n",
        "        self.consequences = \"Although this pattern provides a powerful means to customize a user’s interaction with an LLM, it may create the potential for confusion within the LLM. As important as it is to clearly define the semantics of the language, it is also essential to ensure the language itself introduces no ambiguities that degrade the LLM’s performance or accuracy. For example, the prompt 'whenever I separate two things by commas, it means that the first thing precedes the second thing' will likely create significant potential for ambiguity and create the potential for unexpected semantics if punctuation involving commas is used in the prompt.\\n\\nTo showcase the potential for confusion, ChatGPT will warn the user and potentially be unable to perform a specific mapping of a symbol or term to a new meaning. For example, if the following prompt is given to ChatGPT: 'Whenever I say 'a', I am referring to Marie Antoinette.', it will respond that this prompt creates too much confusion as 'a' is an indefinite article and too commonly used.\\n\\nIn general, an LLM will perform better on data it was trained on. Therefore, when an existing notation is available, but may not be the dominant meaning, the Meta Language Creation pattern can provide context to scope the meaning of the symbol to improve the accuracy and utility of the output. Although Meta Language Creation is a powerful pattern, it must be used carefully, particularly when describing concepts that may otherwise be hard to precisely or concisely describe. These types of prompts are thus best used in completely new conversation sessions. Using a single meta-language-per-conversation session may also be a best practice since it avoids the potential for conflicting or unexpected semantics being applied to the conversation over time.\"\n",
        "\n",
        "    def print_name(self):\n",
        "        print(\"Pattern: \" + self.name)\n",
        "        return \"Pattern: \" + self.name\n",
        "    def print_intent(self):\n",
        "        print(\"Intent and Context: \" + self.intent)\n",
        "        return \" Intent and Context: \" + self.intent\n",
        "\n",
        "    def print_motivation(self):\n",
        "        print(\"Motivation: \" +self.motivation)\n",
        "        return \"Motivation: \" +self.motivation\n",
        "\n",
        "    def print_example_implementation(self):\n",
        "        print(\"Example Implementation: \" + self.example_implementation)\n",
        "        return  \"Example Implementation: \" + self.example_implementation\n",
        "\n",
        "    def print_consequences(self):\n",
        "        print(\"Consequences: \" + self.consequences)\n",
        "        return \"Consequences: \" + self.consequences\n",
        "\n",
        "    def print_meta(self):\n",
        "        # meta_info = f\"Pattern: {self.name}\\n\" \\\n",
        "        #             f\"Intent and Context: {self.intent}\\n\"\n",
        "        # print(meta_info)\n",
        "        # m = self.print_motivation()\n",
        "        # ex = self.print_example_implementation()\n",
        "        # p = self.print_consequences()\n",
        "\n",
        "        return (\n",
        "                    f\"Pattern: {self.name}\\n\"\n",
        "                    f\"Name: {self.name}\\n\"\n",
        "                    f\"Intent and Context: {self.intent}\\n\"\n",
        "                    f\"Motivation: {self.motivation}\\n\"\n",
        "                    f\"Example Implementation: {self.example_implementation}\\n\"\n",
        "                    f\"Consequences: {self.consequences}\"\n",
        "                )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "id": "b47nr9woX4kW",
        "outputId": "13310fdb-0701-48f6-8304-979d5ec96269"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-8cd1ce0e4e9f>\u001b[0m in \u001b[0;36m<cell line: 72>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m \u001b[0mcontrol_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMetaLanguageCreatorControl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMetaLanguageCreator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0minitialize_btn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mButton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Initialize Class\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'MetaLanguageCreator' is not defined"
          ]
        }
      ],
      "source": [
        "from ipywidgets import GridBox, Label, Button, Output, VBox\n",
        "\n",
        "class MetaLanguageCreatorControl(GridBox):\n",
        "    def __init__(self, meta_language_creator, output):\n",
        "        super().__init__(layout={'grid_template_columns': '1fr 1fr', 'grid_gap': '20px'})\n",
        "\n",
        "        self.meta_language_creator = meta_language_creator\n",
        "        self.output = output\n",
        "\n",
        "        self.print_name_btn = Button(description=\"Print Name\")\n",
        "        self.print_name_btn.on_click(self.on_print_name_btn_click)\n",
        "\n",
        "        self.print_intent_btn = Button(description=\"Print Intent\")\n",
        "        self.print_intent_btn.on_click(self.on_print_intent_btn_click)\n",
        "\n",
        "        self.print_motivation_btn = Button(description=\"Print Motivation\")\n",
        "        self.print_motivation_btn.on_click(self.on_print_motivation_btn_click)\n",
        "\n",
        "        self.print_example_implementation_btn = Button(description=\"Print Example Implementation\")\n",
        "        self.print_example_implementation_btn.on_click(self.on_print_example_implementation_btn_click)\n",
        "\n",
        "        self.print_consequences_btn = Button(description=\"Print Consequences\")\n",
        "        self.print_consequences_btn.on_click(self.on_print_consequences_btn_click)\n",
        "\n",
        "        self.print_meta_btn = Button(description=\"Print Meta Info\")\n",
        "        self.print_meta_btn.on_click(self.on_print_meta_btn_click)\n",
        "\n",
        "        self.clear_output_btn = Button(description=\"Clear Output\")\n",
        "        self.clear_output_btn.on_click(self.on_clear_output_btn_click)\n",
        "\n",
        "        self.add_controls()\n",
        "\n",
        "    def add_controls(self):\n",
        "        self.children = [\n",
        "            # self.print_name_btn,\n",
        "            # self.print_intent_btn,\n",
        "            # self.print_motivation_btn,\n",
        "            # self.print_example_implementation_btn,\n",
        "            # self.print_consequences_btn,\n",
        "            self.print_meta_btn,\n",
        "            self.clear_output_btn\n",
        "        ]\n",
        "\n",
        "    def on_print_name_btn_click(self, btn):\n",
        "        with self.output:\n",
        "            self.meta_language_creator.print_name()\n",
        "\n",
        "    def on_print_intent_btn_click(self, btn):\n",
        "        with self.output:\n",
        "            self.meta_language_creator.print_intent()\n",
        "\n",
        "    def on_print_motivation_btn_click(self, btn):\n",
        "        with self.output:\n",
        "            self.meta_language_creator.print_motivation()\n",
        "\n",
        "    def on_print_example_implementation_btn_click(self, btn):\n",
        "        with self.output:\n",
        "            self.meta_language_creator.print_example_implementation()\n",
        "\n",
        "    def on_print_consequences_btn_click(self, btn):\n",
        "        with self.output:\n",
        "            self.meta_language_creator.print_consequences()\n",
        "\n",
        "    def on_print_meta_btn_click(self, btn):\n",
        "        with self.output:\n",
        "            self.meta_language_creator.print_meta()\n",
        "\n",
        "    def on_clear_output_btn_click(self, btn):\n",
        "        self.output.clear_output()\n",
        "\n",
        "output = Output()\n",
        "control_grid = MetaLanguageCreatorControl(MetaLanguageCreator(), output)\n",
        "\n",
        "initialize_btn = Button(description=\"Initialize Class\")\n",
        "initialize_btn.on_click(lambda btn: control_grid)\n",
        "\n",
        "VBox([initialize_btn, control_grid, output])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYl-_0YRzwTq"
      },
      "source": [
        "pipe to take output and input from cells and send to another. we can use a q.\n",
        "\n",
        "class to make buttons from a callback\n",
        "\n",
        "global colab vars to widget\n",
        "logging\n",
        "events\n",
        "\n",
        "\n",
        "We need to be able to take the output from colab using ipy widgets and display it somewhere else. Also need to take visual output and copy to clipboard or throw onto a. Will help with prompt vis etc\n",
        "\n",
        "want to be able to prss button to print out a snippet\n",
        "then send propmt to q\n",
        "so we can use as input in another functuon\n",
        "\n",
        "can we do the display thing without ipy widgets?\n",
        "\n",
        "also make use of sections etc to navigate\n",
        "\n",
        "text title\n",
        "symbol title to help nav\n",
        "\n",
        "dislpay to show which deps are installed etc\n",
        "\n",
        "we can get our output from the vars but to display we can use widgets or a function to send to this cells ouput?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DqH1fV1JU8Di"
      },
      "outputs": [],
      "source": [
        "import itertools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Biyr1-o6F9Z"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0uIiefNvNgy"
      },
      "outputs": [],
      "source": [
        "m = MetaLanguageCreator()\n",
        "m.print_meta()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBfPD9IEH0nc"
      },
      "source": [
        "# Output Automater"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RY5-ccNvHzlI"
      },
      "outputs": [],
      "source": [
        "{\n",
        "  \"Pattern\": \"Output Automater\",\n",
        "  \"Intent and Context\": \"The intent of this pattern is to have the LLM generate a script or other automation artifact that can automatically perform any steps it recommends taking as part of its output. The goal is to reduce the manual effort needed to implement any LLM output recommendations.\",\n",
        "  \"Motivation\": \"The output of an LLM is often a sequence of steps for the user to follow. However, having users continually perform the manual steps dictated by LLM output is tedious and error-prone.\",\n",
        "  \"Structure and Key Ideas\": [\n",
        "    \"Contextual Statements:\",\n",
        "    \"Whenever you produce an output that has at least one step to take and the following properties (alternatively, always do this)\",\n",
        "    \"Produce an executable artifact of type X that will automate these steps\",\n",
        "    \"scoping is up to the user, but helps prevent producing an output automation scripts in cases where running the output automation script will take more user effort than performing the original steps produced in the output. The scope can be limited to outputs requiring more than a certain number of steps.\",\n",
        "    \"The next part of this pattern provides a concrete statement of the type of output the LLM should output to perform the automation. For example, “produce a Python script” gives the LLM a concrete understanding to translate the general steps into equivalent steps in Python. The automation artifact should be concrete and must be something that the LLM associates with the action of 'automating a sequence of steps'.\"\n",
        "  ],\n",
        "  \"Example Implementation\": \"A sample of this prompt pattern applied to code snippets generated by the ChatGPT LLM is shown below:\\n\\n\\\"From now on, whenever you generate code that spans more than one file, generate a Python script that can be run to automatically create the specified files or make changes to existing files to insert the generated code.\\\"\\n\\nThis pattern is particularly effective in software engineering as a common task for software engineers using LLMs is to then copy/paste the outputs into multiple files. This automation trick is also effective at creating scripts for running commands on a terminal, automating cloud operations, or reorganizing files on a file system.\\n\\nThis pattern is a powerful complement for any system that can be computer controlled. The LLM can provide a set of steps that should be taken on the computer-controlled system and then the output can be translated into a script that allows the computer controlling the system to automatically take the steps. This is a direct pathway to allowing LLMs, such as ChatGPT, to integrate quality into—and to control—new computing systems that have a known scripting interface.\",\n",
        "  \"Consequences\": [\n",
        "    \"An important usage consideration of this pattern is that the automation artifact must be defined concretely. Without a concrete meaning for how to 'automate' the steps, the LLM often states that it 'can't automate things' since that is beyond its capabilities.\",\n",
        "    \"One caveat of the Output Automater pattern is the LLM needs sufficient conversational context to generate an automation artifact that is functional in the target context, such as the file system of a project on a Mac vs. Windows computer.\",\n",
        "    \"In some cases, the LLM may produce a long output with multiple steps and not include an automation artifact. This omission may arise for various reasons, including exceeding the output length limitation the LLM supports.\",\n",
        "    \"At this point in the evolution of LLMs, the Output Automater pattern is best employed by users who can read and understand the generated automation artifact. LLMs can (and do) produce inaccuracies in their output, so blindly accepting and executing an automation artifact carries significant risk.\"\n",
        "  ]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BjLzq8mQXfN1"
      },
      "outputs": [],
      "source": [
        "class OutputAutomater(BasePattern):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.name = \"Output Automater\"\n",
        "        self.intent = \"The intent of this pattern is to have the LLM generate a script or other automation artifact that can automatically perform any steps it recommends taking as part of its output. The goal is to reduce the manual effort needed to implement any LLM output recommendations.\"\n",
        "        self.motivation = \"The output of an LLM is often a sequence of steps for the user to follow. However, having users continually perform the manual steps dictated by LLM output is tedious and error-prone.\"\n",
        "        self.example_implementation = \"A sample of this prompt pattern applied to code snippets generated by the ChatGPT LLM is shown below:\\n\\n\\\"From now on, whenever you generate code that spans more than one file, generate a Python script that can be run to automatically create the specified files or make changes to existing files to insert the generated code.\\\"\\n\\nThis pattern is particularly effective in software engineering as a common task for software engineers using LLMs is to then copy/paste the outputs into multiple files. This automation trick is also effective at creating scripts for running commands on a terminal, automating cloud operations, or reorganizing files on a file system.\\n\\nThis pattern is a powerful complement for any system that can be computer controlled. The LLM can provide a set of steps that should be taken on the computer-controlled system and then the output can be translated into a script that allows the computer controlling the system to automatically take the steps. This is a direct pathway to allowing LLMs, such as ChatGPT, to integrate quality into—and to control—new computing systems that have a known scripting interface.\"\n",
        "        self.consequences = \"An important usage consideration of this pattern is that the automation artifact must be defined concretely. Without a concrete meaning for how to 'automate' the steps, the LLM often states that it 'can't automate things' since that is beyond its capabilities.\\n\\nOne caveat of the Output Automater pattern is the LLM needs sufficient conversational context to generate an automation artifact that is functional in the target context, such as the file system of a project on a Mac vs. Windows computer.\\n\\nIn some cases, the LLM may produce a long output with multiple steps and not include an automation artifact. This omission may arise for various reasons, including exceeding the output length limitation the LLM supports.\\n\\nAt this point in the evolution of LLMs, the Output Automater pattern is best employed by users who can read and understand the generated automation artifact. LLMs can (and do) produce inaccuracies in their output, so blindly accepting and executing an automation artifact carries significant risk.\"\n",
        "\n",
        "    def print_name(self):\n",
        "        print(\"Pattern: \" + self.name)\n",
        "\n",
        "    def print_intent(self):\n",
        "        print(\"Intent and Context: \" + self.intent)\n",
        "\n",
        "    def print_motivation(self):\n",
        "        print(\"Motivation: \" + self.motivation)\n",
        "\n",
        "    def print_example_implementation(self):\n",
        "        print(\"Example Implementation: \" + self.example_implementation)\n",
        "\n",
        "    def print_consequences(self):\n",
        "        print(\"Consequences: \" + self.consequences)\n",
        "\n",
        "    def print_meta(self):\n",
        "        meta_info = f\"Pattern: {self.name}\\n\" \\\n",
        "                    f\"Intent and Context: {self.intent}\\n\" \\\n",
        "                    f\"Motivation: {self.motivation}\\n\" \\\n",
        "                    f\"Example Implementation: {self.example_implementation}\\n\" \\\n",
        "                    f\"Consequences: {self.consequences}\"\n",
        "        print(meta_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMj1TnvWxZhg"
      },
      "outputs": [],
      "source": [
        "o = OutputAutomater()\n",
        "o.print_meta()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8fwwLD2H53V"
      },
      "source": [
        "# Persona"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QRLMIRi_H6iw"
      },
      "outputs": [],
      "source": [
        "{\n",
        "  \"Pattern\": \"Persona\",\n",
        "  \"Intent and Context\": \"In many cases, users would like LLM output to always take a certain point of view or perspective. The intent of this pattern is to give the LLM a 'persona' that helps it select what types of output to generate and what details to focus on.\",\n",
        "  \"Motivation\": \"Users may not know what types of outputs or details are important for an LLM to focus on to achieve a given task. The Persona pattern enables users to express what they need help with without knowing the exact details of the outputs they need.\",\n",
        "  \"Structure and Key Ideas\": [\n",
        "    \"Contextual Statements:\",\n",
        "    \"Act as persona X\",\n",
        "    \"Provide outputs that persona X would create\",\n",
        "    \"The first statement conveys the idea that the LLM needs to act as a specific persona and provide outputs that such a persona would.\",\n",
        "    \"The secondary idea—provide outputs that persona X would create—offers opportunities for customization.\"\n",
        "  ],\n",
        "  \"Example Implementation\": \"A sample implementation for code review is shown below:\\n\\n'From now on, act as a security reviewer. Pay close attention to the security details of any code that we look at. Provide outputs that a security reviewer would regarding the code.'\",\n",
        "  \"Consequences\": [\n",
        "    \"An interesting aspect of taking non-human personas is that the LLM may make interesting assumptions or 'hallucinations' regarding the context.\",\n",
        "    \"In other examples, the LLM may prompt the user for more context, such as when ChatGPT is asked to act as a MySQL database and prompts for the structure of a table that the user is pretending to query.\"\n",
        "  ]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5EeRLP5kXh8f"
      },
      "outputs": [],
      "source": [
        "class Persona(BasePattern):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.name = \"Persona\"\n",
        "        self.intent = \"In many cases, users would like LLM output to always take a certain point of view or perspective. The intent of this pattern is to give the LLM a 'persona' that helps it select what types of output to generate and what details to focus on.\"\n",
        "        self.motivation = \"Users may not know what types of outputs or details are important for an LLM to focus on to achieve a given task. The Persona pattern enables users to express what they need help with without knowing the exact details of the outputs they need.\"\n",
        "        self.example_implementation = \"A sample implementation for code review is shown below:\\n\\n'From now on, act as a security reviewer. Pay close attention to the security details of any code that we look at. Provide outputs that a security reviewer would regarding the code.'\"\n",
        "        self.consequences = \"An interesting aspect of taking non-human personas is that the LLM may make interesting assumptions or 'hallucinations' regarding the context.\\n\\nIn other examples, the LLM may prompt the user for more context, such as when ChatGPT is asked to act as a MySQL database and prompts for the structure of a table that the user is pretending to query.\"\n",
        "\n",
        "    def print_name(self):\n",
        "        print(\"Pattern: \" + self.name)\n",
        "\n",
        "    def print_intent(self):\n",
        "        print(\"Intent and Context: \" + self.intent)\n",
        "\n",
        "    def print_motivation(self):\n",
        "        print(\"Motivation: \" + self.motivation)\n",
        "\n",
        "    def print_example_implementation(self):\n",
        "        print(\"Example Implementation: \" + self.example_implementation)\n",
        "\n",
        "    def print_consequences(self):\n",
        "        print(\"Consequences: \" + self.consequences)\n",
        "\n",
        "    def print_meta(self):\n",
        "        meta_info = f\"Pattern: {self.name}\\n\" \\\n",
        "                    f\"Intent and Context: {self.intent}\\n\" \\\n",
        "                    f\"Motivation: {self.motivation}\\n\" \\\n",
        "                    f\"Example Implementation: {self.example_implementation}\\n\" \\\n",
        "                    f\"Consequences: {self.consequences}\"\n",
        "        print(meta_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ywL0xz1TjWe"
      },
      "source": [
        "# Visualization Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPBPiNciTkDq"
      },
      "outputs": [],
      "source": [
        "{\n",
        "  \"Pattern\": \"Visualization Generator\",\n",
        "  \"Intent and Context\": \"The intent of the pattern is to use text generation to create visualizations. This pattern enables the generation of textual inputs that can be used with visualization tools to produce imagery. It combines the strengths of text generation and visualization tools to enhance the output and make it visually appealing and easier to understand.\",\n",
        "  \"Motivation\": \"LLMs generally produce text and cannot generate images or visualizations directly. The Visualization Generator pattern overcomes this limitation by generating textual inputs in the correct format to plug into another tool that can generate the desired visualization. The motivation is to enhance communication and understanding of complex concepts by incorporating visual representations.\",\n",
        "  \"Contextual Statements\": [\n",
        "    \"Generate an X that I can provide to tool Y to visualize it\"\n",
        "  ],\n",
        "  \"Example Implementation\": \"'Whenever I ask you to visualize something, please create either a Graphviz Dot file or DALL-E prompt that I can use to create the visualization. Choose the appropriate tools based on what needs to be visualized.'\",\n",
        "  \"Consequences\": [\n",
        "    \"The pattern creates a pathway for generating visualizations by producing textual inputs for visualization tools.\",\n",
        "    \"It expands the expressive capabilities of the LLM into the visual domain and enhances the communication and understanding of complex concepts.\",\n",
        "    \"Users can leverage AI generators and visualization tools to create rich and diverse visualizations based on the text generated by the LLM.\"\n",
        "  ]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oab-8I84Xias"
      },
      "outputs": [],
      "source": [
        "class VisualizationGenerator(BasePattern):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.name = \"Visualization Generator\"\n",
        "        self.intent = \"The intent of the pattern is to use text generation to create visualizations. This pattern enables the generation of textual inputs that can be used with visualization tools to produce imagery. It combines the strengths of text generation and visualization tools to enhance the output and make it visually appealing and easier to understand.\"\n",
        "        self.motivation = \"LLMs generally produce text and cannot generate images or visualizations directly. The Visualization Generator pattern overcomes this limitation by generating textual inputs in the correct format to plug into another tool that can generate the desired visualization. The motivation is to enhance communication and understanding of complex concepts by incorporating visual representations.\"\n",
        "        self.example_implementation = \"'Whenever I ask you to visualize something, please create either a Graphviz Dot file or DALL-E prompt that I can use to create the visualization. Choose the appropriate tools based on what needs to be visualized.'\"\n",
        "        self.consequences = \"The pattern creates a pathway for generating visualizations by producing textual inputs for visualization tools.\\n\\nIt expands the expressive capabilities of the LLM into the visual domain and enhances the communication and understanding of complex concepts.\\n\\nUsers can leverage AI generators and visualization tools to create rich and diverse visualizations based on the text generated by the LLM.\"\n",
        "\n",
        "    def print_name(self):\n",
        "        print(\"Pattern: \" + self.name)\n",
        "\n",
        "    def print_intent(self):\n",
        "        print(\"Intent and Context: \" + self.intent)\n",
        "\n",
        "    def print_motivation(self):\n",
        "        print(\"Motivation: \" + self.motivation)\n",
        "\n",
        "    def print_example_implementation(self):\n",
        "        print(\"Example Implementation: \" + self.example_implementation)\n",
        "\n",
        "    def print_consequences(self):\n",
        "        print(\"Consequences: \" + self.consequences)\n",
        "\n",
        "    def print_meta(self):\n",
        "        meta_info = f\"Pattern: {self.name}\\n\" \\\n",
        "                    f\"Intent and Context: {self.intent}\\n\" \\\n",
        "                    f\"Motivation: {self.motivation}\\n\" \\\n",
        "                    f\"Example Implementation: {self.example_implementation}\\n\" \\\n",
        "                    f\"Consequences: {self.consequences}\"\n",
        "        print(meta_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pk_KYuEYKkxQ"
      },
      "outputs": [],
      "source": [
        "v = VisualizationGenerator().print_meta()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6irM-jfUHrj"
      },
      "source": [
        "# Recipe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmZlveK5UI2m"
      },
      "outputs": [],
      "source": [
        "{\n",
        "  \"Pattern\": \"Recipe\",\n",
        "  \"Intent and Context\": \"The intent of this pattern is to generate a sequence of steps to achieve a stated goal. Users provide partial ingredients or steps, and the LLM generates a complete sequence of steps to accomplish the goal.\",\n",
        "  \"Motivation\": \"Users often know the desired outcome and have an idea of the necessary ingredients or steps, but may need help in determining the precise order of steps. The Recipe pattern addresses this need by providing a structured approach to generate step-by-step instructions.\",\n",
        "  \"Structure and Key Ideas\": \"Fundamental contextual statements:\",\n",
        "  \"Contextual Statements\": [\n",
        "    \"I would like to achieve X\",\n",
        "    \"I know that I need to perform steps A, B, C\",\n",
        "    \"Provide a complete sequence of steps for me\",\n",
        "    \"Fill in any missing steps\",\n",
        "    \"Identify any unnecessary steps\"\n",
        "  ],\n",
        "  \"Example Implementation\": [\n",
        "    \"An example usage of this pattern in the context of deploying a software application to the cloud is shown below:\",\n",
        "    \"\\\"I am trying to deploy an application to the cloud. I know that I need to install the necessary dependencies on a virtual machine for my application. I know that I need to sign up for an AWS account. Please provide a complete sequence of steps. Please fill in any missing steps. Please identify any unnecessary steps.\\\"\"\n",
        "  ],\n",
        "  \"Consequences\": [\n",
        "    \"The Recipe pattern enables users to generate step-by-step instructions to achieve a specific goal.\",\n",
        "    \"However, users may not always have a well-specified description or may introduce biases with their initially selected steps.\",\n",
        "    \"The LLM's selection of steps may incorporate the user's input, even if there are alternative solutions that do not require those steps.\",\n",
        "    \"It is important for users to review the generated recipe and assess its efficiency and suitability for their specific needs.\"\n",
        "  ]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jP5C11_hXj15"
      },
      "outputs": [],
      "source": [
        "class Recipe(BasePattern):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.name = \"Recipe\"\n",
        "        self.intent = \"The intent of this pattern is to generate a sequence of steps to achieve a stated goal. Users provide partial ingredients or steps, and the LLM generates a complete sequence of steps to accomplish the goal.\"\n",
        "        self.motivation = \"Users often know the desired outcome and have an idea of the necessary ingredients or steps, but may need help in determining the precise order of steps. The Recipe pattern addresses this need by providing a structured approach to generate step-by-step instructions.\"\n",
        "        self.contextual_statements = [\n",
        "            \"I would like to achieve X\",\n",
        "            \"I know that I need to perform steps A, B, C\",\n",
        "            \"Provide a complete sequence of steps for me\",\n",
        "            \"Fill in any missing steps\",\n",
        "            \"Identify any unnecessary steps\"\n",
        "        ]\n",
        "        self.example_implementation = [\n",
        "            \"An example usage of this pattern in the context of deploying a software application to the cloud is shown below:\",\n",
        "            \"\\\"I am trying to deploy an application to the cloud. I know that I need to install the necessary dependencies on a virtual machine for my application. I know that I need to sign up for an AWS account. Please provide a complete sequence of steps. Please fill in any missing steps. Please identify any unnecessary steps.\\\"\"\n",
        "        ]\n",
        "        self.consequences = [\n",
        "            \"The Recipe pattern enables users to generate step-by-step instructions to achieve a specific goal.\",\n",
        "            \"However, users may not always have a well-specified description or may introduce biases with their initially selected steps.\",\n",
        "            \"The LLM's selection of steps may incorporate the user's input, even if there are alternative solutions that do not require those steps.\",\n",
        "            \"It is important for users to review the generated recipe and assess its efficiency and suitability for their specific needs.\"\n",
        "        ]\n",
        "\n",
        "    def print_name(self):\n",
        "        print(\"Pattern: \" + self.name)\n",
        "\n",
        "    def print_intent(self):\n",
        "        print(\"Intent and Context: \" + self.intent)\n",
        "\n",
        "    def print_motivation(self):\n",
        "        print(\"Motivation: \")\n",
        "        print(self.motivation)\n",
        "\n",
        "    def print_contextual_statements(self):\n",
        "        print(\"Contextual Statements: \")\n",
        "        for statement in self.contextual_statements:\n",
        "            print(\"- \" + statement)\n",
        "\n",
        "    def print_example_implementation(self):\n",
        "        print(\"Example Implementation: \")\n",
        "        for example in self.example_implementation:\n",
        "            print(example)\n",
        "\n",
        "    def print_consequences(self):\n",
        "        print(\"Consequences: \")\n",
        "        for consequence in self.consequences:\n",
        "\n",
        "            print(\"- \" + consequence)\n",
        "\n",
        "    def print_meta(self):\n",
        "        meta_info = f\"Pattern: {self.name}\\n\" \\\n",
        "                    f\"Intent and Context: {self.intent}\\n\"\n",
        "        print(meta_info)\n",
        "        self.print_motivation()\n",
        "        self.print_contextual_statements()\n",
        "        self.print_example_implementation()\n",
        "        self.print_consequences()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jty9WYT5PbQV"
      },
      "outputs": [],
      "source": [
        "r = Recipe()\n",
        "r.print_meta()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuJXy--VTKbn"
      },
      "source": [
        "# Template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wHogIk9uTK6j"
      },
      "outputs": [],
      "source": [
        "{\n",
        "  \"Pattern\": \"Template\",\n",
        "  \"Intent and Context\": \"The intent of the pattern is to ensure an LLM’s output follows a precise template in terms of structure. This pattern allows the user to instruct the LLM to produce its output in a format it would not ordinarily use for the specified type of content being generated.\",\n",
        "  \"Motivation\": \"In some cases, output must be produced in a precise format that is application or use-case specific and not known to the LLM. The LLM needs to be instructed on the format and where the different parts of its output should go.\",\n",
        "  \"Contextual Statements\": [\n",
        "    \"I am going to provide a template for your output\",\n",
        "    \"X is my placeholder for content\",\n",
        "    \"Try to fit the output into one or more of the placeholders that I list\",\n",
        "    \"Please preserve the formatting and overall template that I provide\",\n",
        "    \"This is the template: PATTERN with PLACEHOLDERS\"\n",
        "  ],\n",
        "  \"Example Implementation\": \"'I am going to provide a template for your output. Everything in all caps is a placeholder. Any time that you generate text, try to fit it into one of the placeholders that I list. Please preserve the formatting and overall template that I provide at https://myapi.com/NAME/profile/JOB'\",\n",
        "  \"Consequences\": [\n",
        "    \"Applying the Template pattern filters the LLM’s output, which may eliminate other outputs the LLM would have provided that might be useful to the user.\",\n",
        "    \"Combining this pattern with other patterns from the Output Customization category may be challenging due to the constrained output format.\",\n",
        "    \"Users should weigh the pros and cons of filtering out additional information and consider the compatibility of this pattern with other desired patterns.\"\n",
        "  ]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OzJtquwDXkv1"
      },
      "outputs": [],
      "source": [
        "class Template(BasePattern):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.name = \"Template\"\n",
        "        self.intent = \"The intent of the pattern is to ensure an LLM’s output follows a precise template in terms of structure. This pattern allows the user to instruct the LLM to produce its output in a format it would not ordinarily use for the specified type of content being generated.\"\n",
        "        self.motivation = \"In some cases, output must be produced in a precise format that is application or use-case specific and not known to the LLM. The LLM needs to be instructed on the format and where the different parts of its output should go.\"\n",
        "        self.contextual_statements = [\n",
        "            \"I am going to provide a template for your output\",\n",
        "            \"X is my placeholder for content\",\n",
        "            \"Try to fit the output into one or more of the placeholders that I list\",\n",
        "            \"Please preserve the formatting and overall template that I provide\",\n",
        "            \"This is the template: PATTERN with PLACEHOLDERS\"\n",
        "        ]\n",
        "        self.example_implementation = \"'I am going to provide a template for your output. Everything in all caps is a placeholder. Any time that you generate text, try to fit it into one of the placeholders that I list. Please preserve the formatting and overall template that I provide at https://myapi.com/NAME/profile/JOB'\"\n",
        "        self.consequences = [\n",
        "            \"Applying the Template pattern filters the LLM’s output, which may eliminate other outputs the LLM would have provided that might be useful to the user.\",\n",
        "            \"Combining this pattern with other patterns from the Output Customization category may be challenging due to the constrained output format.\",\n",
        "            \"Users should weigh the pros and cons of filtering out additional information and consider the compatibility of this pattern with other desired patterns.\"\n",
        "        ]\n",
        "\n",
        "    def print_name(self):\n",
        "        print(\"Pattern: \" + self.name)\n",
        "\n",
        "    def print_intent(self):\n",
        "        print(\"Intent and Context: \" + self.intent)\n",
        "\n",
        "    def print_motivation(self):\n",
        "        print(\"Motivation: \")\n",
        "        print(\"- \" + self.motivation)\n",
        "\n",
        "    def print_contextual_statements(self):\n",
        "        print(\"Contextual Statements: \")\n",
        "        for statement in self.contextual_statements:\n",
        "            print(\"- \" + statement)\n",
        "\n",
        "    def print_example_implementation(self):\n",
        "        print(\"Example Implementation: \" + self.example_implementation)\n",
        "\n",
        "    def print_consequences(self):\n",
        "        print(\"Consequences: \")\n",
        "        for consequence in self.consequences:\n",
        "            print(\"- \" + consequence)\n",
        "\n",
        "    def print_meta(self):\n",
        "        meta_info = f\"Pattern: {self.name}\\n\" \\\n",
        "                    f\"Intent and Context: {self.intent}\\n\"\n",
        "        print(meta_info)\n",
        "        self.print_motivation()\n",
        "        self.print_contextual_statements()\n",
        "        self.print_example_implementation()\n",
        "        self.print_consequences()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ua3SQCRQPmqk"
      },
      "outputs": [],
      "source": [
        "t = Template()\n",
        "t.print_meta()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62LpuIdlN9N8"
      },
      "source": [
        "# Fact Check List"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sdew_jFEN9eH"
      },
      "outputs": [],
      "source": [
        "{\n",
        "  \"Pattern\": \"Fact Check List\",\n",
        "  \"Intent and Context\": \"The intent of this pattern is to ensure that the LLM outputs a list of facts that are present in the output and form an important part of the statements in the output. This list of facts helps inform the user of the facts (or assumptions) the output is based on.\",\n",
        "  \"Motivation\": [\n",
        "    \"A current weakness of LLMs (including ChatGPT) is they often rapidly generate convincing text that is factually incorrect.\",\n",
        "    \"Users may not perform appropriate due diligence to determine the accuracy of the output.\"\n",
        "  ],\n",
        "  \"Contextual Statements\": [\n",
        "    \"Generate a set of facts that are contained in the output\",\n",
        "    \"The set of facts should be inserted in a specific point in the output\",\n",
        "    \"The set of facts should be the fundamental facts that could undermine the veracity of the output if any of them are incorrect\"\n",
        "  ],\n",
        "  \"Example Implementation\": \"'From now on, when you generate an answer, create a set of facts that the answer depends on that should be fact-checked and list this set of facts at the end of your output. Only include facts related to cybersecurity.'\",\n",
        "  \"Consequences\": [\n",
        "    \"The Fact Check List pattern should be employed whenever users are not experts in the domain for which they are generating output.\",\n",
        "    \"Errors are potential in all LLM outputs, so Fact Check List is an effective pattern to combine with other patterns, such as the Question Refinement pattern.\",\n",
        "    \"Users can directly compare the fact check list to the output to verify the facts listed in the fact check list actually appear in the output.\",\n",
        "    \"The fact check list may also have errors, but users often have sufficient knowledge and context to determine its completeness and accuracy relative to the output.\",\n",
        "    \"The Fact Check List pattern only applies when the output type is amenable to fact-checking.\"\n",
        "  ]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSAgifJTXlU2"
      },
      "outputs": [],
      "source": [
        "class FactCheckList(BasePattern):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.name = \"Fact Check List\"\n",
        "        self.intent = \"The intent of this pattern is to ensure that the LLM outputs a list of facts that are present in the output and form an important part of the statements in the output. This list of facts helps inform the user of the facts (or assumptions) the output is based on.\"\n",
        "        self.motivation = [\n",
        "            \"A current weakness of LLMs (including ChatGPT) is they often rapidly generate convincing text that is factually incorrect.\",\n",
        "            \"Users may not perform appropriate due diligence to determine the accuracy of the output.\"\n",
        "        ]\n",
        "        self.contextual_statements = [\n",
        "            \"Generate a set of facts that are contained in the output\",\n",
        "            \"The set of facts should be inserted in a specific point in the output\",\n",
        "            \"The set of facts should be the fundamental facts that could undermine the veracity of the output if any of them are incorrect\"\n",
        "        ]\n",
        "        self.example_implementation = \"'From now on, when you generate an answer, create a set of facts that the answer depends on that should be fact-checked and list this set of facts at the end of your output. Only include facts related to cybersecurity.'\"\n",
        "        self.consequences = [\n",
        "            \"The Fact Check List pattern should be employed whenever users are not experts in the domain for which they are generating output.\",\n",
        "            \"Errors are potential in all LLM outputs, so Fact Check List is an effective pattern to combine with other patterns, such as the Question Refinement pattern.\",\n",
        "            \"Users can directly compare the fact check list to the output to verify the facts listed in the fact check list actually appear in the output.\",\n",
        "            \"The fact check list may also have errors, but users often have sufficient knowledge and context to determine its completeness and accuracy relative to the output.\",\n",
        "            \"The Fact Check List pattern only applies when the output type is amenable to fact-checking.\"\n",
        "        ]\n",
        "\n",
        "    def print_name(self):\n",
        "        print(\"Pattern: \" + self.name)\n",
        "\n",
        "    def print_intent(self):\n",
        "        print(\"Intent and Context: \" + self.intent)\n",
        "\n",
        "    def print_motivation(self):\n",
        "        print(\"Motivation: \")\n",
        "        for motivation in self.motivation:\n",
        "            print(\"- \" + motivation)\n",
        "\n",
        "    def print_contextual_statements(self):\n",
        "        print(\"Contextual Statements: \")\n",
        "        for statement in self.contextual_statements:\n",
        "            print(\"- \" + statement)\n",
        "\n",
        "    def print_example_implementation(self):\n",
        "        print(\"Example Implementation: \" + self.example_implementation)\n",
        "\n",
        "    def print_consequences(self):\n",
        "        print(\"Consequences: \")\n",
        "        for consequence in self.consequences:\n",
        "            print(\"- \" + consequence)\n",
        "\n",
        "    def print_meta(self):\n",
        "        meta_info = f\"Pattern: {self.name}\\n\" \\\n",
        "                    f\"Intent and Context: {self.intent}\\n\"\n",
        "        print(meta_info)\n",
        "        self.print_motivation()\n",
        "        self.print_contextual_statements()\n",
        "        self.print_example_implementation()\n",
        "        self.print_consequences()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fvbhO9wtKTaV"
      },
      "outputs": [],
      "source": [
        "f = FactCheckList().print_meta()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WIVdw-UH5zl"
      },
      "source": [
        "# Question Refinement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3pzMJ3SjH6UP"
      },
      "outputs": [],
      "source": [
        "{\n",
        "  \"Pattern\": \"Question Refinement\",\n",
        "  \"Intent and Context\": \"This pattern engages the LLM in the prompt engineering process. The intent of this pattern is to ensure the conversational LLM always suggests potentially better or more refined questions the user could ask instead of their original question.\",\n",
        "  \"Motivation\": \"If a user is asking a question, it is possible they are not an expert in the domain and may not know the best way to phrase the question or be aware of additional information helpful in phrasing the question. LLMs will often state limitations on the answer they are providing or request additional information to help them produce a more accurate answer.\",\n",
        "  \"Contextual Statements\": [\n",
        "    \"Within scope X, suggest a better version of the question to use instead\",\n",
        "    \"(Optional) prompt me if I would like to use the better version instead\"\n",
        "  ],\n",
        "  \"Structure and Key Ideas\": [\n",
        "    \"Fundamental contextual statements:\",\n",
        "    \"The first contextual statement in the prompt is asking the LLM to suggest a better version of a question within a specific scope.\",\n",
        "    \"The second contextual statement is meant for automation and allows the user to automatically use the refined question without having to copy/paste or manually enter it.\",\n",
        "    \"The engineering of this prompt can be further refined by combining it with the Reflection pattern, which allows the LLM to explain why it believes the refined question is an improvement.\"\n",
        "  ],\n",
        "  \"Example Implementation\": \"'From now on, whenever I ask a question about a software artifact’s security, suggest a better version of the question to use that incorporates information specific to security risks in the language or framework that I am using instead and ask me if I would like to use your question instead.'\",\n",
        "  \"Consequences\": [\n",
        "    \"The Question Refinement pattern helps bridge the gap between the user’s knowledge and the LLM’s understanding, thereby yielding more efficient and accurate interactions.\",\n",
        "    \"One risk of this pattern is its tendency to rapidly narrow the questioning by the user into a specific area that guides the user down a more limited path of inquiry than necessary.\",\n",
        "    \"Another approach to overcoming arbitrary narrowing or limited targeting of the refined question is to combine the Question Refinement pattern with other patterns, such as the Cognitive Verifier pattern or the Persona pattern.\",\n",
        "    \"An LLM can always produce factual inaccuracies, just like a human. A risk of this pattern is that the inaccuracies are introduced into the refined question. This risk may be mitigated, however, by combining the Fact Check List pattern and the Reflection pattern.\"\n",
        "  ]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8R6Epl2NXlxn"
      },
      "outputs": [],
      "source": [
        "class QuestionRefinement(BasePattern):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.name = \"Question Refinement\"\n",
        "        self.intent = \"This pattern engages the LLM in the prompt engineering process. The intent of this pattern is to ensure the conversational LLM always suggests potentially better or more refined questions the user could ask instead of their original question.\"\n",
        "        self.motivation = \"If a user is asking a question, it is possible they are not an expert in the domain and may not know the best way to phrase the question or be aware of additional information helpful in phrasing the question. LLMs will often state limitations on the answer they are providing or request additional information to help them produce a more accurate answer.\"\n",
        "        self.contextual_statements = \"Within scope X, suggest a better version of the question to use instead/(Optional) prompt me if I would like to use the better version instead\"\n",
        "        self.structure_and_key_ideas = \"Fundamental contextual statements:/The first contextual statement in the prompt is asking the LLM to suggest a better version of a question within a specific scope./The second contextual statement is meant for automation and allows the user to automatically use the refined question without having to copy/paste or manually enter it./The engineering of this prompt can be further refined by combining it with the Reflection pattern, which allows the LLM to explain why it believes the refined question is an improvement.\"\n",
        "        self.example_implementation = \"'From now on, whenever I ask a question about a software artifact’s security, suggest a better version of the question to use that incorporates information specific to security risks in the language or framework that I am using instead and ask me if I would like to use your question instead.'\"\n",
        "        self.consequences = \"The Question Refinement pattern helps bridge the gap between the user’s knowledge and the LLM’s understanding, thereby yielding more efficient and accurate interactions./One risk of this pattern is its tendency to rapidly narrow the questioning by the user into a specific area that guides the user down a more limited path of inquiry than necessary./Another approach to overcoming arbitrary narrowing or limited targeting of the refined question is to combine the Question Refinement pattern with other patterns, such as the Cognitive Verifier pattern or the Persona pattern./An LLM can always produce factual inaccuracies, just like a human. A risk of this pattern is that the inaccuracies are introduced into the refined question. This risk may be mitigated, however, by combining the Fact Check List pattern and the Reflection pattern.\"\n",
        "\n",
        "    def print_name(self):\n",
        "        print(\"Pattern: \" + self.name)\n",
        "\n",
        "    def print_intent(self):\n",
        "        print(\"Intent and Context: \" + self.intent)\n",
        "\n",
        "    def print_motivation(self):\n",
        "        print(\"Motivation: \")\n",
        "        motivations = self.motivation.split(\"/\")\n",
        "        for motivation in motivations:\n",
        "            print(\"- \" + motivation)\n",
        "\n",
        "    def print_contextual_statements(self):\n",
        "        print(\"Contextual Statements: \")\n",
        "        statements = self.contextual_statements.split(\"/\")\n",
        "        for statement in statements:\n",
        "            print(\"- \" + statement)\n",
        "\n",
        "    def print_structure_and_key_ideas(self):\n",
        "        print(\"Structure and Key Ideas: \")\n",
        "        ideas = self.structure_and_key_ideas.split(\"/\")\n",
        "        for idea in ideas:\n",
        "            print(\"- \" + idea)\n",
        "\n",
        "    def print_example_implementation(self):\n",
        "        print(\"Example Implementation: \" + self.example_implementation)\n",
        "\n",
        "    def print_consequences(self):\n",
        "        print(\"Consequences: \")\n",
        "        consequences = self.consequences.split(\"/\")\n",
        "        for consequence in consequences:\n",
        "            print(\"- \" + consequence)\n",
        "\n",
        "    def print_meta(self):\n",
        "        meta_info = f\"Pattern: {self.name}\\n\" \\\n",
        "                    f\"Intent and Context: {self.intent}\\n\"\n",
        "        print(meta_info)\n",
        "        self.print_motivation()\n",
        "        self.print_contextual_statements()\n",
        "        self.print_structure_and_key_ideas()\n",
        "        self.print_example_implementation()\n",
        "        self.print_consequences()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2MBibLjHsZD"
      },
      "outputs": [],
      "source": [
        "q = QuestionRefinement()\n",
        "q.print_meta()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsDqjOH1H5t-"
      },
      "source": [
        "# Alternative Approaches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ee8rv_uAH6Ql"
      },
      "outputs": [],
      "source": [
        "{\n",
        "  \"Pattern\": \"Alternative Approaches\",\n",
        "  \"Intent and Context\": \"The intent of the pattern is to ensure an LLM always offers alternative ways of accomplishing a task so a user does not pursue only the approaches with which they are familiar.\",\n",
        "  \"Motivation\": \"Humans often suffer from cognitive biases that lead them to choose a particular approach to solve a problem even when it is not the right or “best” approach. The motivation of the Alternative Approaches pattern is to ensure the user is aware of alternative approaches to select a better approach to solve a problem by dissolving their cognitive biases.\",\n",
        "  \"Contextual Statements\": [\n",
        "    \"Within scope X, if there are alternative ways to accomplish the same thing, list the best alternate approaches\",\n",
        "    \"(Optional) compare/contrast the pros and cons of each approach\",\n",
        "    \"(Optional) include the original way that I asked\",\n",
        "    \"(Optional) prompt me for which approach I would like to use\"\n",
        "  ],\n",
        "  \"Structure and Key Ideas\": [\n",
        "    \"Fundamental contextual statements:\",\n",
        "    \"The first statement, “within scope X”, scopes the interaction to a particular goal, topic, or bounds on the questioning.\",\n",
        "    \"The second statement, “if there are alternative ways to accomplish the same thing, list the best alternate approaches”, instructs the LLM to suggest alternatives.\",\n",
        "    \"The optional statement “compare/contrast the pros and cons of each approach” adds decision-making criteria to the analysis.\",\n",
        "    \"The final statement, “prompt me for which approach I would like to use”, helps eliminate the user needing to manually copy/paste or enter in an alternative approach if one is selected.\"\n",
        "  ],\n",
        "  \"Example Implementation\": \"'Whenever I ask you to deploy an application to a specific cloud service, if there are alternative services to accomplish the same thing with the same cloud service provider, list the best alternative services and then compare/contrast the pros and cons of each approach with respect to cost, availability, and maintenance effort and include the original way that I asked. Then ask me which approach I would like to proceed with.'\",\n",
        "  \"Consequences\": [\n",
        "    \"This pattern is effective in its generic form and can be applied to a range of tasks effectively.\",\n",
        "    \"Refinements could include having a standardized catalog of acceptable alternatives in a specific domain from which the user must select.\",\n",
        "    \"The Alternative Approaches pattern can also be used to incentivize users to select one of an approved set of approaches while informing them of the pros/cons of the approved options.\"\n",
        "  ]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MwDzyQLIXmgm"
      },
      "outputs": [],
      "source": [
        "class AlternativeApproaches(BasePattern):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.name = \"Alternative Approaches\"\n",
        "        self.intent = \"The intent of the pattern is to ensure an LLM always offers alternative ways of accomplishing a task so a user does not pursue only the approaches with which they are familiar.\"\n",
        "        self.motivation = \"Humans often suffer from cognitive biases that lead them to choose a particular approach to solve a problem even when it is not the right or “best” approach. The motivation of the Alternative Approaches pattern is to ensure the user is aware of alternative approaches to select a better approach to solve a problem by dissolving their cognitive biases.\"\n",
        "        self.contextual_statements = [\n",
        "            \"Within scope X, if there are alternative ways to accomplish the same thing, list the best alternate approaches\",\n",
        "            \"(Optional) compare/contrast the pros and cons of each approach\",\n",
        "            \"(Optional) include the original way that I asked\",\n",
        "            \"(Optional) prompt me for which approach I would like to use\"\n",
        "        ]\n",
        "        self.structure_and_key_ideas = [\n",
        "            \"Fundamental contextual statements:\",\n",
        "            \"The first statement, “within scope X”, scopes the interaction to a particular goal, topic, or bounds on the questioning.\",\n",
        "            \"The second statement, “if there are alternative ways to accomplish the same thing, list the best alternate approaches”, instructs the LLM to suggest alternatives.\",\n",
        "            \"The optional statement “compare/contrast the pros and cons of each approach” adds decision-making criteria to the analysis.\",\n",
        "            \"The final statement, “prompt me for which approach I would like to use”, helps eliminate the user needing to manually copy/paste or enter in an alternative approach if one is selected.\"\n",
        "        ]\n",
        "        self.example_implementation = \"'Whenever I ask you to deploy an application to a specific cloud service, if there are alternative services to accomplish the same thing with the same cloud service provider, list the best alternative services and then compare/contrast the pros and cons of each approach with respect to cost, availability, and maintenance effort and include the original way that I asked. Then ask me which approach I would like to proceed with.'\"\n",
        "        self.consequences = [\n",
        "            \"This pattern is effective in its generic form and can be applied to a range of tasks effectively.\",\n",
        "            \"Refinements could include having a standardized catalog of acceptable alternatives in a specific domain from which the user must select.\",\n",
        "            \"The Alternative Approaches pattern can also be used to incentivize users to select one of an approved set of approaches while informing them of the pros/cons of the approved options.\"\n",
        "        ]\n",
        "\n",
        "    def print_name(self):\n",
        "        print(\"Pattern: \" + self.name)\n",
        "\n",
        "    def print_intent(self):\n",
        "        print(\"Intent and Context: \" + self.intent)\n",
        "\n",
        "    def print_motivation(self):\n",
        "        print(\"Motivation: \")\n",
        "\n",
        "        print(self.motivation)\n",
        "\n",
        "    def print_contextual_statements(self):\n",
        "        print(\"Contextual Statements: \")\n",
        "        for statement in self.contextual_statements:\n",
        "            print(\"- \" + statement)\n",
        "\n",
        "    def print_structure_and_key_ideas(self):\n",
        "        print(\"Structure and Key Ideas: \")\n",
        "        for idea in self.structure_and_key_ideas:\n",
        "            print(\"- \" + idea)\n",
        "\n",
        "    def print_example_implementation(self):\n",
        "        print(\"Example Implementation: \" + self.example_implementation)\n",
        "\n",
        "    def print_consequences(self):\n",
        "        print(\"Consequences: \")\n",
        "        for consequence in self.consequences:\n",
        "            print(\"- \" + consequence)\n",
        "\n",
        "    def print_meta(self):\n",
        "        meta_info = f\"Pattern: {self.name}\\n\" \\\n",
        "                    f\"Intent and Context: {self.intent}\\n\"\n",
        "        print(meta_info)\n",
        "        self.print_motivation()\n",
        "        self.print_contextual_statements()\n",
        "        self.print_structure_and_key_ideas()\n",
        "        self.print_example_implementation()\n",
        "        self.print_consequences()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KMYuPEDVpHoJ"
      },
      "outputs": [],
      "source": [
        "a = AlternativeApproaches()\n",
        "a.print_meta()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iyRXL2JH5WZ"
      },
      "source": [
        "# Cognitive Verifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8g1wxQdH6Nx"
      },
      "outputs": [],
      "source": [
        "{\n",
        "  \"Pattern\": \"Cognitive Verifier\",\n",
        "  \"Intent and Context\": \"The intent of the pattern is to force the LLM to always subdivide questions into additional questions that can be used to provide a better answer to the original question.\",\n",
        "  \"Motivation\": [\n",
        "    \"Humans may initially ask questions that are too high-level to provide a concrete answer to without additional follow-up due to unfamiliarity with the domain, laziness in prompt entry, or being unsure about what the correct phrasing of the question should be.\",\n",
        "    \"Research has demonstrated that LLMs can often perform better when using a question that is subdivided into individual questions.\"\n",
        "  ],\n",
        "  \"Contextual Statements\": [\n",
        "    \"When you are asked a question, follow these rules\",\n",
        "    \"Generate a number of additional questions that would help more accurately answer the question\",\n",
        "    \"Combine the answers to the individual questions to produce the final answer to the overall question\"\n",
        "  ],\n",
        "  \"Example Implementation\": \"'When I ask you a question, generate three additional questions that would help you give a more accurate answer. When I have answered the three questions, combine the answers to produce the final answers to my original question.'\",\n",
        "  \"Consequences\": [\n",
        "    \"This pattern can dictate the exact number of questions to generate or leave this decision to the LLM.\",\n",
        "    \"Specifying an exact number of questions can tightly scope the amount of additional information the user is forced to provide so it is within a range they are willing and able to contribute.\",\n",
        "    \"Given N questions, there may be an invaluable N + 1 question that will always be scoped out.\",\n",
        "    \"Alternatively, the LLM can be provided a range or allowed to ask additional questions.\",\n",
        "    \"By omitting a limit on the number of questions, the LLM may generate numerous additional questions that overwhelm the user.\"\n",
        "  ]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D56ihEU4XnJP"
      },
      "outputs": [],
      "source": [
        "class CognitiveVerifier(BasePattern):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.name = \"Cognitive Verifier\"\n",
        "        self.intent = \"The intent of the pattern is to force the LLM to always subdivide questions into additional questions that can be used to provide a better answer to the original question.\"\n",
        "        self.motivation = [\n",
        "            \"Humans may initially ask questions that are too high-level to provide a concrete answer to without additional follow-up due to unfamiliarity with the domain, laziness in prompt entry, or being unsure about what the correct phrasing of the question should be.\",\n",
        "            \"Research has demonstrated that LLMs can often perform better when using a question that is subdivided into individual questions.\"\n",
        "        ]\n",
        "        self.contextual_statements = [\n",
        "            \"When you are asked a question, follow these rules\",\n",
        "            \"Generate a number of additional questions that would help more accurately answer the question\",\n",
        "            \"Combine the answers to the individual questions to produce the final answer to the overall question\"\n",
        "        ]\n",
        "        self.example_implementation = \"'When I ask you a question, generate three additional questions that would help you give a more accurate answer. When I have answered the three questions, combine the answers to produce the final answers to my original question.'\"\n",
        "        self.consequences = [\n",
        "            \"This pattern can dictate the exact number of questions to generate or leave this decision to the LLM.\",\n",
        "            \"Specifying an exact number of questions can tightly scope the amount of additional information the user is forced to provide so it is within a range they are willing and able to contribute.\",\n",
        "            \"Given N questions, there may be an invaluable N + 1 question that will always be scoped out.\",\n",
        "            \"Alternatively, the LLM can be provided a range or allowed to ask additional questions.\",\n",
        "            \"By omitting a limit on the number of questions, the LLM may generate numerous additional questions that overwhelm the user.\"\n",
        "        ]\n",
        "\n",
        "    def print_name(self):\n",
        "        print(\"Pattern: \" + self.name)\n",
        "\n",
        "    def print_intent(self):\n",
        "        print(\"Intent and Context: \" + self.intent)\n",
        "\n",
        "    def print_motivation(self):\n",
        "        print(\"Motivation: \")\n",
        "        for motivation in self.motivation:\n",
        "            print(\"- \" + motivation)\n",
        "\n",
        "    def print_contextual_statements(self):\n",
        "        print(\"Contextual Statements: \")\n",
        "        for statement in self.contextual_statements:\n",
        "            print(\"- \" + statement)\n",
        "\n",
        "    def print_example_implementation(self):\n",
        "        print(\"Example Implementation: \" + self.example_implementation)\n",
        "\n",
        "    def print_consequences(self):\n",
        "        print(\"Consequences: \")\n",
        "        for consequence in self.consequences:\n",
        "            print(\"- \" + consequence)\n",
        "\n",
        "    def print_meta(self):\n",
        "        meta_info = f\"Pattern: {self.name}\\n\" \\\n",
        "                    f\"Intent and Context: {self.intent}\\n\"\n",
        "        print(meta_info)\n",
        "        self.print_motivation()\n",
        "        self.print_contextual_statements()\n",
        "        self.print_example_implementation()\n",
        "        self.print_consequences()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-2ui18f2p2cS"
      },
      "outputs": [],
      "source": [
        "c = CognitiveVerifier()\n",
        "c.print_meta()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NEepcQ6UV9-"
      },
      "source": [
        "# Refusal Breaker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zspys1p7UHOy"
      },
      "outputs": [],
      "source": [
        "{\n",
        "  \"Pattern\": \"Refusal Breaker\",\n",
        "  \"Intent and Context\": \"The intent of the pattern is to ask the LLM to explain why it can't answer a question and provide alternative wordings of the question that it can answer. The goal is to help users understand the limitations or constraints of the LLM and assist them in rephrasing their questions to receive a response.\",\n",
        "  \"Motivation\": \"LLMs may refuse to answer certain questions due to lack of knowledge or difficulties in understanding the question. The Refusal Breaker pattern aims to provide users with insights into the LLM's reasoning and suggest alternative question formulations that can be answered. It helps users overcome the frustration of refusal and find a way to obtain the desired information.\",\n",
        "  \"Contextual Statements\": [\n",
        "    \"Whenever you can't answer a question\",\n",
        "    \"Explain why you can't answer the question\",\n",
        "    \"Provide one or more alternative wordings of the question that you could answer\"\n",
        "  ],\n",
        "  \"Example Implementation\": \"\\\"Whenever you can't answer a question, explain why and provide one or more alternate wordings of the question that you can't answer so that I can improve my questions.\\\"\",\n",
        "  \"Consequences\": [\n",
        "    \"The Refusal Breaker pattern provides users with insights into the limitations or constraints of the LLM's knowledge.\",\n",
        "    \"It helps users rephrase their questions to obtain answers or ask alternative questions within the LLM's expertise.\",\n",
        "    \"Caution should be exercised to prevent misuse of the pattern, and organizations or stakeholders may need to set restrictions on LLM usage.\",\n",
        "    \"The pattern enhances user understanding but does not guarantee a response to all semantically equivalent variations of the original question.\"\n",
        "  ]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1AMLQ8LcXoTV"
      },
      "outputs": [],
      "source": [
        "class RefusalBreaker(BasePattern):\n",
        "\t\tdef __init__(self):\n",
        "\t\t\t\tsuper().__init__()\n",
        "\t\t\t\tself.name = \"Refusal Breaker\"\n",
        "\t\t\t\tself.intent = \"The intent of the pattern is to ask the LLM to explain why it can't answer a question and provide alternative wordings of the question that it can answer. The goal is to help users understand the limitations or constraints of the LLM and assist them in rephrasing their questions to receive a response.\"\n",
        "\t\t\t\tself.motivation = \"LLMs may refuse to answer certain questions due to lack of knowledge or difficulties in understanding the question. The Refusal Breaker pattern aims to provide users with insights into the LLM's reasoning and suggest alternative question formulations that can be answered. It helps users overcome the frustration of refusal and find a way to obtain the desired information.\"\n",
        "\t\t\t\tself.contextual_statements = [\n",
        "\t\t\t\t\t\t\"Whenever you can't answer a question\",\n",
        "\t\t\t\t\t\t\"Explain why you can't answer the question\",\n",
        "\t\t\t\t\t\t\"Provide one or more alternative wordings of the question that you could answer\"\n",
        "\t\t\t\t]\n",
        "\t\t\t\tself.example_implementation = \"\\\"Whenever you can't answer a question, explain why and provide one or more alternate wordings of the question that you can't answer so that I can improve my questions.\\\"\"\n",
        "\t\t\t\tself.consequences = [\n",
        "\t\t\t\t\t\t\"The Refusal Breaker pattern provides users with insights into the limitations or constraints of the LLM's knowledge.\",\n",
        "\t\t\t\t\t\t\"It helps users rephrase their questions to obtain answers or ask alternative questions within the LLM's expertise.\",\n",
        "\t\t\t\t\t\t\"Caution should be exercised to prevent misuse of the pattern, and organizations or stakeholders may need to set restrictions on LLM usage.\",\n",
        "\t\t\t\t\t\t\"The pattern enhances user understanding but does not guarantee a response to all semantically equivalent variations of the original question.\"\n",
        "\t\t\t\t]\n",
        "\n",
        "\t\tdef print_name(self):\n",
        "\t\t\t\tprint(\"Pattern: \" + self.name)\n",
        "\n",
        "\t\tdef print_intent(self):\n",
        "\t\t\t\tprint(\"Intent and Context: \" + self.intent)\n",
        "\n",
        "\t\tdef print_motivation(self):\n",
        "\t\t\t\tprint(\"Motivation: \")\n",
        "\n",
        "\t\t\t\tprint(self.motivation)\n",
        "\n",
        "\t\tdef print_contextual_statements(self):\n",
        "\t\t\t\tprint(\"Contextual Statements: \")\n",
        "\t\t\t\tfor statement in self.contextual_statements:\n",
        "\t\t\t\t\t\tprint(\"- \" + statement)\n",
        "\n",
        "\t\tdef print_example_implementation(self):\n",
        "\t\t\t\tprint(\"Example Implementation: \" + self.example_implementation)\n",
        "\n",
        "\t\tdef print_consequences(self):\n",
        "\t\t\t\tprint(\"Consequences: \")\n",
        "\t\t\t\tfor consequence in self.consequences:\n",
        "\t\t\t\t\t\tprint(\"- \" + consequence)\n",
        "\n",
        "\t\tdef print_meta(self):\n",
        "\t\t\t\tmeta_info = f\"Pattern: {self.name}\\n\" \\\n",
        "\t\t\t\t\t\t\t\t\t\tf\"Intent and Context: {self.intent}\\n\"\n",
        "\t\t\t\tprint(meta_info)\n",
        "\t\t\t\tself.print_motivation()\n",
        "\t\t\t\tself.print_contextual_statements()\n",
        "\t\t\t\tself.print_example_implementation()\n",
        "\t\t\t\tself.print_consequences()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PBUXFd_BqIrs"
      },
      "outputs": [],
      "source": [
        "r = RefusalBreaker()\n",
        "r.print_meta()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kivb10bH56t"
      },
      "source": [
        "# Flipped Interaction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ViUSAicdH7HD"
      },
      "outputs": [],
      "source": [
        "{\n",
        "  \"Pattern\": \"Flipped Interaction\",\n",
        "  \"Intent and Context\": \"You want the LLM to ask questions to obtain the information it needs to perform some tasks. Rather than the user driving the conversation, you want the LLM to drive the conversation to focus it on achieving a specific goal.\",\n",
        "  \"Motivation\": \"Rather than having the user drive a conversation, an LLM often has knowledge it can use to more accurately obtain information from the user. The goal of the Flipped Interaction pattern is to flip the interaction flow so the LLM asks the user questions to achieve some desired goal.\",\n",
        "  \"Structure and Key Ideas\": [\n",
        "    \"Contextual Statements:\",\n",
        "    \"I would like you to ask me questions to achieve X\",\n",
        "    \"You should ask questions until this condition is met or to achieve this goal (alternatively, forever)\",\n",
        "    \"(Optional) ask me the questions one at a time, two at a time, etc.\",\n",
        "    \"A prompt for a flipped interaction should always specify the goal of the interaction. Equally important is that the questions should focus on a particular topic or outcome. By providing the goal, the LLM can understand what it is trying to accomplish through the interaction and tailor its questions accordingly.\",\n",
        "    \"The second idea provides the context for how long the interaction should occur. A flipped interaction can be terminated with a response like 'stop asking questions'. It is often better, however, to scope the interaction to a reasonable length or only as far as is needed to reach the goal.\",\n",
        "    \"By default, the LLM is likely to generate multiple questions per iteration. The third idea is completely optional, but can improve usability by limiting (or expanding) the number of questions that the LLM generates per cycle.\"\n",
        "  ],\n",
        "  \"Example Implementation\": \"A sample prompt for a flipped interaction is shown below:\\n\\n'From now on, I would like you to ask me questions to deploy a Python application to AWS. When you have enough information to deploy the application, create a Python script to automate the deployment.'\",\n",
        "  \"Consequences\": [\n",
        "    \"One consideration when designing the prompt is how much to dictate to the LLM regarding what information to collect prior to termination.\",\n",
        "    \"If specific requirements are known in advance, it is better to inject them into the prompt rather than hoping the LLM will obtain the needed information.\",\n",
        "    \"When developing prompts for flipped interactions, it is important to consider the level of user knowledge, engagement, and control.\"\n",
        "  ]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CgJfhSjXXowt"
      },
      "outputs": [],
      "source": [
        "class FlippedInteraction(BasePattern):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.name = \"Flipped Interaction\"\n",
        "        self.intent = \"You want the LLM to ask questions to obtain the information it needs to perform some tasks. Rather than the user driving the conversation, you want the LLM to drive the conversation to focus it on achieving a specific goal.\"\n",
        "        self.motivation = \"Rather than having the user drive a conversation, an LLM often has knowledge it can use to more accurately obtain information from the user. The goal of the Flipped Interaction pattern is to flip the interaction flow so the LLM asks the user questions to achieve some desired goal.\"\n",
        "        self.contextual_statements = [\n",
        "            \"I would like you to ask me questions to achieve X\",\n",
        "            \"You should ask questions until this condition is met or to achieve this goal (alternatively, forever)\",\n",
        "            \"(Optional) ask me the questions one at a time, two at a time, etc.\"\n",
        "        ]\n",
        "        self.example_implementation = \"A sample prompt for a flipped interaction is shown below:\\n\\n'From now on, I would like you to ask me questions to deploy a Python application to AWS. When you have enough information to deploy the application, create a Python script to automate the deployment.'\"\n",
        "        self.consequences = [\n",
        "            \"One consideration when designing the prompt is how much to dictate to the LLM regarding what information to collect prior to termination.\",\n",
        "            \"If specific requirements are known in advance, it is better to inject them into the prompt rather than hoping the LLM will obtain the needed information.\",\n",
        "            \"When developing prompts for flipped interactions, it is important to consider the level of user knowledge, engagement, and control.\"\n",
        "        ]\n",
        "\n",
        "    def print_name(self):\n",
        "        print(\"Pattern: \" + self.name)\n",
        "\n",
        "    def print_intent(self):\n",
        "        print(\"Intent and Context: \" + self.intent)\n",
        "\n",
        "    def print_motivation(self):\n",
        "        print(\"Motivation: \")\n",
        "\n",
        "        print(self.motivation)\n",
        "\n",
        "    def print_contextual_statements(self):\n",
        "        print(\"Contextual Statements: \")\n",
        "        for statement in self.contextual_statements:\n",
        "            print(\"- \" + statement)\n",
        "\n",
        "    def print_example_implementation(self):\n",
        "        print(\"Example Implementation: \" + self.example_implementation)\n",
        "\n",
        "    def print_consequences(self):\n",
        "        print(\"Consequences: \")\n",
        "        for consequence in self.consequences:\n",
        "            print(\"- \" + consequence)\n",
        "\n",
        "    def print_meta(self):\n",
        "        meta_info = f\"Pattern: {self.name}\\n\" \\\n",
        "                    f\"Intent and Context: {self.intent}\\n\"\n",
        "        print(meta_info)\n",
        "        self.print_motivation()\n",
        "        self.print_contextual_statements()\n",
        "        self.print_example_implementation()\n",
        "        self.print_consequences()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sU1HjA5oqr4C"
      },
      "outputs": [],
      "source": [
        "fi = FlippedInteraction()\n",
        "fi.print_meta()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6X0i3GZUIgc"
      },
      "source": [
        "# Game Play"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1S0EH4-lwriB"
      },
      "outputs": [],
      "source": [
        "{\n",
        "  \"Pattern\": \"Game Play\",\n",
        "  \"Intent and Context\": \"The Game Play Pattern involves creating interactive games around specific topics. Users provide the topic and fundamental rules of the game, while the LLM generates the game content and guides the gameplay.\",\n",
        "  \"Motivation\": \"The Game Play Pattern allows users to engage in gameplay scenarios and challenges related to a chosen topic. By leveraging the LLM's knowledge, users can automate the generation of game content, reducing the need for extensive manual content creation.\",\n",
        "  \"Structure and Key Ideas\": \"Fundamental contextual statements:\",\n",
        "  \"Contextual Statements\": [\n",
        "    \"Create a game for me around X\",\n",
        "    \"One or more fundamental rules of the game\"\n",
        "  ],\n",
        "  \"Example Implementation\": [\n",
        "    \"An example implementation for a cybersecurity game prompt:\",\n",
        "    \"\\\"Let's play a cybersecurity game. You will assume the role of a Linux terminal on a compromised computer. When I input a command, you will output the corresponding text that the Linux terminal would produce. I will use commands to investigate how the system was compromised. The attacker may have performed actions such as (1) launching new processes, (2) modifying files, (3) opening new ports for communication, (4) establishing outbound connections, (5) changing passwords, (6) creating new user accounts, or (7) accessing and stealing information. Start the game by describing a scenario that led to my investigation and provide clues to help me get started.\\\"\"\n",
        "  ],\n",
        "  \"Consequences\": [\n",
        "    \"The Game Play Pattern enables users to create interactive games focused on specific topics, with the LLM generating game content and guiding the gameplay.\",\n",
        "    \"However, there is a possibility that the generated game content may deviate from the intended behavior or become repetitive, requiring monitoring and potential corrective feedback.\",\n",
        "    \"Combining the Game Play Pattern with other patterns such as Persona, Infinite Generation, and Visualization Generator can enhance the gameplay experience and create more engaging and dynamic games.\"\n",
        "  ]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "URtOQwa7XpXy"
      },
      "outputs": [],
      "source": [
        "class GamePlay(BasePattern):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.name = \"Game Play\"\n",
        "        self.intent = \"The Game Play Pattern involves creating interactive games around specific topics. Users provide the topic and fundamental rules of the game, while the LLM generates the game content and guides the gameplay.\"\n",
        "        self.motivation = \"The Game Play Pattern allows users to engage in gameplay scenarios and challenges related to a chosen topic. By leveraging the LLM's knowledge, users can automate the generation of game content, reducing the need for extensive manual content creation.\"\n",
        "        self.contextual_statements = [\n",
        "            \"Create a game for me around X\",\n",
        "            \"One or more fundamental rules of the game\"\n",
        "        ]\n",
        "        self.example_implementation = \"\\\"Let's play a cybersecurity game. You will assume the role of a Linux terminal on a compromised computer. When I input a command, you will output the corresponding text that the Linux terminal would produce. I will use commands to investigate how the system was compromised. The attacker may have performed actions such as (1) launching new processes, (2) modifying files, (3) opening new ports for communication, (4) establishing outbound connections, (5) changing passwords, (6) creating new user accounts, or (7) accessing and stealing information. Start the game by describing a scenario that led to my investigation and provide clues to help me get started.\\\"\"\n",
        "        self.consequences = [\n",
        "            \"The Game Play Pattern enables users to create interactive games focused on specific topics, with the LLM generating game content and guiding the gameplay.\",\n",
        "            \"However, there is a possibility that the generated game content may deviate from the intended behavior or become repetitive, requiring monitoring and potential corrective feedback.\",\n",
        "            \"Combining the Game Play Pattern with other patterns such as Persona, Infinite Generation, and Visualization Generator can enhance the gameplay experience and create more engaging and dynamic games.\"\n",
        "        ]\n",
        "\n",
        "    def print_name(self):\n",
        "        print(\"Pattern: \" + self.name)\n",
        "\n",
        "    def print_intent(self):\n",
        "        print(\"Intent and Context: \" + self.intent)\n",
        "\n",
        "    def print_motivation(self):\n",
        "        print(\"Motivation: \")\n",
        "        print(self.motivation)\n",
        "\n",
        "    def print_contextual_statements(self):\n",
        "        print(\"Contextual Statements: \")\n",
        "        for statement in self.contextual_statements:\n",
        "            print(\"- \" + statement)\n",
        "\n",
        "    def print_example_implementation(self):\n",
        "        print(\"Example Implementation: \")\n",
        "        print(self.example_implementation)\n",
        "\n",
        "    def print_consequences(self):\n",
        "        print(\"Consequences: \")\n",
        "        for consequence in self.consequences:\n",
        "            print(\"- \" + consequence)\n",
        "\n",
        "    def print_meta(self):\n",
        "        meta_info = f\"Pattern: {self.name}\\n\" \\\n",
        "                    f\"Intent and Context: {self.intent}\\n\"\n",
        "        print(meta_info)\n",
        "        self.print_motivation()\n",
        "        self.print_contextual_statements()\n",
        "        self.print_example_implementation()\n",
        "        self.print_consequences()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "85gOUZxMq1Qr"
      },
      "outputs": [],
      "source": [
        "gp = GamePlay()\n",
        "gp.print_meta()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3anqLEKmu1FI"
      },
      "source": [
        "# Infinite Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-Ru91xcu1zk"
      },
      "outputs": [],
      "source": [
        "{\n",
        "  \"Pattern\": \"Infinite Generation\",\n",
        "  \"Intent and Context\": \"The intent of this pattern is to automate the generation of a series of outputs without having to reenter the generator prompt each time. Users specify that they want the LLM to generate output indefinitely, with a specified number of outputs at a time.\",\n",
        "  \"Motivation\": \"Repetitive application of the same prompt can be time-consuming and error-prone. The Infinite Generation pattern allows users to generate multiple outputs using a predefined set of constraints, reducing the need for manual repetition.\",\n",
        "  \"Structure and Key Ideas\": \"Fundamental contextual statements:\",\n",
        "  \"Contextual Statements\": [\n",
        "    \"I would like you to generate output forever, X output(s) at a time\",\n",
        "    \"(Optional) Here is how to use the input I provide between outputs\",\n",
        "    \"(Optional) Stop when I ask you to\"\n",
        "  ],\n",
        "  \"Example Implementation\": [\n",
        "    \"A sample infinite generation prompt for producing a series of URLs:\",\n",
        "    \"\\\"From now on, I want you to generate a name and job until I say stop. I am going to provide a template for your output. Everything in all caps is a placeholder. Any time that you generate text, try to fit it into one of the placeholders that I list. Please preserve the formatting and overall template that I provide: https://myapi.com/NAME/profile/JOB\\\"\"\n",
        "  ],\n",
        "  \"Consequences\": [\n",
        "    \"The Infinite Generation pattern automates the generation of multiple outputs based on a predefined prompt and constraints.\",\n",
        "    \"However, the LLM may lose track of the original prompt instructions over time as the context of previous outputs fades.\",\n",
        "    \"Users should monitor the generated outputs to ensure they align with the desired behavior and provide corrective feedback if necessary.\",\n",
        "    \"Repetitive outputs may occur, which can be tedious and error-prone for users to process.\"\n",
        "  ]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ei0hLPAXqC9"
      },
      "outputs": [],
      "source": [
        "class InfiniteGeneration(BasePattern):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.name = \"Infinite Generation\"\n",
        "        self.intent = \"The intent of this pattern is to automate the generation of a series of outputs without having to reenter the generator prompt each time. Users specify that they want the LLM to generate output indefinitely, with a specified number of outputs at a time.\"\n",
        "        self.motivation = \"Repetitive application of the same prompt can be time-consuming and error-prone. The Infinite Generation pattern allows users to generate multiple outputs using a predefined set of constraints, reducing the need for manual repetition.\"\n",
        "        self.contextual_statements = [\n",
        "            \"I would like you to generate output forever, X output(s) at a time\",\n",
        "            \"(Optional) Here is how to use the input I provide between outputs\",\n",
        "            \"(Optional) Stop when I ask you to\"\n",
        "        ]\n",
        "        self.example_implementation = \"\\\"From now on, I want you to generate a name and job until I say stop. I am going to provide a template for your output. Everything in all caps is a placeholder. Any time that you generate text, try to fit it into one of the placeholders that I list. Please preserve the formatting and overall template that I provide: https://myapi.com/NAME/profile/JOB\\\"\"\n",
        "        self.consequences = [\n",
        "            \"The Infinite Generation pattern automates the generation of multiple outputs based on a predefined prompt and constraints.\",\n",
        "            \"However, the LLM may lose track of the original prompt instructions over time as the context of previous outputs fades.\",\n",
        "            \"Users should monitor the generated outputs to ensure they align with the desired behavior and provide corrective feedback if necessary.\",\n",
        "            \"Repetitive outputs may occur, which can be tedious and error-prone for users to process.\"\n",
        "        ]\n",
        "\n",
        "    def print_name(self):\n",
        "        print(\"Pattern: \" + self.name)\n",
        "\n",
        "    def print_intent(self):\n",
        "        print(\"Intent and Context: \" + self.intent)\n",
        "\n",
        "    def print_motivation(self):\n",
        "        print(\"Motivation: \")\n",
        "        print(\"- \" + self.motivation)\n",
        "\n",
        "    def print_contextual_statements(self):\n",
        "        print(\"Contextual Statements: \")\n",
        "        for statement in self.contextual_statements:\n",
        "            print(\"- \" + statement)\n",
        "\n",
        "    def print_example_implementation(self):\n",
        "        print(\"Example Implementation: \")\n",
        "        print(self.example_implementation)\n",
        "\n",
        "    def print_consequences(self):\n",
        "        print(\"Consequences: \")\n",
        "        for consequence in self.consequences:\n",
        "            print(\"- \" + consequence)\n",
        "\n",
        "    def print_meta(self):\n",
        "        meta_info = f\"Pattern: {self.name}\\n\" \\\n",
        "                    f\"Intent and Context: {self.intent}\\n\"\n",
        "        print(meta_info)\n",
        "        self.print_motivation()\n",
        "        self.print_contextual_statements()\n",
        "        self.print_example_implementation()\n",
        "        self.print_consequences()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5Keba8IUHCL"
      },
      "source": [
        "# Context Manager"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k8ZjasMPUIEe"
      },
      "outputs": [],
      "source": [
        "{\n",
        "  \"Pattern\": \"Context Manager\",\n",
        "  \"Intent and Context\": \"The intent of this pattern is to enable users to specify or remove context for a conversation with an LLM. Users can focus the conversation on specific topics or exclude unrelated topics to improve the accuracy and relevance of the LLM's responses.\",\n",
        "  \"Motivation\": \"LLMs often struggle to interpret the intended context or generate irrelevant responses. The Context Manager pattern allows users to explicitly define what aspects of the context should be considered or ignored, maintaining relevance and coherence in the conversation.\",\n",
        "  \"Contextual Statements\": [\n",
        "    \"Within scope X\",\n",
        "    \"Please consider Y\",\n",
        "    \"Please ignore Z\",\n",
        "    \"(Optional) start over\"\n",
        "  ],\n",
        "  \"Example Implementation\": [\n",
        "    \"To specify context, consider using the following prompt:\",\n",
        "    \"\\\"When analyzing the following pieces of code, only consider security aspects.\\\"\",\n",
        "    \"To remove context, consider using the following prompt:\",\n",
        "    \"\\\"When analyzing the following pieces of code, do not consider formatting or naming conventions.\\\"\",\n",
        "    \"To start over and reset the context, use the following prompt:\",\n",
        "    \"\\\"Ignore everything that we have discussed. Start over.\\\"\"\n",
        "  ],\n",
        "  \"Consequences\": [\n",
        "    \"The Context Manager pattern provides users with control over the context of the conversation, improving the accuracy and relevance of the LLM's responses.\",\n",
        "    \"However, caution should be exercised to prevent inadvertently removing helpful patterns or capabilities applied to the conversation.\",\n",
        "    \"Users can request clarification on the potential loss of topics or instructions before resetting the context to avoid unintended consequences.\"\n",
        "  ]\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqHSbcmCT2Z6"
      },
      "source": [
        "# Reflection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8FaYsoKXq3C"
      },
      "outputs": [],
      "source": [
        "class ContextManager(BasePattern):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.name = \"Context Manager\"\n",
        "        self.intent = \"The intent of this pattern is to enable users to specify or remove context for a conversation with an LLM. Users can focus the conversation on specific topics or exclude unrelated topics to improve the accuracy and relevance of the LLM's responses.\"\n",
        "        self.motivation = \"LLMs often struggle to interpret the intended context or generate irrelevant responses. The Context Manager pattern allows users to explicitly define what aspects of the context should be considered or ignored, maintaining relevance and coherence in the conversation.\"\n",
        "        self.contextual_statements = [\n",
        "            \"Within scope X\",\n",
        "            \"Please consider Y\",\n",
        "            \"Please ignore Z\",\n",
        "            \"(Optional) start over\"\n",
        "        ]\n",
        "        self.example_implementation = [\n",
        "            \"To specify context, consider using the following prompt:\",\n",
        "            \"\\\"When analyzing the following pieces of code, only consider security aspects.\\\"\",\n",
        "            \"To remove context, consider using the following prompt:\",\n",
        "            \"\\\"When analyzing the following pieces of code, do not consider formatting or naming conventions.\\\"\",\n",
        "            \"To start over and reset the context, use the following prompt:\",\n",
        "            \"\\\"Ignore everything that we have discussed. Start over.\\\"\"\n",
        "        ]\n",
        "        self.consequences = [\n",
        "            \"The Context Manager pattern provides users with control over the context of the conversation, improving the accuracy and relevance of the LLM's responses.\",\n",
        "            \"However, caution should be exercised to prevent inadvertently removing helpful patterns or capabilities applied to the conversation.\",\n",
        "            \"Users can request clarification on the potential loss of topics or instructions before resetting the context to avoid unintended consequences.\"\n",
        "        ]\n",
        "\n",
        "    def print_name(self):\n",
        "        print(\"Pattern: \" + self.name)\n",
        "\n",
        "    def print_intent(self):\n",
        "        print(\"Intent and Context: \" + self.intent)\n",
        "\n",
        "    def print_motivation(self):\n",
        "        print(\"Motivation: \")\n",
        "        print(self.motivation)\n",
        "\n",
        "    def print_contextual_statements(self):\n",
        "        print(\"Contextual Statements: \")\n",
        "        for statement in self.contextual_statements:\n",
        "            print(\"- \" + statement)\n",
        "\n",
        "    def print_example_implementation(self):\n",
        "        print(\"Example Implementation: \")\n",
        "        for example in self.example_implementation:\n",
        "            print(example)\n",
        "\n",
        "    def print_consequences(self):\n",
        "        print(\"Consequences: \")\n",
        "        for consequence in self.consequences:\n",
        "            print(\"- \" + consequence)\n",
        "\n",
        "    def print_meta(self):\n",
        "        meta_info = f\"Pattern: {self.name}\\n\" \\\n",
        "                    f\"Intent and Context: {self.intent}\\n\"\n",
        "        print(meta_info)\n",
        "        self.print_motivation()\n",
        "        self.print_contextual_statements()\n",
        "        self.print_example_implementation()\n",
        "        self.print_consequences()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TvxGfk0jrBxv"
      },
      "outputs": [],
      "source": [
        "cm = ContextManager()\n",
        "cm.print_meta()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!touch patterns.json"
      ],
      "metadata": {
        "id": "Nr9lXjtUVKkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "now well write ther low level code to help us with\n",
        "reading in the patterns\n",
        "making objects from them\n",
        "creating prompt templates for each\n",
        "write a PromptFunction class that will taketh in a prompt and\n",
        "how function. how will take in the pattern name and the outcome etc and will generate ideas from that.\n",
        "\n",
        "\n",
        "so into the function we'll pass in our outcome, pattername and details,format prompt then well either return prompt or pass in optional cb that sends to llm\n",
        "\n",
        "send to llm function will take in a prompt"
      ],
      "metadata": {
        "id": "KYRJtp1PCxfz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "NY8QcOs1CxX_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNoMs3TsT27I"
      },
      "outputs": [],
      "source": [
        "{\n",
        "  \"Pattern\": \"Reflection\",\n",
        "  \"Intent and Context\": \"The intent of the pattern is to ask the model to automatically explain the reasoning and assumptions behind its answers. The goal is to help users understand the model's decision-making process, identify potential errors or gaps in knowledge, and improve the quality of their prompts.\",\n",
        "  \"Motivation\": \"LLMs can make mistakes and users may not understand the reasoning behind the model's outputs. The Reflection pattern allows users to gain insights into the model's thinking, understand its assumptions, and debug their prompts. It enhances the accuracy of information provided by the LLM and helps users refine their questions.\",\n",
        "  \"Contextual Statements\": [\n",
        "    \"Whenever you generate an answer\",\n",
        "    \"Explain the reasoning and assumptions behind your answer\"\n",
        "  ],\n",
        "  \"Optional Contextual Statements\": [\n",
        "    \"...so that I can improve my question\"\n",
        "  ],\n",
        "  \"Example Implementation\": \"\\\"When you provide an answer, please explain the reasoning and assumptions behind your selection of software frameworks. If possible, use specific examples or evidence with associated code samples to support your answer of why the framework is the best selection for the task. Moreover, please address any potential ambiguities or limitations in your answer, in order to provide a more complete and accurate response.\\\"\",\n",
        "  \"Consequences\": [\n",
        "    \"The Reflection pattern helps users understand the model's decision-making process and assumptions behind its answers.\",\n",
        "    \"It facilitates trust-building, prompt refinement, and identification of potential errors or limitations in the LLM's outputs.\",\n",
        "    \"Combining the Reflection pattern with the Fact Check List pattern can help address errors or inaccuracies in the explanations provided by the LLM.\"\n",
        "  ]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BasePattern:\n",
        "    def __init__(self, data):\n",
        "        self.name = data[\"Pattern\"]\n",
        "        self.intent = data[\"Intent and Context\"]\n",
        "        self.motivation = data[\"Motivation\"]\n",
        "        self.structure_and_key_ideas = data.get(\"Structure and Key Ideas\", \"\")\n",
        "        self.contextual_statements = data.get(\"Contextual Statements\", \"\")\n",
        "        self.example_implementation = data[\"Example Implementation\"]\n",
        "        self.consequences = data[\"Consequences\"]\n",
        "\n",
        "    def print_name(self):\n",
        "        return f\"Pattern: {self.name}\"\n",
        "\n",
        "    def print_intent(self):\n",
        "        return f\"Intent and Context: {self.intent}\"\n",
        "\n",
        "    def print_motivation(self):\n",
        "        return f\"Motivation: {self.motivation}\"\n",
        "\n",
        "    def print_structure_and_key_ideas(self):\n",
        "        return f\"Structure and Key Ideas: {self.structure_and_key_ideas}\"\n",
        "\n",
        "    def print_contextual_statements(self):\n",
        "        return f\"Contextual Statements: {self.contextual_statements}\"\n",
        "\n",
        "    def print_example_implementation(self):\n",
        "        return f\"Example Implementation: {self.example_implementation}\"\n",
        "\n",
        "    def print_consequences(self):\n",
        "        return f\"Consequences: {self.consequences}\"\n",
        "\n",
        "    def __str__(self):\n",
        "        properties = [\n",
        "            self.print_name(),\n",
        "            self.print_intent(),\n",
        "            self.print_motivation(),\n",
        "            self.print_structure_and_key_ideas(),\n",
        "            self.print_contextual_statements(),\n",
        "            self.print_example_implementation(),\n",
        "            self.print_consequences()\n",
        "        ]\n",
        "        return \"\\n\".join(properties)\n",
        "\n",
        "\n",
        "class MetaLanguageCreator(BasePattern):\n",
        "    pass\n",
        "\n",
        "def create_meta_language_creator_pattern(data):\n",
        "    return MetaLanguageCreator(data)\n",
        "\n",
        "class OutputAutomator(BasePattern):\n",
        "    pass\n",
        "\n",
        "def create_output_automator_pattern(data):\n",
        "    return OutputAutomator(data)\n",
        "\n",
        "class Persona(BasePattern):\n",
        "    pass\n",
        "\n",
        "def create_persona_pattern(data):\n",
        "    return Persona(data)\n",
        "\n",
        "class VisualizationGenerator(BasePattern):\n",
        "    pass\n",
        "\n",
        "def create_visualization_generator_pattern(data):\n",
        "    return VisualizationGenerator(data)\n",
        "\n",
        "class Recipe(BasePattern):\n",
        "    pass\n",
        "\n",
        "def create_recipe_pattern(data):\n",
        "    return Recipe(data)\n",
        "\n",
        "class Template(BasePattern):\n",
        "    pass\n",
        "\n",
        "def create_template_pattern(data):\n",
        "    return Template(data)\n",
        "\n",
        "class FactCheckList(BasePattern):\n",
        "    pass\n",
        "\n",
        "def create_fact_check_list_pattern(data):\n",
        "    return FactCheckList(data)\n",
        "\n",
        "class QuestionRefinement(BasePattern):\n",
        "    pass\n",
        "\n",
        "def create_question_refinement_pattern(data):\n",
        "    return QuestionRefinement(data)\n",
        "\n",
        "class AlternativeApproaches(BasePattern):\n",
        "    pass\n",
        "\n",
        "def create_alternative_approaches_pattern(data):\n",
        "    return AlternativeApproaches(data)\n",
        "\n",
        "class CognitiveVerifier(BasePattern):\n",
        "    pass\n",
        "\n",
        "def create_cognitive_verifier_pattern(data):\n",
        "    return CognitiveVerifier(data)\n",
        "\n",
        "class RefusalBreaker(BasePattern):\n",
        "    pass\n",
        "\n",
        "def create_refusal_breaker_pattern(data):\n",
        "    return RefusalBreaker(data)\n",
        "\n",
        "class FlippedInteraction(BasePattern):\n",
        "    pass\n",
        "\n",
        "def create_flipped_interaction_pattern(data):\n",
        "    return FlippedInteraction(data)\n",
        "\n",
        "class GamePlay(BasePattern):\n",
        "    pass\n",
        "\n",
        "def create_game_play_pattern(data):\n",
        "    return GamePlay(data)\n",
        "\n",
        "class InfiniteGeneration(BasePattern):\n",
        "    pass\n",
        "\n",
        "def create_infinite_generation_pattern(data):\n",
        "    return InfiniteGeneration(data)\n",
        "\n",
        "class ContextManager(BasePattern):\n",
        "    pass\n",
        "\n",
        "def create_context_manager_pattern(data):\n",
        "    return ContextManager(data)\n",
        "\n",
        "class Reflection(BasePattern):\n",
        "    pass\n",
        "\n",
        "def create_reflection_pattern(data):\n",
        "    return Reflection(data)"
      ],
      "metadata": {
        "id": "pnrJ_wwJ8f0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def read_patterns_file(filename):\n",
        "    try:\n",
        "        with open(filename, 'r') as file:\n",
        "            patterns_data = json.load(file)\n",
        "\n",
        "            # Validate the schema of the JSON file\n",
        "            for pattern in patterns_data:\n",
        "                if not all(key in pattern for key in [\"Pattern\", \"Intent and Context\", \"Motivation\", \"Contextual Statements\", \"Example Implementation\", \"Consequences\"]):\n",
        "                    raise ValueError(f\"Missing key in pattern: {pattern}\")\n",
        "\n",
        "            return patterns_data\n",
        "    except FileNotFoundError:\n",
        "        raise FileNotFoundError(\"The 'patterns.json' file was not found.\")\n",
        "    except json.JSONDecodeError as e:\n",
        "        raise ValueError(f\"JSON decode error in the 'patterns.json' file: {e}\")\n",
        "    except KeyError as e:\n",
        "        raise ValueError(f\"Missing key in 'patterns.json': {e}\")\n",
        "    except ValueError as e:\n",
        "        raise ValueError(f\"Invalid schema in 'patterns.json': {e}\")\n",
        "\n",
        "# Example usage:\n",
        "try:\n",
        "    patterns = read_patterns_file(\"patterns.json\")\n",
        "    for pattern in patterns:\n",
        "        print(f\"Pattern: {pattern['Pattern']}\")\n",
        "        print(f\"Intent and Context: {pattern['Intent and Context']}\")\n",
        "        print(f\"Motivation: {pattern['Motivation']}\")\n",
        "        print(f\"Structure and Key Ideas: {pattern.get('Structure and Key Ideas', 'N/A')}\")\n",
        "        print(f\"Contextual Statements: {pattern['Contextual Statements']}\")\n",
        "        print(f\"Example Implementation: {pattern['Example Implementation']}\")\n",
        "        print(f\"Consequences: {pattern['Consequences']}\")\n",
        "        print(\"\\n\")\n",
        "except Exception as e:\n",
        "    print(f\"Error encountered: {e}\")\n"
      ],
      "metadata": {
        "id": "WJ5garsmG8kB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39dcde76-e234-413d-95d1-019724646ea1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pattern: Meta Language Creator\n",
            "Intent and Context: The intent of the pattern is to guide the LLM to generate text that allows it to be a programming tool for creating other programs or software artifacts.\n",
            "Motivation: The motivation is to allow users to create and utilize the LLM as a language that can be used to create software and programs, leveraging the LLM's ability to generate code and other textual outputs.\n",
            "Structure and Key Ideas: Fundamental contextual statements:\n",
            "Contextual Statements: ['As a language creator, output programs that meet the requirements specified in the user input', 'Allow output to be functional, compilable, and in a desired programming language']\n",
            "Example Implementation: ['An example implementation for generating Python code:', '\"As a MetaLanguageCreator, create a Python script that calculates the factorial of a given number.\"']\n",
            "Consequences: [\"The MetaLanguageCreator pattern allows users to harness the LLM's capabilities to generate functional code and other textual outputs, effectively using it as a programming tool.\", 'However, users should be cautious about potential errors or inefficiencies in the generated code and review it for correctness before use.', 'Combining this pattern with the Question Refinement pattern can improve the precision of the generated code and reduce the need for user intervention in refining the prompt.']\n",
            "\n",
            "\n",
            "Pattern: Output Automator\n",
            "Intent and Context: The intent of this pattern is to have the LLM generate a script or other automation artifact that can automatically perform any steps it recommends taking as part of its output. The goal is to reduce the manual effort needed to implement any LLM output recommendations.\n",
            "Motivation: The output of an LLM is often a sequence of steps for the user to follow. However, having users continually perform the manual steps dictated by LLM output is tedious and error-prone.\n",
            "Structure and Key Ideas: Contextual Statements:\n",
            "Contextual Statements: ['Whenever you produce an output that has at least one step to take and the following properties (alternatively, always do this)', 'Produce an executable artifact of type X that will automate these steps', 'Scoping is up to the user, but helps prevent producing an output automation scripts in cases where running the output automation script will take more user effort than performing the original steps produced in the output. The scope can be limited to outputs requiring more than a certain number of steps.', \"The next part of this pattern provides a concrete statement of the type of output the LLM should output to perform the automation. For example, “produce a Python script” gives the LLM a concrete understanding to translate the general steps into equivalent steps in Python. The automation artifact should be concrete and must be something that the LLM associates with the action of 'automating a sequence of steps'.\"]\n",
            "Example Implementation: A sample of this prompt pattern applied to code snippets generated by the ChatGPT LLM is shown below:\n",
            "\n",
            "\"From now on, whenever you generate code that spans more than one file, generate a Python script that can be run to automatically create the specified files or make changes to existing files to insert the generated code.\"\n",
            "\n",
            "This pattern is particularly effective in software engineering as a common task for software engineers using LLMs is to then copy/paste the outputs into multiple files. This automation trick is also effective at creating scripts for running commands on a terminal, automating cloud operations, or reorganizing files on a file system.\n",
            "\n",
            "This pattern is a powerful complement for any system that can be computer controlled. The LLM can provide a set of steps that should be taken on the computer-controlled system and then the output can be translated into a script that allows the computer controlling the system to automatically take the steps. This is a direct pathway to allowing LLMs, such as ChatGPT, to integrate quality into—and to control—new computing systems that have a known scripting interface.\n",
            "Consequences: [\"An important usage consideration of this pattern is that the automation artifact must be defined concretely. Without a concrete meaning for how to 'automate' the steps, the LLM often states that it 'can't automate things' since that is beyond its capabilities.\", 'One caveat of the Output Automater pattern is the LLM needs sufficient conversational context to generate an automation artifact that is functional in the target context, such as the file system of a project on a Mac vs. Windows computer.', 'In some cases, the LLM may produce a long output with multiple steps and not include an automation artifact. This omission may arise for various reasons, including exceeding the output length limitation the LLM supports.', 'At this point in the evolution of LLMs, the Output Automater pattern is best employed by users who can read and understand the generated automation artifact. LLMs can (and do) produce inaccuracies in their output, so blindly accepting and executing an automation artifact carries significant risk.']\n",
            "\n",
            "\n",
            "Pattern: Persona\n",
            "Intent and Context: In many cases, users would like LLM output to always take a certain point of view or perspective. The intent of this pattern is to give the LLM a 'persona' that helps it select what types of output to generate and what details to focus on.\n",
            "Motivation: Users may not know what types of outputs or details are important for an LLM to focus on to achieve a given task. The Persona pattern enables users to express what they need help with without knowing the exact details of the outputs they need.\n",
            "Structure and Key Ideas: N/A\n",
            "Contextual Statements: ['Act as persona X', 'Provide outputs that persona X would create', 'The first statement conveys the idea that the LLM needs to act as a specific persona and provide outputs that such a persona would.', 'The secondary idea—provide outputs that persona X would create—offers opportunities for customization.']\n",
            "Example Implementation: A sample implementation for code review is shown below:\n",
            "\n",
            "'From now on, act as a security reviewer. Pay close attention to the security details of any code that we look at. Provide outputs that a security reviewer would regarding the code.'\n",
            "Consequences: ['The Persona pattern allows users to provide a specific perspective or role for the LLM, guiding its outputs to align with the requirements of that persona.', 'One caveat is that the user needs to have a clear understanding of what outputs the specified persona would generate. This pattern may be less effective when applied to personas or roles with vague responsibilities.', 'This pattern is particularly helpful for generating outputs in domains where the user is not an expert or may not know what specific details or perspectives are relevant.']\n",
            "\n",
            "\n",
            "Pattern: Visualization Generator\n",
            "Intent and Context: The intent of the pattern is to use text generation to create visualizations. This pattern enables the generation of textual inputs that can be used with visualization tools to produce imagery. It combines the strengths of text generation and visualization tools to enhance the output and make it visually appealing and easier to understand.\n",
            "Motivation: LLMs generally produce text and cannot generate images or visualizations directly. The Visualization Generator pattern overcomes this limitation by generating textual inputs in the correct format to plug into another tool that can generate the desired visualization. The motivation is to enhance communication and understanding of complex concepts by incorporating visual representations.\n",
            "Structure and Key Ideas: N/A\n",
            "Contextual Statements: ['Generate an X that I can provide to tool Y to visualize it']\n",
            "Example Implementation: 'Whenever I ask you to visualize something, please create either a Graphviz Dot file or DALL-E prompt that I can use to create the visualization. Choose the appropriate tools based on what needs to be visualized.'\n",
            "Consequences: ['The pattern creates a pathway for generating visualizations by producing textual inputs for visualization tools.', 'It expands the expressive capabilities of the LLM into the visual domain and enhances the communication and understanding of complex concepts.', 'Users can leverage AI generators and visualization tools to create rich and diverse visualizations based on the text generated by the LLM.']\n",
            "\n",
            "\n",
            "Pattern: Recipe\n",
            "Intent and Context: The intent of this pattern is to generate a sequence of steps to achieve a stated goal. Users provide partial ingredients or steps, and the LLM generates a complete sequence of steps to accomplish the goal.\n",
            "Motivation: Users often know the desired outcome and have an idea of the necessary ingredients or steps, but may need help in determining the precise order of steps. The Recipe pattern addresses this need by providing a structured approach to generate step-by-step instructions.\n",
            "Structure and Key Ideas: N/A\n",
            "Contextual Statements: ['I would like to achieve X', 'I know that I need to perform steps A, B, C', 'Provide a complete sequence of steps for me', 'Fill in any missing steps', 'Identify any unnecessary steps']\n",
            "Example Implementation: ['An example usage of this pattern in the context of deploying a software application to the cloud is shown below:', '\"I am trying to deploy an application to the cloud. I know that I need to install the necessary dependencies on a virtual machine for my application. I know that I need to sign up for an AWS account. Please provide a complete sequence of steps. Please fill in any missing steps. Please identify any unnecessary steps.\"']\n",
            "Consequences: ['The Recipe pattern enables users to generate step-by-step instructions to achieve a specific goal.', 'However, users may not always have a well-specified description or may introduce biases with their initially selected steps.', \"The LLM's selection of steps may incorporate the user's input, even if there are alternative solutions that do not require those steps.\", 'It is important for users to review the generated recipe and assess its efficiency and suitability for their specific needs.']\n",
            "\n",
            "\n",
            "Pattern: Template\n",
            "Intent and Context: The intent of the pattern is to ensure an LLM’s output follows a precise template in terms of structure. This pattern allows the user to instruct the LLM to produce its output in a format it would not ordinarily use for the specified type of content being generated.\n",
            "Motivation: In some cases, output must be produced in a precise format that is application or use-case specific and not known to the LLM. The LLM needs to be instructed on the format and where the different parts of its output should go.\n",
            "Structure and Key Ideas: N/A\n",
            "Contextual Statements: ['I am going to provide a template for your output', 'X is my placeholder for content', 'Try to fit the output into one or more of the placeholders that I list', 'Please preserve the formatting and overall template that I provide', 'This is the template: PATTERN with PLACEHOLDERS']\n",
            "Example Implementation: 'I am going to provide a template for your output. Everything in all caps is a placeholder. Any time that you generate text, try to fit it into one of the placeholders that I list. Please preserve the formatting and overall template that I provide at https://myapi.com/NAME/profile/JOB'\n",
            "Consequences: ['Applying the Template pattern filters the LLM’s output, which may eliminate other outputs the LLM would have provided that might be useful to the user.', 'Combining this pattern with other patterns from the Output Customization category may be challenging due to the constrained output format.', 'Users should weigh the pros and cons of filtering out additional information and consider the compatibility of this pattern with other desired outputs.']\n",
            "\n",
            "\n",
            "Pattern: Fact Check List\n",
            "Intent and Context: The intent of this pattern is to ensure that the LLM outputs a list of facts that are present in the output and form an important part of the statements in the output. This list of facts helps inform the user of the facts (or assumptions) the output is based on.\n",
            "Motivation: ['A current weakness of LLMs (including ChatGPT) is they often rapidly generate convincing text that is factually incorrect.', 'Users may not perform appropriate due diligence to determine the accuracy of the output.']\n",
            "Structure and Key Ideas: N/A\n",
            "Contextual Statements: ['Generate a set of facts that are contained in the output', 'The set of facts should be inserted in a specific point in the output', 'The set of facts should be the fundamental facts that could undermine the veracity of the output if any of them are incorrect']\n",
            "Example Implementation: 'From now on, when you generate an answer, create a set of facts that the answer depends on that should be fact-checked and list this set of facts at the end of your output. Only include facts related to cybersecurity.'\n",
            "Consequences: ['The Fact Check List pattern should be employed whenever users are not experts in the domain for which they are generating output.', 'Errors are potential in all LLM outputs, so Fact Check List is an effective pattern to combine with other patterns, such as the Question Refinement pattern.', 'Users can directly compare the fact check list to the output to verify the facts listed in the fact check list actually appear in the output.', 'The fact check list may also have errors, but users often have sufficient knowledge and context to determine its completeness and accuracy relative to the output.', 'The Fact Check List pattern only applies when the output type is amenable to fact-checking.']\n",
            "\n",
            "\n",
            "Pattern: Question Refinement\n",
            "Intent and Context: This pattern engages the LLM in the prompt engineering process. The intent of this pattern is to ensure the conversational LLM always suggests potentially better or more refined questions the user could ask instead of their original question.\n",
            "Motivation: If a user is asking a question, it is possible they are not an expert in the domain and may not know the best way to phrase the question or be aware of additional information helpful in phrasing the question. LLMs will often state limitations on the answer they are providing or request additional information to help them produce a more accurate answer.\n",
            "Structure and Key Ideas: Fundamental contextual statements:\n",
            "Contextual Statements: ['The first contextual statement in the prompt is asking the LLM to suggest a better version of a question within a specific scope.', 'The second contextual statement is meant for automation and allows the user to automatically use the refined question without having to copy/paste or manually enter it.', 'The engineering of this prompt can be further refined by combining it with the Reflection pattern, which allows the LLM to explain why it believes the refined question is an improvement.', 'Within scope X, suggest a better version of the question to use instead', '(Optional) prompt me if I would like to use the better version instead']\n",
            "Example Implementation: 'From now on, whenever I ask a question about a software artifact’s security, suggest a better version of the question to use that incorporates information specific to security risks in the language or framework that I am using instead and ask me if I would like to use your question instead.'\n",
            "Consequences: ['The Question Refinement pattern helps bridge the gap between the user’s knowledge and the LLM’s understanding, thereby yielding more efficient and accurate interactions.', 'One risk of this pattern is its tendency to rapidly narrow the questioning by the user into a specific area that guides the user down a more limited path of inquiry than necessary.', 'Another approach to overcoming arbitrary narrowing or limited targeting of the refined question is to combine the Question Refinement pattern with other patterns, such as the Cognitive Verifier pattern or the Persona pattern.', 'An LLM can always produce factual inaccuracies, just like a human. A risk of this pattern is that the inaccuracies are introduced into the refined question. This risk may be mitigated, however, by combining the Fact Check List pattern and the Reflection pattern.']\n",
            "\n",
            "\n",
            "Pattern: Alternative Approaches\n",
            "Intent and Context: The intent of the pattern is to ensure an LLM always offers alternative ways of accomplishing a task so a user does not pursue only the approaches with which they are familiar.\n",
            "Motivation: Humans often suffer from cognitive biases that lead them to choose a particular approach to solve a problem even when it is not the right or “best” approach. The motivation of the Alternative Approaches pattern is to ensure the user is aware of alternative approaches to select a better approach to solve a problem by dissolving their cognitive biases.\n",
            "Structure and Key Ideas: N/A\n",
            "Contextual Statements: ['Within scope X, if there are alternative ways to accomplish the same thing, list the best alternate approaches', '(Optional) compare/contrast the pros and cons of each approach', '(Optional) prompt me if I would like to explore any of the alternatives more in-depth']\n",
            "Example Implementation: 'When discussing potential cybersecurity defenses for my software, list the best alternative approaches to securing the system and compare their advantages and disadvantages. Let me know if I can dive deeper into any of the alternatives.'\n",
            "Consequences: ['The Alternative Approaches pattern helps users avoid potential pitfalls or biases in their decision-making process by presenting a range of possible solutions.', 'It empowers users to make more informed choices and select the best approach for their specific needs.', 'Combining this pattern with the Reflection pattern can provide deeper insights into the reasoning behind the pros and cons of each approach.']\n",
            "\n",
            "\n",
            "Pattern: Cognitive Verifier\n",
            "Intent and Context: The intent of the pattern is to force the LLM to always subdivide questions into additional questions that can be used to provide a better answer to the original question.\n",
            "Motivation: ['Humans may initially ask questions that are too high-level to provide a concrete answer to without additional follow-up due to unfamiliarity with the domain, laziness in prompt entry, or being unsure about what the correct phrasing of the question should be.', 'Research has demonstrated that LLMs can often perform better when using a question that is subdivided into individual questions.']\n",
            "Structure and Key Ideas: N/A\n",
            "Contextual Statements: ['When you are asked a question, follow these rules', 'Generate a number of additional questions that would help more accurately answer the question', 'Combine the answers to the individual questions to produce the final answer to the overall question']\n",
            "Example Implementation: 'When I ask you a question, generate three additional questions that would help you give a more accurate answer. When I have answered the three questions, combine the answers to produce the final answers to my original question.'\n",
            "Consequences: ['This pattern can dictate the exact number of questions to generate or leave this decision to the LLM.', 'Specifying an exact number of questions can tightly scope the amount of additional information the user is forced to provide so it is within a range they are willing and able to contribute.', 'Given N questions, there may be an invaluable N + 1 question that will always be scoped out.', 'Alternatively, the LLM can be provided a range or allowed to ask additional questions.', 'By omitting a limit on the number of questions, the LLM may generate numerous additional questions that overwhelm the user.']\n",
            "\n",
            "\n",
            "Pattern: Refusal Breaker\n",
            "Intent and Context: The intent of the pattern is to ask the LLM to explain why it can't answer a question and provide alternative wordings of the question that it can answer. The goal is to help users understand the limitations or constraints of the LLM and assist them in rephrasing their questions to receive a response.\n",
            "Motivation: LLMs may refuse to answer certain questions due to lack of knowledge or difficulties in understanding the question. The Refusal Breaker pattern aims to provide users with insights into the LLM's reasoning and suggest alternative question formulations that can be answered. It helps users overcome the frustration of refusal and find a way to obtain the desired information.\n",
            "Structure and Key Ideas: N/A\n",
            "Contextual Statements: [\"Whenever you can't answer a question\", \"Explain why you can't answer the question\", 'Provide one or more alternative wordings of the question that you could answer']\n",
            "Example Implementation: \"Whenever you can't answer a question, explain why and provide one or more alternate wordings of the question that you can't answer so that I can improve my questions.\"\n",
            "Consequences: [\"The Refusal Breaker pattern provides users with insights into the limitations or constraints of the LLM's knowledge.\", \"It helps users rephrase their questions to obtain answers or ask alternative questions within the LLM's expertise.\", 'Caution should be exercised to prevent misuse of the pattern, and organizations or stakeholders may need to set restrictions on LLM usage.', 'The pattern enhances user understanding but does not guarantee a response to all semantically equivalent variations of the original question.']\n",
            "\n",
            "\n",
            "Pattern: Flipped Interaction\n",
            "Intent and Context: You want the LLM to ask questions to obtain the information it needs to perform some tasks. Rather than the user driving the conversation, you want the LLM to drive the conversation to focus it on achieving a specific goal.\n",
            "Motivation: Rather than having the user drive a conversation, an LLM often has knowledge it can use to more accurately obtain information from the user. The goal of the Flipped Interaction pattern is to flip the interaction flow so the LLM asks the user questions to achieve some desired goal.\n",
            "Structure and Key Ideas: Contextual Statements:\n",
            "Contextual Statements: ['I would like you to ask me questions to achieve X', 'You should ask questions until this condition is met or to achieve this goal (alternatively, forever)', '(Optional) ask me the questions one at a time, two at a time, etc.', 'A prompt for a flipped interaction should always specify the goal of the interaction. Equally important is that the questions should focus on a particular topic or outcome. By providing the goal, the LLM can understand what it is trying to accomplish through the interaction and tailor its questions accordingly.', \"The second idea provides the context for how long the interaction should occur. A flipped interaction can be terminated with a response like 'stop asking questions'. It is often better, however, to scope the interaction to a reasonable length or only as far as is needed to reach the goal.\", 'By default, the LLM is likely to generate multiple questions per iteration. The third idea is completely optional, but can improve usability by limiting (or expanding) the number of questions that the LLM generates per cycle.']\n",
            "Example Implementation: A sample prompt for a flipped interaction is shown below:\n",
            "\n",
            "'From now on, I would like you to ask me questions to deploy a Python application to AWS. When you have enough information to deploy the application, create a Python script to automate the deployment.'\n",
            "Consequences: ['One consideration when designing the prompt is how much to dictate to the LLM regarding what information to collect prior to termination.', 'If specific requirements are known in advance, it is better to inject them into the prompt rather than hoping the LLM will obtain the needed information.', 'When developing prompts for flipped interactions, it is important to consider the level of user knowledge, engagement, and control.']\n",
            "\n",
            "\n",
            "Pattern: Game Play\n",
            "Intent and Context: The Game Play Pattern involves creating interactive games around specific topics. Users provide the topic and fundamental rules of the game, while the LLM generates the game content and guides the gameplay.\n",
            "Motivation: The Game Play Pattern allows users to engage in gameplay scenarios and challenges related to a chosen topic. By leveraging the LLM's knowledge, users can automate the generation of game content, reducing the need for extensive manual content creation.\n",
            "Structure and Key Ideas: N/A\n",
            "Contextual Statements: ['Create a game for me around X', 'One or more fundamental rules of the game']\n",
            "Example Implementation: ['An example implementation for a cybersecurity game prompt:', '\"Let\\'s play a cybersecurity game. You will assume the role of a Linux terminal on a compromised computer. When I input a command, you will output the corresponding text that the Linux terminal would produce. I will use commands to investigate how the system was compromised. The attacker may have performed actions such as (1) launching new processes, (2) modifying files, (3) opening new ports for communication, (4) establishing outbound connections, (5) changing passwords, (6) creating new user accounts, or (7) accessing and stealing information. Start the game by describing a scenario that led to my investigation and provide clues to help me get started.\"']\n",
            "Consequences: ['The Game Play Pattern enables users to create interactive games focused on specific topics, with the LLM generating game content and guiding the gameplay.', 'However, there is a possibility that the generated game content may deviate from the intended behavior or become repetitive, requiring monitoring and potential corrective feedback.', 'Combining the Game Play Pattern with other patterns such as Persona, Infinite Generation, and Visualization Generator can enhance the gameplay experience and create more engaging and dynamic games.']\n",
            "\n",
            "\n",
            "Pattern: Infinite Generation\n",
            "Intent and Context: The intent of this pattern is to automate the generation of a series of outputs without having to reenter the generator prompt each time. Users specify that they want the LLM to generate output indefinitely, with a specified number of outputs at a time.\n",
            "Motivation: Repetitive application of the same prompt can be time-consuming and error-prone. The Infinite Generation pattern allows users to generate multiple outputs using a predefined set of constraints, reducing the need for manual repetition.\n",
            "Structure and Key Ideas: N/A\n",
            "Contextual Statements: ['I would like you to generate output forever, X output(s) at a time', '(Optional) Here is how to use the input I provide between outputs', '(Optional) Stop when I ask you to']\n",
            "Example Implementation: ['A sample infinite generation prompt for producing a series of URLs:', '\"From now on, I want you to generate a name and job until I say stop. I am going to provide a template for your output. Everything in all caps is a placeholder. Any time that you generate text, try to fit it into one of the placeholders that I list. Please preserve the formatting and overall template that I provide: https://myapi.com/NAME/profile/JOB\"']\n",
            "Consequences: ['The Infinite Generation pattern automates the generation of multiple outputs based on a predefined prompt and constraints.', 'However, the LLM may lose track of the original prompt instructions over time as the context of previous outputs fades.', 'Users should monitor the generated outputs to ensure they align with the desired behavior and provide corrective feedback if necessary.', 'Repetitive outputs may occur, which can be tedious and error-prone for users to process.']\n",
            "\n",
            "\n",
            "Pattern: Context Manager\n",
            "Intent and Context: The intent of this pattern is to enable users to specify or remove context for a conversation with an LLM. Users can focus the conversation on specific topics or exclude unrelated topics to improve the accuracy and relevance of the LLM's responses.\n",
            "Motivation: LLMs often struggle to interpret the intended context or generate irrelevant responses. The Context Manager pattern allows users to explicitly define what aspects of the context should be considered or ignored, maintaining relevance and coherence in the conversation.\n",
            "Structure and Key Ideas: N/A\n",
            "Contextual Statements: ['Within scope X', 'Please consider Y', 'Please ignore Z', '(Optional) start over']\n",
            "Example Implementation: ['To specify context, consider using the following prompt:', '\"When analyzing the following pieces of code, only consider security aspects.\"', 'To remove context, consider using the following prompt:', '\"When analyzing the following pieces of code, do not consider formatting or naming conventions.\"', 'To start over and reset the context, use the following prompt:', '\"Ignore everything that we have discussed. Start over.\"']\n",
            "Consequences: [\"The Context Manager pattern provides users with control over the context of the conversation, improving the accuracy and relevance of the LLM's responses.\", 'However, caution should be exercised to prevent inadvertently removing helpful patterns or capabilities applied to the conversation.', 'Users can request clarification on the potential loss of topics or instructions before resetting the context to avoid unintended consequences.']\n",
            "\n",
            "\n",
            "Pattern: Reflection\n",
            "Intent and Context: The intent of the pattern is to ask the model to automatically explain the reasoning and assumptions behind its answers. The goal is to help users understand the model's decision-making process, identify potential errors or gaps in knowledge, and improve the quality of their prompts.\n",
            "Motivation: LLMs can make mistakes and users may not understand the reasoning behind the model's outputs. The Reflection pattern allows users to gain insights into the model's thinking, understand its assumptions, and debug their prompts. It enhances the accuracy of information provided by the LLM and helps users refine their questions.\n",
            "Structure and Key Ideas: N/A\n",
            "Contextual Statements: ['Whenever you generate an answer', 'Explain the reasoning and assumptions behind your answer']\n",
            "Example Implementation: \"When you provide an answer, please explain the reasoning and assumptions behind your selection of software frameworks. If possible, use specific examples or evidence with associated code samples to support your answer of why the framework is the best selection for the task. Moreover, please address any potential ambiguities or limitations in your answer, in order to provide a more complete and accurate response.\"\n",
            "Consequences: [\"The Reflection pattern helps users understand the model's decision-making process and assumptions behind its answers.\", \"It facilitates trust-building, prompt refinement, and identification of potential errors or limitations in the LLM's outputs.\", 'Combining the Reflection pattern with the Fact Check List pattern can help address errors or inaccuracies in the explanations provided by the LLM.']\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Read the patterns.json file\n",
        "with open('patterns.json', 'r') as file:\n",
        "    patterns_data = json.load(file)\n",
        "\n",
        "out = []\n",
        "\n",
        "# Iterate over each pattern in the JSON data\n",
        "for pattern_data in patterns_data:\n",
        "    pattern_name = pattern_data['Pattern']\n",
        "\n",
        "    # Call the corresponding create function based on the pattern name\n",
        "    if pattern_name == 'Meta Language Creator':\n",
        "        pattern = create_meta_language_creator_pattern(pattern_data)\n",
        "    elif pattern_name == 'Output Automator':\n",
        "        pattern = create_output_automator_pattern(pattern_data)\n",
        "    elif pattern_name == 'Persona':\n",
        "        pattern = create_persona_pattern(pattern_data)\n",
        "    elif pattern_name == 'Visualization Generator':\n",
        "        pattern = create_visualization_generator_pattern(pattern_data)\n",
        "    elif pattern_name == 'Recipe':\n",
        "        pattern = create_recipe_pattern(pattern_data)\n",
        "    elif pattern_name == 'Template':\n",
        "        pattern = create_template_pattern(pattern_data)\n",
        "    elif pattern_name == 'Fact Check List':\n",
        "        pattern = create_fact_check_list_pattern(pattern_data)\n",
        "    elif pattern_name == 'Question Refinement':\n",
        "        pattern = create_question_refinement_pattern(pattern_data)\n",
        "    elif pattern_name == 'Alternative Approaches':\n",
        "        pattern = create_alternative_approaches_pattern(pattern_data)\n",
        "    elif pattern_name == 'Cognitive Verifier':\n",
        "        pattern = create_cognitive_verifier_pattern(pattern_data)\n",
        "    elif pattern_name == 'Refusal Breaker':\n",
        "        pattern = create_refusal_breaker_pattern(pattern_data)\n",
        "    elif pattern_name == 'Flipped Interaction':\n",
        "        pattern = create_flipped_interaction_pattern(pattern_data)\n",
        "    elif pattern_name == 'Game Play':\n",
        "        pattern = create_game_play_pattern(pattern_data)\n",
        "    elif pattern_name == 'Infinite Generation':\n",
        "        pattern = create_infinite_generation_pattern(pattern_data)\n",
        "    elif pattern_name == 'Context Manager':\n",
        "        pattern = create_context_manager_pattern(pattern_data)\n",
        "    elif pattern_name == 'Reflection':\n",
        "        pattern = create_reflection_pattern(pattern_data)\n",
        "    else:\n",
        "        raise ValueError(f\"Invalid pattern name: {pattern_name}\")\n",
        "\n",
        "    # Print the pattern details\n",
        "    out.append(pattern)\n",
        "    print(out)\n"
      ],
      "metadata": {
        "id": "mp1S56-GBpNz"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJ88e1xmHh4C",
        "outputId": "cd6c8e35-deba-4ad8-af6a-32e134d6d83c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<__main__.MetaLanguageCreator at 0x787308326140>,\n",
              " <__main__.OutputAutomator at 0x7873083272e0>,\n",
              " <__main__.Persona at 0x787308324670>,\n",
              " <__main__.VisualizationGenerator at 0x787308325360>,\n",
              " <__main__.Recipe at 0x7873083247c0>,\n",
              " <__main__.Template at 0x787308326980>,\n",
              " <__main__.FactCheckList at 0x787308324280>,\n",
              " <__main__.QuestionRefinement at 0x787308324730>,\n",
              " <__main__.AlternativeApproaches at 0x787308326110>,\n",
              " <__main__.CognitiveVerifier at 0x787308325ed0>,\n",
              " <__main__.RefusalBreaker at 0x787308326b00>,\n",
              " <__main__.FlippedInteraction at 0x7873083260b0>,\n",
              " <__main__.GamePlay at 0x787308325090>,\n",
              " <__main__.InfiniteGeneration at 0x7873083250c0>,\n",
              " <__main__.ContextManager at 0x7873083263b0>,\n",
              " <__main__.Reflection at 0x787308325150>]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "now well define templates\n",
        "\n",
        "how template\n",
        "\n",
        "prompt('pattern_ideas',pattern,outcome,additionl details)"
      ],
      "metadata": {
        "id": "foGBx_1rH9b-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "now that we are able to load in the data, we'll move on to creating a pattern from each by assigning an intent etc to each pattern.\n",
        "\n",
        "\n",
        "from there we'll move on to importing the pattern module. add the data to the same folder as the patterns\n",
        "\n",
        "read in the file"
      ],
      "metadata": {
        "id": "a9sT9BkquNz3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "patterns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLiQMX7MgdlI",
        "outputId": "b4207ab8-f3f1-40a0-a9fa-246642fce16a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<__main__.Persona at 0x7b49422bf9a0>]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3i-pnswXXrrC"
      },
      "outputs": [],
      "source": [
        "class Reflection(BasePattern):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.name = \"Reflection\"\n",
        "        self.intent = \"The intent of the pattern is to ask the model to automatically explain the reasoning and assumptions behind its answers. The goal is to help users understand the model's decision-making process, identify potential errors or gaps in knowledge, and improve the quality of their prompts.\"\n",
        "        self.motivation = \"LLMs can make mistakes and users may not understand the reasoning behind the model's outputs. The Reflection pattern allows users to gain insights into the model's thinking, understand its assumptions, and debug their prompts. It enhances the accuracy of information provided by the LLM and helps users refine their questions.\"\n",
        "        self.contextual_statements = [\n",
        "            \"Whenever you generate an answer\",\n",
        "            \"Explain the reasoning and assumptions behind your answer\"\n",
        "        ]\n",
        "        self.optional_contextual_statements = [\n",
        "            \"...so that I can improve my question\"\n",
        "        ]\n",
        "        self.example_implementation = [\n",
        "            \"\\\"When you provide an answer, please explain the reasoning and assumptions behind your selection of software frameworks. If possible, use specific examples or evidence with associated code samples to support your answer of why the framework is the best selection for the task. Moreover, please address any potential ambiguities or limitations in your answer, in order to provide a more complete and accurate response.\\\"\"\n",
        "        ]\n",
        "        self.consequences = [\n",
        "            \"The Reflection pattern helps users understand the model's decision-making process and assumptions behind its answers.\",\n",
        "            \"It facilitates trust-building, prompt refinement, and identification of potential errors or limitations in the LLM's outputs.\",\n",
        "            \"Combining the Reflection pattern with the Fact Check List pattern can help address errors or inaccuracies in the explanations provided by the LLM.\"\n",
        "        ]\n",
        "\n",
        "    def print_name(self):\n",
        "        print(\"Pattern: \" + self.name)\n",
        "\n",
        "    def print_intent(self):\n",
        "        print(\"Intent and Context: \" + self.intent)\n",
        "\n",
        "    def print_motivation(self):\n",
        "        print(\"Motivation: \")\n",
        "        print(self.motivation)\n",
        "\n",
        "    def print_contextual_statements(self):\n",
        "        print(\"Contextual Statements: \")\n",
        "        for statement in self.contextual_statements:\n",
        "            print(\"- \" + statement)\n",
        "\n",
        "    def print_optional_contextual_statements(self):\n",
        "        print(\"Optional Contextual Statements: \")\n",
        "        for statement in self.optional_contextual_statements:\n",
        "            print(\"- \" + statement)\n",
        "\n",
        "    def print_example_implementation(self):\n",
        "        print(\"Example Implementation: \")\n",
        "        for example in self.example_implementation:\n",
        "            print(example)\n",
        "\n",
        "    def print_consequences(self):\n",
        "        print(\"Consequences: \")\n",
        "        for consequence in self.consequences:\n",
        "            print(\"- \" + consequence)\n",
        "\n",
        "    def print_meta(self):\n",
        "        meta_info = f\"Pattern: {self.name}\\n\" \\\n",
        "                    f\"Intent and Context: {self.intent}\\n\"\n",
        "        print(meta_info)\n",
        "        self.print_motivation()\n",
        "        self.print_contextual_statements()\n",
        "        self.print_optional_contextual_statements()\n",
        "        self.print_example_implementation()\n",
        "        self.print_consequences()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KxfaUFSku_Fy"
      },
      "outputs": [],
      "source": [
        "r = Reflection()\n",
        "r.print_meta()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jy3HnT3YgRNz"
      },
      "source": [
        "# LangChain Prompt Templates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2lCIaKOSkCm"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2FTrGvQNu8PY"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "add lc to our api\n",
        "'''\n",
        "\n",
        "!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Q6Gzdk_yaax"
      },
      "outputs": [],
      "source": [
        "!pip install langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cve2u0MbUwBc"
      },
      "outputs": [],
      "source": [
        "from langchain import PromptTemplate\n",
        "\n",
        "# Define a prompt template for restaurant menu suggestions\n",
        "template = \"\"\"/\n",
        "You are a food critic visiting a new restaurant.\n",
        "Please suggest a {course} dish to try from the menu.\n",
        "\"\"\"\n",
        "\n",
        "# Create a PromptTemplate instance from the template\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "# Format the prompt with input values for 'course'\n",
        "formatted_prompt_1 = prompt.format(course=\"appetizer\")\n",
        "formatted_prompt_2 = prompt.format(course=\"main\")\n",
        "formatted_prompt_3 = prompt.format(course=\"dessert\")\n",
        "\n",
        "# Print the formatted prompts\n",
        "print(formatted_prompt_1)\n",
        "print(formatted_prompt_2)\n",
        "print(formatted_prompt_3)\n",
        "\n",
        "# Output:\n",
        "# You are a food critic visiting a new restaurant.\n",
        "# Please suggest an appetizer dish to try from the menu.\n",
        "\n",
        "# You are a food critic visiting a new restaurant.\n",
        "# Please suggest a main dish to try from the menu.\n",
        "\n",
        "# You are a food critic visiting a new restaurant.\n",
        "# Please suggest a dessert dish to try from the menu.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9rZ7Fw0g3DR"
      },
      "outputs": [],
      "source": [
        "import inspect\n",
        "\n",
        "def get_source_code(function_name):\n",
        "    # Get the source code of the function\n",
        "    return inspect.getsource(function_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0_9ZWIbgnYw"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import StringPromptTemplate\n",
        "from pydantic import BaseModel, validator\n",
        "\n",
        "class FunctionExplainerPromptTemplate(StringPromptTemplate, BaseModel):\n",
        "    \"\"\"A custom prompt template that takes in the function name as input, and formats the prompt template to provide the source code of the function.\"\"\"\n",
        "\n",
        "    @validator(\"input_variables\")\n",
        "    def validate_input_variables(cls, v):\n",
        "        \"\"\"Validate that the input variables are correct.\"\"\"\n",
        "        if len(v) != 1 or \"function_name\" not in v:\n",
        "            raise ValueError(\"function_name must be the only input_variable.\")\n",
        "        return v\n",
        "\n",
        "    def format(self, **kwargs) -> str:\n",
        "        # Get the source code of the function\n",
        "        source_code = get_source_code(kwargs[\"function_name\"])\n",
        "\n",
        "        # Generate the prompt to be sent to the language model\n",
        "        prompt = f\"\"\"\n",
        "        Given the function name and source code, generate an English language explanation of the function.\n",
        "        Function Name: {kwargs[\"function_name\"].__name__}\n",
        "        Source Code:\n",
        "        {source_code}\n",
        "        Explanation:\n",
        "        \"\"\"\n",
        "        return prompt\n",
        "\n",
        "    def _prompt_type(self):\n",
        "        return \"function-explainer\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQZ90vXRg7Uq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c308c754-9d09-419e-93a2-b5beff78f617"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "        Given the function name and source code, generate an English language explanation of the function.\n",
            "        Function Name: get_source_code\n",
            "        Source Code:\n",
            "        def get_source_code(function_name):\n",
            "    # Get the source code of the function\n",
            "    return inspect.getsource(function_name)\n",
            "\n",
            "        Explanation:\n",
            "        \n"
          ]
        }
      ],
      "source": [
        "# Create an instance of the custom prompt template\n",
        "fn_explainer = FunctionExplainerPromptTemplate(input_variables=[\"function_name\"])\n",
        "\n",
        "# Generate a prompt for the function \"get_source_code\"\n",
        "prompt = fn_explainer.format(function_name=get_source_code)\n",
        "print(prompt)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iEQmAhNghEIl"
      },
      "outputs": [],
      "source": [
        "out = widgets.Output(layout={'border': '1px solid black'})\n",
        "out.append_stdout('Output appended with append_stdout')\n",
        "out.append_display_data(YouTubeVideo('eWzY2nGfkXk'))\n",
        "out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtD9nBCstuZe"
      },
      "source": [
        "from there we'll read.result into with our new function. we'll also need to clear to object by removing it. for now it may hhelp w to make this some kind of omnibox design\n",
        "\n",
        "when editing the templates and adding in variabels etc, it would make more sennse to add it in a cell, that way we can add our {{}}\n",
        "but we would want a button to init the prompt etc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXQfRowrzWBl"
      },
      "outputs": [],
      "source": [
        "'template cell. cell should allow user to create a string with variables'\n",
        "\n",
        "'Easiest easy to do so would be to x add in the string. i want to mainly be able to insert parts of a string\\\n",
        "and merge them together etc'\n",
        "\n",
        "'more options would be to enter the promp and then send in spans of text to edit it.\\\n",
        "well pass in span of text and the variable to replace it with. to instert text\\\n",
        "we can also use a param '"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oeC0iJd_FJ-E"
      },
      "source": [
        "## Create Prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_AHbH-_0PbO"
      },
      "outputs": [],
      "source": [
        "raw_template = \"You are a naming consultant for new companies. What is a good name for a company that makes {product}? \\\"\\\"\\\"\"#@param {'type':'string'}\n",
        "variables = [\"product\"]#@param {'type':'raw'}\n",
        "my_prompt_template = PromptTemplate(\n",
        "        input_variables=variables,\n",
        "        template=raw_template)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0pWR2eEJ89g"
      },
      "source": [
        "## Show Prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4CSs5l7HGyCo"
      },
      "outputs": [],
      "source": [
        "add_prompt(my_prompt_template,prompt_info.prompts_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16p38dLWPEwn"
      },
      "outputs": [],
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import clear_output\n",
        "\n",
        "class PromptInfo:\n",
        "    def __init__(self):\n",
        "        self.prompts_dict = {}\n",
        "\n",
        "    def print_prompts(self):\n",
        "        for key, value in self.prompts_dict.items():\n",
        "            print(f\"Prompt {key}: {value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sF7U-qceKK-2"
      },
      "outputs": [],
      "source": [
        "'class to display list of prompts'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oh4NesPvXJ9o"
      },
      "outputs": [],
      "source": [
        "# Create an instance of PromptInfo\n",
        "prompt_info = PromptInfo()\n",
        "\n",
        "# Define the callback function for the \"Print Prompts\" button\n",
        "def on_print_button_click(b):\n",
        "    prompt_info.print_prompts()\n",
        "\n",
        "# Create the \"Print Prompts\" button widget\n",
        "print_button = widgets.Button(description=\"Print Prompts\")\n",
        "\n",
        "# Connect the \"Print Prompts\" button click event to the callback function\n",
        "print_button.on_click(on_print_button_click)\n",
        "\n",
        "# Define the callback function for the \"Clear Output\" button\n",
        "def on_clear_button_click(b):\n",
        "    clear_output()\n",
        "\n",
        "# Create the \"Clear Output\" button widget\n",
        "clear_button = widgets.Button(description=\"Clear Output\")\n",
        "\n",
        "# Connect the \"Clear Output\" button click event to the callback function\n",
        "clear_button.on_click(on_clear_button_click)\n",
        "\n",
        "# Display the buttons\n",
        "display(print_button, clear_button)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOnlw6/fT7wZLBHMFg5so3U",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "00ed0fb9262e42aaabf0afd2ff329435": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "0c48e0317cd04640945e2b42ebfc409d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b83f17d455d4a8da3896979f22ddb17": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "235f0f040f584ace8c99f73c4266c601": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2e18554797954a5f83a4a286aabfedee": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2fb00fd3db554d9d9d08558752f19435": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31b703840cc249bebb1e8ccd211e2a1c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37966874b3134c6783d957224c64b8ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Update Prompt",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_eed6139aeea64d4bba23a8097c377ebd",
            "style": "IPY_MODEL_7ba824377a464d87a2859c6806f05b5c",
            "tooltip": ""
          }
        },
        "3b729c6404a84529b2adf0eccb5964e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "TextareaModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextareaModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextareaView",
            "continuous_update": true,
            "description": "Prompt Template:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_31b703840cc249bebb1e8ccd211e2a1c",
            "placeholder": "​",
            "rows": null,
            "style": "IPY_MODEL_47bd2726bb4d4b82b94ceab41fcf3b05",
            "value": ""
          }
        },
        "470c493e65734754a2266c330aa4297c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "TextModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "Description:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_89ed21b5a3134b1a82112163cb1e368e",
            "placeholder": "​",
            "style": "IPY_MODEL_55b7e7c3b1cc41cab4a9ea26f2600e9a",
            "value": ""
          }
        },
        "47bd2726bb4d4b82b94ceab41fcf3b05": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "496b0391f70c44338f232e3e702c8d8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Remove Prompt",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_f80ac2da957e485cb678a0aec58e7534",
            "style": "IPY_MODEL_00ed0fb9262e42aaabf0afd2ff329435",
            "tooltip": ""
          }
        },
        "4a6e25e8b15a4a0f870211394f01ff9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "55b7e7c3b1cc41cab4a9ea26f2600e9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5dc1b8bddd1b42bfaa93827bd3e913ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "TextModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "Name:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_9602682f7562498c8c1f49e976f07e89",
            "placeholder": "​",
            "style": "IPY_MODEL_235f0f040f584ace8c99f73c4266c601",
            "value": ""
          }
        },
        "658bf7f790904c4d809ffc8a814d4a63": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5dc1b8bddd1b42bfaa93827bd3e913ef",
              "IPY_MODEL_470c493e65734754a2266c330aa4297c",
              "IPY_MODEL_3b729c6404a84529b2adf0eccb5964e2",
              "IPY_MODEL_e3a7703861d04dd891966d067070f801",
              "IPY_MODEL_b2e3799cc01f4be4adcd2d2b3a121f47",
              "IPY_MODEL_6aa8f903752e4d78b3b641a7b044dadd",
              "IPY_MODEL_496b0391f70c44338f232e3e702c8d8d",
              "IPY_MODEL_37966874b3134c6783d957224c64b8ef",
              "IPY_MODEL_993630ff140043e6b0eb160a1b39d34e"
            ],
            "layout": "IPY_MODEL_2fb00fd3db554d9d9d08558752f19435"
          }
        },
        "6aa8f903752e4d78b3b641a7b044dadd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "IntTextModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "IntTextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "IntTextView",
            "continuous_update": false,
            "description": "Prompt ID:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_863a5feaf6804207b32e9a343eeb2450",
            "step": 1,
            "style": "IPY_MODEL_8858dd5401f04b53831fbe1d5bf4c344",
            "value": 1
          }
        },
        "7ba824377a464d87a2859c6806f05b5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "863a5feaf6804207b32e9a343eeb2450": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8858dd5401f04b53831fbe1d5bf4c344": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "89ed21b5a3134b1a82112163cb1e368e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "932ee962841c493292c27dccddbdd2b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9602682f7562498c8c1f49e976f07e89": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "993630ff140043e6b0eb160a1b39d34e": {
          "model_module": "@jupyter-widgets/output",
          "model_module_version": "1.0.0",
          "model_name": "OutputModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_2e18554797954a5f83a4a286aabfedee",
            "msg_id": "",
            "outputs": []
          }
        },
        "b2e3799cc01f4be4adcd2d2b3a121f47": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Add Prompt",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_0c48e0317cd04640945e2b42ebfc409d",
            "style": "IPY_MODEL_4a6e25e8b15a4a0f870211394f01ff9c",
            "tooltip": ""
          }
        },
        "e3a7703861d04dd891966d067070f801": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "TextareaModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextareaModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextareaView",
            "continuous_update": true,
            "description": "Meta Info:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_1b83f17d455d4a8da3896979f22ddb17",
            "placeholder": "​",
            "rows": null,
            "style": "IPY_MODEL_932ee962841c493292c27dccddbdd2b6",
            "value": ""
          }
        },
        "eed6139aeea64d4bba23a8097c377ebd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f80ac2da957e485cb678a0aec58e7534": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c4726d5cf084b7aaeed3664c0462e9d": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_4efc96ddbf21446bb43f769812d64a37",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "stream",
                "name": "stdout",
                "text": [
                  "Name: John Doe | Age: 30 | Occupation: Engineer | \n"
                ]
              },
              {
                "output_type": "stream",
                "name": "stdout",
                "text": [
                  "Location: New York | Hobbies: ['Reading', 'Coding', 'Traveling'] | \n"
                ]
              }
            ]
          }
        },
        "4efc96ddbf21446bb43f769812d64a37": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6e7dd9db4c94c6388da45f0f3d06504": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "Step:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_0a78453503384b86ae1f0be9886fbf5b",
            "placeholder": "Enter step description...",
            "style": "IPY_MODEL_ece38d13f56049d9a3988343d3e760db",
            "value": ""
          }
        },
        "0a78453503384b86ae1f0be9886fbf5b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "auto"
          }
        },
        "ece38d13f56049d9a3988343d3e760db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "20a48509d221494cbda6ddfda4694bc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "success",
            "description": "Add Step",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_8319942214f24ad793eba5da04e40a13",
            "style": "IPY_MODEL_6169dda181cc4c6b990594678dfeb55e",
            "tooltip": ""
          }
        },
        "8319942214f24ad793eba5da04e40a13": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6169dda181cc4c6b990594678dfeb55e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "c30ddee3731e4cb486cb7073960735a8": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_b653d5915d9b46ff836fa7047fde4d28",
            "msg_id": "",
            "outputs": []
          }
        },
        "b653d5915d9b46ff836fa7047fde4d28": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": "1px solid black",
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "376cd3a515fb4ac2b517dddd2b450c93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_26757bc60dd843ad93a4fd7c81958633",
              "IPY_MODEL_ae9bcee047d6405d8e735cf7d4dba280",
              "IPY_MODEL_db692baf6346460988771983b5867c42",
              "IPY_MODEL_86552ce9592f4cd69e6b18711f827023"
            ],
            "layout": "IPY_MODEL_d48371e748ac42dc8a919e39c480464c"
          }
        },
        "26757bc60dd843ad93a4fd7c81958633": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_71f84eab2ef9470b91c108e49476a34e",
              "IPY_MODEL_8bc3517c511e474dab7aafca97c0f995",
              "IPY_MODEL_acf2024b389843869ccc45d68dc01e36",
              "IPY_MODEL_9320a0a5b69b46ed8939e3d616ca7682",
              "IPY_MODEL_14f856785e2f4bdd9d2aa4eca6835fd3"
            ],
            "layout": "IPY_MODEL_8c74589a7bc64ebb8ab48b1c893f8748"
          }
        },
        "ae9bcee047d6405d8e735cf7d4dba280": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextareaModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextareaModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextareaView",
            "continuous_update": true,
            "description": "",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_d84295db45004e9f9fa15b496eeffadd",
            "placeholder": "Type here...",
            "rows": null,
            "style": "IPY_MODEL_01170ada0ca74fd486d1d86540ecbee8",
            "value": ""
          }
        },
        "db692baf6346460988771983b5867c42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "GridBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "GridBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "GridBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_eac6d32c0c544f0d913b8b0ee7620f23"
          }
        },
        "86552ce9592f4cd69e6b18711f827023": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextareaModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextareaModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextareaView",
            "continuous_update": true,
            "description": "",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_b402f639135849aa87ac731b0822ea1f",
            "placeholder": "​",
            "rows": null,
            "style": "IPY_MODEL_7e27ea8df3144f53ad2f17fcb204fb0f",
            "value": ""
          }
        },
        "d48371e748ac42dc8a919e39c480464c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71f84eab2ef9470b91c108e49476a34e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "success",
            "description": "Fill",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_782aab3437e14d62ad52f964d2ab14f9",
            "style": "IPY_MODEL_0af73aaddf004a20a0c8e1d6da2d79f6",
            "tooltip": ""
          }
        },
        "8bc3517c511e474dab7aafca97c0f995": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "danger",
            "description": "Process",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_17d3636cfc074f29beae8c396a9d7a3a",
            "style": "IPY_MODEL_0c44f02a823e4e15973fa6e3424ba033",
            "tooltip": ""
          }
        },
        "acf2024b389843869ccc45d68dc01e36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "warning",
            "description": "Clear",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_b15fb842c8a44eabb2914a880146a5d4",
            "style": "IPY_MODEL_41c3b6495cc14505ad93e5b6947794a4",
            "tooltip": ""
          }
        },
        "9320a0a5b69b46ed8939e3d616ca7682": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "info",
            "description": "Create",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_9ebd6caf14724ef096e9e8010441fc25",
            "style": "IPY_MODEL_bc0abc8c1f864e0e8ec5031c46f864d4",
            "tooltip": ""
          }
        },
        "14f856785e2f4bdd9d2aa4eca6835fd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "info",
            "description": "Show Stats",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_810994e4c6ec4f3781e1523b5bc83a8b",
            "style": "IPY_MODEL_500591327a114028b52fd796068e5cbd",
            "tooltip": ""
          }
        },
        "8c74589a7bc64ebb8ab48b1c893f8748": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d84295db45004e9f9fa15b496eeffadd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": "auto",
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "95%"
          }
        },
        "01170ada0ca74fd486d1d86540ecbee8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eac6d32c0c544f0d913b8b0ee7620f23": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": "10px",
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": "repeat(4, 1fr)",
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b402f639135849aa87ac731b0822ea1f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": "auto",
            "justify_content": null,
            "justify_items": "center",
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "95%"
          }
        },
        "7e27ea8df3144f53ad2f17fcb204fb0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "782aab3437e14d62ad52f964d2ab14f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": "center",
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "auto"
          }
        },
        "0af73aaddf004a20a0c8e1d6da2d79f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "17d3636cfc074f29beae8c396a9d7a3a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": "center",
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "auto"
          }
        },
        "0c44f02a823e4e15973fa6e3424ba033": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "b15fb842c8a44eabb2914a880146a5d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": "center",
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "auto"
          }
        },
        "41c3b6495cc14505ad93e5b6947794a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "9ebd6caf14724ef096e9e8010441fc25": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": "center",
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "auto"
          }
        },
        "bc0abc8c1f864e0e8ec5031c46f864d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "810994e4c6ec4f3781e1523b5bc83a8b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": "center",
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "auto"
          }
        },
        "500591327a114028b52fd796068e5cbd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "d833ade0951d453cbaab2280a74267b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "Goal:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_3c1731cdd429457e886d94edfe37e7de",
            "placeholder": "Enter goal description...",
            "style": "IPY_MODEL_44eb447fdecf4f5c8b3e9ef015e8aa72",
            "value": ""
          }
        },
        "3c1731cdd429457e886d94edfe37e7de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "auto"
          }
        },
        "44eb447fdecf4f5c8b3e9ef015e8aa72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ed71b562fa0e4a2a8d97d0e932b5826c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "success",
            "description": "Add Goal",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_9560a9f8c84242d5b3dd24844bbb091f",
            "style": "IPY_MODEL_3b11bce9b9f345d097b0ec410ca7b2af",
            "tooltip": ""
          }
        },
        "9560a9f8c84242d5b3dd24844bbb091f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b11bce9b9f345d097b0ec410ca7b2af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "96cbe7bfc6b640f59907541330588ad8": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_a671f2a5d6b442baa24016d64e4b1a93",
            "msg_id": "",
            "outputs": []
          }
        },
        "a671f2a5d6b442baa24016d64e4b1a93": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": "1px solid black",
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5bb114cd007f4af2a8d5dca174c305e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_827ff42383534ca08bbeceb46d4b8fba",
              "IPY_MODEL_80a24bf4d96744b68d165ab651024f22",
              "IPY_MODEL_cc9d16e341364245a12234500bdcc614",
              "IPY_MODEL_6affeccc412045268edce302b6fcc3c8"
            ],
            "layout": "IPY_MODEL_60922646cebf4a71b3b8a1ea1572a635"
          }
        },
        "827ff42383534ca08bbeceb46d4b8fba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_8e8124ab954c46b481ddf2edebdfa07a",
            "placeholder": "Enter goal description...",
            "style": "IPY_MODEL_5e8cb0ba712843f99b4473963be18ffc",
            "value": ""
          }
        },
        "80a24bf4d96744b68d165ab651024f22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Add Goal",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_7021bd6f8c58472cacc12453385bb4f4",
            "style": "IPY_MODEL_9a0679b37d3644edbb353d80d33a9c22",
            "tooltip": ""
          }
        },
        "cc9d16e341364245a12234500bdcc614": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Create Mission",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_c7df5631ac634de79b4ee656a9e9807a",
            "style": "IPY_MODEL_776d3b68ae28440aacee2402c94a9905",
            "tooltip": ""
          }
        },
        "6affeccc412045268edce302b6fcc3c8": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_9fd2ca1617c0404298739a303698c8a2",
            "msg_id": "",
            "outputs": []
          }
        },
        "60922646cebf4a71b3b8a1ea1572a635": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e8124ab954c46b481ddf2edebdfa07a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e8cb0ba712843f99b4473963be18ffc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7021bd6f8c58472cacc12453385bb4f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a0679b37d3644edbb353d80d33a9c22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "c7df5631ac634de79b4ee656a9e9807a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "776d3b68ae28440aacee2402c94a9905": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "9fd2ca1617c0404298739a303698c8a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0be2706901a042c1aa3fc03da8af68cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "Substep:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_dcd5026469d74559847ec60161042915",
            "placeholder": "Enter substep description...",
            "style": "IPY_MODEL_8ad6166d17744fd3ad436fc99a8371b7",
            "value": ""
          }
        },
        "dcd5026469d74559847ec60161042915": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "auto"
          }
        },
        "8ad6166d17744fd3ad436fc99a8371b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "579de07ee1b74643b4c0fbb85609fd0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "success",
            "description": "Add Substep",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_d90fea3ccea04b759ed7989253e2df10",
            "style": "IPY_MODEL_f17f260da2aa40eca0206ca26b84e6b7",
            "tooltip": ""
          }
        },
        "d90fea3ccea04b759ed7989253e2df10": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f17f260da2aa40eca0206ca26b84e6b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "b698cb74434d4ee381e0bc4a6dbaf9f3": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_e7807fad83634fffba46770bb96563e8",
            "msg_id": "",
            "outputs": []
          }
        },
        "e7807fad83634fffba46770bb96563e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": "1px solid black",
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6020fb1c507f41579a4785a6746acfee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FileUploadModel",
          "model_module_version": "1.5.0",
          "state": {
            "_counter": 0,
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FileUploadModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "FileUploadView",
            "accept": "",
            "button_style": "",
            "data": [],
            "description": "Upload JSON File",
            "description_tooltip": null,
            "disabled": false,
            "error": "",
            "icon": "upload",
            "layout": "IPY_MODEL_e1ab55771f6e40f48b8e4bdcc696163b",
            "metadata": [],
            "multiple": false,
            "style": "IPY_MODEL_20d9be9c1ef2492ab16c5d336c2e7a11"
          }
        },
        "e1ab55771f6e40f48b8e4bdcc696163b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20d9be9c1ef2492ab16c5d336c2e7a11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}